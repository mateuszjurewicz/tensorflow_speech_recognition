{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model experiments - sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks we have separated a small subset of our data, called \"sample\", on which we can now experiment with simple models to assess the effectiveness of our preprocessing & data augmentation techniques.\n",
    "\n",
    "We do it this way to avoid spending too much time on training on the entire set, the assumption is that the methods which are effective on the sample will work well on a larger scale too. \n",
    "\n",
    "We will start by testing a couple of simple models on untouched sample data (as numpy arrays) and then proceed towards data augmentation and finally spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/tensorflow_speech_recognition\r\n"
     ]
    }
   ],
   "source": [
    "# first make sure we're in the parent dictory of our data/sample folders.\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "We'll need a couple of additional libraries so let's import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import glob\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow\n",
    "\n",
    "# utils\n",
    "from importlib import reload\n",
    "import utils; reload(utils)\n",
    "\n",
    "# keras as tensorflow backend\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, BatchNormalization, Dropout, Convolution1D, MaxPooling1D, Flatten\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "# F1 and accuracy score metric\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "The easiest way to work with data is by turning it into a list of numbers, in our case a numpy array. We can use one of the functions from utils to load the raw data or use the librosa.load() function. The difference lies in the fact that the former returns int16s whereas librosa returns float32s and uses its default sampling rate of 22050Hz, unless we explicitly tell it to use the file's original sampling rate of 16000Hz.\n",
    "\n",
    "We should also consider normalizing our data (so that it all falls within the same scale) and using the preprocessing methods explored in the previous notebook (MFCCs, Mel spectrogram, fast fourier transform and tempogram). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample = \"data/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to go through each of the folders in our sample/train, cv and test sets, one-hot encode their label and load the 16K long array of raw data. The y data will be of shape (m, 12), where m is the number of examples, and the X data will be of shape (m, 16000) - at least for the raw .wav input.\n",
    "\n",
    "Let's calculate **m** first. We will do that by using a function that create a list of all the .wav files within a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of paths\n",
    "We will use the glob module that we learned about in the very first notebook and a function from util.py which can, given a directory, return a list of paths to .wav files within it. We will repeat the process for all 3 sets within sample, and every category subdirectory within those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/sample/train/stop/01b4757a_nohash_0.wav',\n",
       " 'data/sample/train/stop/3ac2e76f_nohash_0.wav',\n",
       " 'data/sample/train/stop/3e31dffe_nohash_3.wav',\n",
       " 'data/sample/train/stop/37bd115d_nohash_1.wav',\n",
       " 'data/sample/train/stop/6c2dd2d5_nohash_0.wav']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example we can grab all .wav files from sample/train/stop\n",
    "path_to_sample_train_stop = os.path.join(path_to_sample, \"train\", \"stop\")\n",
    "utils.grab_wavs(path_to_sample_train_stop)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need a list of all category folder names\n",
    "categories_to_predict = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"silence\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/sample/train/yes/0f3f64d5_nohash_0.wav',\n",
       " 'data/sample/train/yes/8a28231e_nohash_3.wav',\n",
       " 'data/sample/train/yes/d3f22f0e_nohash_0.wav',\n",
       " 'data/sample/train/yes/2d3c8dcb_nohash_1.wav',\n",
       " 'data/sample/train/yes/d486fb84_nohash_0.wav',\n",
       " 'data/sample/train/yes/61d3e51e_nohash_0.wav',\n",
       " 'data/sample/train/yes/8f811bbc_nohash_0.wav',\n",
       " 'data/sample/train/yes/b43de700_nohash_0.wav',\n",
       " 'data/sample/train/yes/66a412a7_nohash_0.wav',\n",
       " 'data/sample/train/yes/92e17cc4_nohash_0.wav',\n",
       " 'data/sample/train/yes/6c9223bd_nohash_0.wav',\n",
       " 'data/sample/train/yes/d5356b9a_nohash_0.wav',\n",
       " 'data/sample/train/yes/e7d0eb3f_nohash_1.wav',\n",
       " 'data/sample/train/yes/712e4d58_nohash_2.wav',\n",
       " 'data/sample/train/yes/a2fefcb4_nohash_0.wav',\n",
       " 'data/sample/train/yes/324210dd_nohash_1.wav',\n",
       " 'data/sample/train/yes/742d6431_nohash_0.wav',\n",
       " 'data/sample/train/yes/70a00e98_nohash_1.wav',\n",
       " 'data/sample/train/yes/28e47b1a_nohash_4.wav',\n",
       " 'data/sample/train/yes/7014b07e_nohash_1.wav',\n",
       " 'data/sample/train/no/f0ebef1b_nohash_0.wav',\n",
       " 'data/sample/train/no/7e783e3f_nohash_0.wav',\n",
       " 'data/sample/train/no/65040d9b_nohash_0.wav',\n",
       " 'data/sample/train/no/fcb25a78_nohash_0.wav',\n",
       " 'data/sample/train/no/28ce0c58_nohash_6.wav',\n",
       " 'data/sample/train/no/3c257192_nohash_1.wav',\n",
       " 'data/sample/train/no/106a6183_nohash_2.wav',\n",
       " 'data/sample/train/no/eb0676ec_nohash_1.wav',\n",
       " 'data/sample/train/no/6add0595_nohash_0.wav',\n",
       " 'data/sample/train/no/3efa7ec4_nohash_1.wav',\n",
       " 'data/sample/train/no/e4a2cf79_nohash_0.wav',\n",
       " 'data/sample/train/no/3c257192_nohash_3.wav',\n",
       " 'data/sample/train/no/b36c27c2_nohash_0.wav',\n",
       " 'data/sample/train/no/73dda36a_nohash_0.wav',\n",
       " 'data/sample/train/no/7ab5b8f7_nohash_0.wav',\n",
       " 'data/sample/train/no/c661be6e_nohash_2.wav',\n",
       " 'data/sample/train/no/7f74626f_nohash_1.wav',\n",
       " 'data/sample/train/no/ab5b211a_nohash_1.wav',\n",
       " 'data/sample/train/no/86cb59b2_nohash_3.wav',\n",
       " 'data/sample/train/no/caa7feaf_nohash_1.wav',\n",
       " 'data/sample/train/up/d3badc9a_nohash_1.wav',\n",
       " 'data/sample/train/up/37e8db82_nohash_0.wav',\n",
       " 'data/sample/train/up/079dfce3_nohash_1.wav',\n",
       " 'data/sample/train/up/fb9d6d23_nohash_0.wav',\n",
       " 'data/sample/train/up/48463cbc_nohash_2.wav',\n",
       " 'data/sample/train/up/229978fd_nohash_4.wav',\n",
       " 'data/sample/train/up/7846fd85_nohash_1.wav',\n",
       " 'data/sample/train/up/f33660af_nohash_1.wav',\n",
       " 'data/sample/train/up/13d7b8c9_nohash_0.wav',\n",
       " 'data/sample/train/up/cb8f8307_nohash_3.wav',\n",
       " 'data/sample/train/up/b5552931_nohash_1.wav',\n",
       " 'data/sample/train/up/9dcfba4b_nohash_0.wav',\n",
       " 'data/sample/train/up/e649aa92_nohash_0.wav',\n",
       " 'data/sample/train/up/8281a2a8_nohash_3.wav',\n",
       " 'data/sample/train/up/6078eb0d_nohash_0.wav',\n",
       " 'data/sample/train/up/10ace7eb_nohash_1.wav',\n",
       " 'data/sample/train/up/422f8cf1_nohash_2.wav',\n",
       " 'data/sample/train/up/1ba20be2_nohash_0.wav',\n",
       " 'data/sample/train/up/742d6431_nohash_1.wav',\n",
       " 'data/sample/train/up/37a3cd33_nohash_0.wav',\n",
       " 'data/sample/train/down/a1cff772_nohash_2.wav',\n",
       " 'data/sample/train/down/4ec7d027_nohash_0.wav',\n",
       " 'data/sample/train/down/742d6431_nohash_4.wav',\n",
       " 'data/sample/train/down/0819edb0_nohash_2.wav',\n",
       " 'data/sample/train/down/884ae8e1_nohash_1.wav',\n",
       " 'data/sample/train/down/834f03fe_nohash_1.wav',\n",
       " 'data/sample/train/down/ced835d3_nohash_1.wav',\n",
       " 'data/sample/train/down/0cd323ec_nohash_0.wav',\n",
       " 'data/sample/train/down/3bfd30e6_nohash_2.wav',\n",
       " 'data/sample/train/down/0e5193e6_nohash_1.wav',\n",
       " 'data/sample/train/down/c5570933_nohash_0.wav',\n",
       " 'data/sample/train/down/b7a0754f_nohash_3.wav',\n",
       " 'data/sample/train/down/ced4e2a1_nohash_0.wav',\n",
       " 'data/sample/train/down/b4bef564_nohash_1.wav',\n",
       " 'data/sample/train/down/dabf67d9_nohash_0.wav',\n",
       " 'data/sample/train/down/3d53244b_nohash_0.wav',\n",
       " 'data/sample/train/down/778a4a01_nohash_0.wav',\n",
       " 'data/sample/train/down/35d1b6ee_nohash_3.wav',\n",
       " 'data/sample/train/down/06076b6b_nohash_2.wav',\n",
       " 'data/sample/train/down/38d78313_nohash_1.wav',\n",
       " 'data/sample/train/left/4ec7d027_nohash_0.wav',\n",
       " 'data/sample/train/left/a74f3917_nohash_0.wav',\n",
       " 'data/sample/train/left/c2bc4489_nohash_1.wav',\n",
       " 'data/sample/train/left/e102119e_nohash_0.wav',\n",
       " 'data/sample/train/left/3cdecb0b_nohash_1.wav',\n",
       " 'data/sample/train/left/51eefcc6_nohash_0.wav',\n",
       " 'data/sample/train/left/743edf9d_nohash_1.wav',\n",
       " 'data/sample/train/left/190821dc_nohash_2.wav',\n",
       " 'data/sample/train/left/813b82a6_nohash_0.wav',\n",
       " 'data/sample/train/left/833a0279_nohash_0.wav',\n",
       " 'data/sample/train/left/763188c4_nohash_0.wav',\n",
       " 'data/sample/train/left/da5dadb9_nohash_0.wav',\n",
       " 'data/sample/train/left/6e2ef3d5_nohash_0.wav',\n",
       " 'data/sample/train/left/d8521ea0_nohash_0.wav',\n",
       " 'data/sample/train/left/834f03fe_nohash_0.wav',\n",
       " 'data/sample/train/left/1b63157b_nohash_0.wav',\n",
       " 'data/sample/train/left/824e8ce5_nohash_0.wav',\n",
       " 'data/sample/train/left/408de0a4_nohash_0.wav',\n",
       " 'data/sample/train/left/9f4098cb_nohash_0.wav',\n",
       " 'data/sample/train/left/a2b16113_nohash_0.wav',\n",
       " 'data/sample/train/right/ece1a95a_nohash_0.wav',\n",
       " 'data/sample/train/right/7e783e3f_nohash_1.wav',\n",
       " 'data/sample/train/right/5ba724a7_nohash_0.wav',\n",
       " 'data/sample/train/right/93f30cc4_nohash_1.wav',\n",
       " 'data/sample/train/right/cb2929ce_nohash_4.wav',\n",
       " 'data/sample/train/right/7846fd85_nohash_1.wav',\n",
       " 'data/sample/train/right/e7ea8b76_nohash_1.wav',\n",
       " 'data/sample/train/right/57b68383_nohash_0.wav',\n",
       " 'data/sample/train/right/340c8b10_nohash_0.wav',\n",
       " 'data/sample/train/right/8781f4c1_nohash_0.wav',\n",
       " 'data/sample/train/right/9735764a_nohash_0.wav',\n",
       " 'data/sample/train/right/02fcd241_nohash_0.wav',\n",
       " 'data/sample/train/right/7f74626f_nohash_3.wav',\n",
       " 'data/sample/train/right/48a9f771_nohash_1.wav',\n",
       " 'data/sample/train/right/b06c19b0_nohash_1.wav',\n",
       " 'data/sample/train/right/be91a00a_nohash_1.wav',\n",
       " 'data/sample/train/right/9080f6d3_nohash_1.wav',\n",
       " 'data/sample/train/right/dc6e9c04_nohash_0.wav',\n",
       " 'data/sample/train/right/e91d173e_nohash_0.wav',\n",
       " 'data/sample/train/right/0137b3f4_nohash_0.wav',\n",
       " 'data/sample/train/on/ffd2ba2f_nohash_0.wav',\n",
       " 'data/sample/train/on/3565ca83_nohash_2.wav',\n",
       " 'data/sample/train/on/14775481_nohash_0.wav',\n",
       " 'data/sample/train/on/b9515bf3_nohash_1.wav',\n",
       " 'data/sample/train/on/5efb758c_nohash_0.wav',\n",
       " 'data/sample/train/on/eb3f7d82_nohash_2.wav',\n",
       " 'data/sample/train/on/9d050657_nohash_0.wav',\n",
       " 'data/sample/train/on/0132a06d_nohash_3.wav',\n",
       " 'data/sample/train/on/15d83b54_nohash_0.wav',\n",
       " 'data/sample/train/on/611d2b50_nohash_4.wav',\n",
       " 'data/sample/train/on/ab353673_nohash_0.wav',\n",
       " 'data/sample/train/on/c79159aa_nohash_1.wav',\n",
       " 'data/sample/train/on/c2e08f08_nohash_1.wav',\n",
       " 'data/sample/train/on/eb3f7d82_nohash_4.wav',\n",
       " 'data/sample/train/on/28ed6bc9_nohash_0.wav',\n",
       " 'data/sample/train/on/4f086393_nohash_0.wav',\n",
       " 'data/sample/train/on/cce17a61_nohash_1.wav',\n",
       " 'data/sample/train/on/8eb4a1bf_nohash_3.wav',\n",
       " 'data/sample/train/on/7799c9cd_nohash_0.wav',\n",
       " 'data/sample/train/on/2579e514_nohash_0.wav',\n",
       " 'data/sample/train/off/a759efbc_nohash_1.wav',\n",
       " 'data/sample/train/off/b16f2d0d_nohash_1.wav',\n",
       " 'data/sample/train/off/611d2b50_nohash_1.wav',\n",
       " 'data/sample/train/off/b59fe16d_nohash_0.wav',\n",
       " 'data/sample/train/off/37bd115d_nohash_0.wav',\n",
       " 'data/sample/train/off/a8f45bdc_nohash_0.wav',\n",
       " 'data/sample/train/off/5f1b1051_nohash_0.wav',\n",
       " 'data/sample/train/off/8fce59ff_nohash_2.wav',\n",
       " 'data/sample/train/off/89f3ab7d_nohash_2.wav',\n",
       " 'data/sample/train/off/d7529c0c_nohash_0.wav',\n",
       " 'data/sample/train/off/ef2578c0_nohash_1.wav',\n",
       " 'data/sample/train/off/c2e08f08_nohash_0.wav',\n",
       " 'data/sample/train/off/1bb6ed89_nohash_0.wav',\n",
       " 'data/sample/train/off/8b39e36f_nohash_2.wav',\n",
       " 'data/sample/train/off/4cb874bb_nohash_0.wav',\n",
       " 'data/sample/train/off/e8d562ca_nohash_1.wav',\n",
       " 'data/sample/train/off/c93d5e22_nohash_2.wav',\n",
       " 'data/sample/train/off/a3fc7884_nohash_0.wav',\n",
       " 'data/sample/train/off/0137b3f4_nohash_0.wav',\n",
       " 'data/sample/train/off/ab353673_nohash_1.wav',\n",
       " 'data/sample/train/stop/01b4757a_nohash_0.wav',\n",
       " 'data/sample/train/stop/3ac2e76f_nohash_0.wav',\n",
       " 'data/sample/train/stop/3e31dffe_nohash_3.wav',\n",
       " 'data/sample/train/stop/37bd115d_nohash_1.wav',\n",
       " 'data/sample/train/stop/6c2dd2d5_nohash_0.wav',\n",
       " 'data/sample/train/stop/cf68fad2_nohash_1.wav',\n",
       " 'data/sample/train/stop/f35eedd7_nohash_0.wav',\n",
       " 'data/sample/train/stop/12c206ea_nohash_1.wav',\n",
       " 'data/sample/train/stop/ae82c78c_nohash_0.wav',\n",
       " 'data/sample/train/stop/824e8ce5_nohash_1.wav',\n",
       " 'data/sample/train/stop/f632210f_nohash_1.wav',\n",
       " 'data/sample/train/stop/d0858dce_nohash_0.wav',\n",
       " 'data/sample/train/stop/0b56bcfe_nohash_0.wav',\n",
       " 'data/sample/train/stop/28ed6bc9_nohash_0.wav',\n",
       " 'data/sample/train/stop/229978fd_nohash_1.wav',\n",
       " 'data/sample/train/stop/f8f60f59_nohash_2.wav',\n",
       " 'data/sample/train/stop/012c8314_nohash_0.wav',\n",
       " 'data/sample/train/stop/a1cff772_nohash_0.wav',\n",
       " 'data/sample/train/stop/834f03fe_nohash_2.wav',\n",
       " 'data/sample/train/stop/479e64cc_nohash_1.wav',\n",
       " 'data/sample/train/go/42c6fff1_nohash_0.wav',\n",
       " 'data/sample/train/go/31583d30_nohash_0.wav',\n",
       " 'data/sample/train/go/324210dd_nohash_4.wav',\n",
       " 'data/sample/train/go/71904de3_nohash_0.wav',\n",
       " 'data/sample/train/go/ced835d3_nohash_1.wav',\n",
       " 'data/sample/train/go/9efe5140_nohash_1.wav',\n",
       " 'data/sample/train/go/c0c701f1_nohash_0.wav',\n",
       " 'data/sample/train/go/0cd323ec_nohash_0.wav',\n",
       " 'data/sample/train/go/126a31d2_nohash_1.wav',\n",
       " 'data/sample/train/go/afd53389_nohash_0.wav',\n",
       " 'data/sample/train/go/b4604db4_nohash_0.wav',\n",
       " 'data/sample/train/go/c1e0e8e3_nohash_3.wav',\n",
       " 'data/sample/train/go/bc065a17_nohash_1.wav',\n",
       " 'data/sample/train/go/ea9f8e11_nohash_1.wav',\n",
       " 'data/sample/train/go/590750e8_nohash_2.wav',\n",
       " 'data/sample/train/go/e41e41f7_nohash_0.wav',\n",
       " 'data/sample/train/go/59fe87e6_nohash_1.wav',\n",
       " 'data/sample/train/go/6414258b_nohash_0.wav',\n",
       " 'data/sample/train/go/26e573a9_nohash_1.wav',\n",
       " 'data/sample/train/go/483e2a6f_nohash_1.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_32.wav',\n",
       " 'data/sample/train/silence/pink_noise_4.wav',\n",
       " 'data/sample/train/silence/pink_noise_25.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_16.wav',\n",
       " 'data/sample/train/silence/doing_the_dishes_53.wav',\n",
       " 'data/sample/train/silence/pink_noise_51.wav',\n",
       " 'data/sample/train/silence/white_noise_10.wav',\n",
       " 'data/sample/train/silence/doing_the_dishes_9.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_17.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_43.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_37.wav',\n",
       " 'data/sample/train/silence/doing_the_dishes_86.wav',\n",
       " 'data/sample/train/silence/running_tap_9.wav',\n",
       " 'data/sample/train/silence/running_tap_51.wav',\n",
       " 'data/sample/train/silence/exercise_bike_57.wav',\n",
       " 'data/sample/train/silence/pink_noise_14.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_38.wav',\n",
       " 'data/sample/train/silence/pink_noise_20.wav',\n",
       " 'data/sample/train/silence/white_noise_1.wav',\n",
       " 'data/sample/train/silence/running_tap_52.wav',\n",
       " 'data/sample/train/unknown/4def68db_nohash_0_seven.wav',\n",
       " 'data/sample/train/unknown/5705a0f9_nohash_0_eight.wav',\n",
       " 'data/sample/train/unknown/238c112c_nohash_0_two.wav',\n",
       " 'data/sample/train/unknown/b55a09be_nohash_1_wow.wav',\n",
       " 'data/sample/train/unknown/421ed23f_nohash_0_zero.wav',\n",
       " 'data/sample/train/unknown/d37e4bf1_nohash_0_nine.wav',\n",
       " 'data/sample/train/unknown/ab5ae445_nohash_0_five.wav',\n",
       " 'data/sample/train/unknown/a527cb3c_nohash_0_sheila.wav',\n",
       " 'data/sample/train/unknown/4a1e736b_nohash_1_seven.wav',\n",
       " 'data/sample/train/unknown/44f68a83_nohash_0_six.wav',\n",
       " 'data/sample/train/unknown/b9515bf3_nohash_3_six.wav',\n",
       " 'data/sample/train/unknown/8f4c551f_nohash_0_four.wav',\n",
       " 'data/sample/train/unknown/10ace7eb_nohash_2_six.wav',\n",
       " 'data/sample/train/unknown/179a61b7_nohash_0_bed.wav',\n",
       " 'data/sample/train/unknown/9f63152b_nohash_0_six.wav',\n",
       " 'data/sample/train/unknown/4d9e07cf_nohash_2_two.wav',\n",
       " 'data/sample/train/unknown/e7ea8b76_nohash_0_cat.wav',\n",
       " 'data/sample/train/unknown/c0f8f4df_nohash_0_two.wav',\n",
       " 'data/sample/train/unknown/c1d39ce8_nohash_6_zero.wav',\n",
       " 'data/sample/train/unknown/1ecfb537_nohash_4_four.wav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first grab the training set\n",
    "path_to_train = os.path.join(path_to_sample, \"train\")\n",
    "sample_train_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_train, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    \n",
    "    # we use extend instead of append to add all elements from the iterable\n",
    "    sample_train_wavs.extend(category_files)\n",
    "    \n",
    "sample_train_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for cv\n",
    "path_to_cv = os.path.join(path_to_sample, \"cv\")\n",
    "sample_cv_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_cv, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    sample_cv_wavs.extend(category_files)\n",
    "\n",
    "# repeat for test\n",
    "path_to_test = os.path.join(path_to_sample, \"test\")\n",
    "sample_test_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_test, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    sample_test_wavs.extend(category_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode the y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the 3 lists of files from each set (train, cv and test) we can construct our train_y, cv_y and test_y numpy arrays. These will be matrices of size (m, 12), one-hot encoded. E.g. if a row belongs to the category \"up\" it will take the form of an array of zeros, where the entry at index 2 (the third from the left) will become a 1.\n",
    "\n",
    "We will use a function from the utils that takes a path to a .wav, the index at which the category name starts within it (we want to control this because we will eventually use this for the main set, not just the sample) and a list of categories to predict. For our current example, the category name in the paths belonging to \"train\" starts at the 18th index (separators count as one char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sample/train/left/4ec7d027_nohash_0.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's grab a single path (this one is an \"left\")\n",
    "a_wav = sample_train_wavs[80]\n",
    "a_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see if the 1 is correctly placed\n",
    "utils.one_hot_encode_path(a_wav, 18, categories_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path belonged to the fifth category (\"left\") and the one-hot encoding correctly placed the 1 at index 4 (zero-indexed).\n",
    "\n",
    "We want to repeat this for all examples in each of the 3 subsets, adding each new one-hot encoded numpy array as a new row of the y matrix, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out the dimensions of train_y\n",
    "rows = len(sample_train_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape: 2880\n",
      "New shape: (240, 12)\n"
     ]
    }
   ],
   "source": [
    "# create train_y as empty array\n",
    "train_y = np.array([])\n",
    "\n",
    "# append each row to train_y\n",
    "for path_to_wav in sample_train_wavs:\n",
    "    row = utils.one_hot_encode_path(path_to_wav, 18, categories_to_predict)\n",
    "    \n",
    "    # append the new row\n",
    "    train_y = np.append(train_y, row)\n",
    "    \n",
    "# we currently have a flattened vector\n",
    "print(\"Current shape: {}\".format(*train_y.shape))\n",
    "\n",
    "# let's reshape it\n",
    "train_y = np.reshape(train_y, dimensions)\n",
    "print(\"New shape: {}\".format(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the train_y matrix to confirm\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first 3 entries have the 1 at 0th index, which means they belong to category \"up\" and the last three have the 1 at the last index, which is also correct given the fact that our list of paths was also ordered.\n",
    "\n",
    "We should bear in mind that by default the np.array contains float64s and our functions for loading a .wav return int16s.\n",
    "\n",
    "Since this is a highly-repetitive task we'll want to use the utils function for obtaining the y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for **CV set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dimensions: (60, 12)\n",
      "Received shape: (60, 12)\n"
     ]
    }
   ],
   "source": [
    "# figure out the dimensions\n",
    "rows = len(sample_cv_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "print(\"Target dimensions: {}\".format(dimensions))\n",
    "\n",
    "# get the y\n",
    "cv_y = utils.get_y(sample_cv_wavs, 15, categories_to_predict)\n",
    "print(\"Received shape: {}\".format(cv_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for **Test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dimensions: (60, 12)\n",
      "Received shape: (60, 12)\n"
     ]
    }
   ],
   "source": [
    "# figure out the dimensions\n",
    "rows = len(sample_test_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "print(\"Target dimensions: {}\".format(dimensions))\n",
    "\n",
    "# get the y\n",
    "test_y = utils.get_y(sample_test_wavs, 17, categories_to_predict)\n",
    "print(\"Received shape: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the X\n",
    "We have the y - the one-hot encoded vectors representing the category for each training, cv and test example in the sample set. We need the feature vectors, conventionally referred to as X. We will use both the simplest way of extracting the .wav data and the preprocessing techniques - MFCCs, Mel spectrogram, FFT and tempogram.\n",
    "\n",
    "Let's start by defining a simple helper function for just the raw .wav data. Since our samples are of slightly differing lengths but each row of our X always has to have the same length, we will **add padding by default.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the desired number of columns (n)\n",
    "n = len(utils.get_wav_info(path_to_wav)[1])\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw .wav data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple helper function\n",
    "def get_X_with_padding(list_of_paths, columns=16000):\n",
    "    \n",
    "    # get shape data\n",
    "    rows = len(list_of_paths)\n",
    "    dimensions = (rows, columns)\n",
    "    \n",
    "    # create placeholder\n",
    "    X = np.array([])\n",
    "    \n",
    "    # go through every file path in the list\n",
    "    for path_to_wav in list_of_paths:\n",
    "\n",
    "        # get raw array of signed ints\n",
    "        row = utils.get_wav_info(path_to_wav)[1]\n",
    "        \n",
    "        # some of our sample have less (or slightly more) than 16000 values, so let's adjust them\n",
    "        # trim to fixed length\n",
    "        row = row[:columns]\n",
    "        \n",
    "        # pad with zeros, calculating amount of padding needed\n",
    "        padding = columns - len(row)\n",
    "        row = np.pad(row, (0, padding), mode='constant', constant_values=0)\n",
    "\n",
    "        # append the new row\n",
    "        X = np.append(X, row)\n",
    "    \n",
    "    # reshape (unroll)\n",
    "    X = np.reshape(X, dimensions)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (240, 16000)\n",
      "CV:  (60, 16000)\n",
      "Test:  (60, 16000)\n"
     ]
    }
   ],
   "source": [
    "# get the X for each set\n",
    "train_X = utils.get_X(sample_train_wavs, n)\n",
    "cv_X = utils.get_X(sample_cv_wavs, n)\n",
    "test_X = utils.get_X(sample_test_wavs, n)\n",
    "\n",
    "print(\"Train: \", train_X.shape)\n",
    "print(\"CV: \", cv_X.shape)\n",
    "print(\"Test: \",test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-11., -21., -25., -42., -33.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same for the MFCCs. We have a choice of whether or not we want to get returned only the mean value (1D) for the MFCCs. For now let's obtain both the 1D (mean) and 2D version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with a reasonable number of mfccs to return\n",
    "n_mfcc = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mfccs:  (240, 100)\n",
      "CV mfccs:  (60, 100)\n",
      "Test mfccs:  (60, 100)\n"
     ]
    }
   ],
   "source": [
    "train_X_mfccs_1D = utils.get_X_mfccs(sample_train_wavs, shape=(n_mfcc, 32), mean=True)\n",
    "cv_X_mfccs_1D = utils.get_X_mfccs(sample_cv_wavs, shape=(n_mfcc, 32), mean=True)\n",
    "test_X_mfccs_1D = utils.get_X_mfccs(sample_test_wavs, shape=(n_mfcc, 32), mean=True)\n",
    "\n",
    "print(\"Train mfccs: \", train_X_mfccs_1D.shape)\n",
    "print(\"CV mfccs: \", cv_X_mfccs_1D.shape)\n",
    "print(\"Test mfccs: \",test_X_mfccs_1D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-445.35308297,   31.80853315,   -3.24210645,   18.580648  ,\n",
       "          0.94357346])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_mfccs_1D[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the 2-dim output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mfccs:  (240, 100, 32)\n",
      "CV mfccs:  (60, 100, 32)\n",
      "Test mfccs:  (60, 100, 32)\n"
     ]
    }
   ],
   "source": [
    "train_X_mfccs_2D = utils.get_X_mfccs(sample_train_wavs, shape=(n_mfcc, 32), mean=False)\n",
    "cv_X_mfccs_2D = utils.get_X_mfccs(sample_cv_wavs, shape=(n_mfcc, 32), mean=False)\n",
    "test_X_mfccs_2D = utils.get_X_mfccs(sample_test_wavs, shape=(n_mfcc, 32), mean=False)\n",
    "\n",
    "print(\"Train mfccs: \", train_X_mfccs_2D.shape)\n",
    "print(\"CV mfccs: \", cv_X_mfccs_2D.shape)\n",
    "print(\"Test mfccs: \",test_X_mfccs_2D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-576.51036871, -570.90724179, -544.55853427, -553.88026846,\n",
       "       -578.52544577])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_mfccs_2D[0][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel spectrogam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of Mel spectrograms we expect to get a matrix from a vector, therefore our final X will be 3 dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (16000,)\n",
      "Mel spectrogram shape: (128, 32)\n"
     ]
    }
   ],
   "source": [
    "# let's see the difference in dimensions\n",
    "sr, raw_data = utils.get_wav_info(path_to_wav)\n",
    "print(\"Raw data shape: {}\".format(raw_data.shape))\n",
    "x = librosa.feature.melspectrogram(raw_data, sr)\n",
    "print(\"Mel spectrogram shape: {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's the function we'll use (via utils.py)\n",
    "def get_X_mel_spectrogram(list_of_paths, shape=(128, 32)):\n",
    "\n",
    "    # get shape data\n",
    "    rows = len(list_of_paths)\n",
    "\n",
    "    # create placeholder\n",
    "    result = np.array([])\n",
    "\n",
    "    # go through every file path in the list\n",
    "    for path_to_wav in list_of_paths:\n",
    "        \n",
    "        # get raw array of signed ints\n",
    "        sr, raw_data = utils.get_wav_info(path_to_wav)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(raw_data, sr)\n",
    "\n",
    "        # some of our samples have less (or slightly more) than the expected amount of values,\n",
    "        # so let's adjust them\n",
    "        placeholder = np.array([])\n",
    "        for row in mel_spectrogram:\n",
    "            \n",
    "            # trim to fixed length\n",
    "            row = row[:shape[1]]\n",
    "\n",
    "            # pad with zeros, calculating amount of padding needed\n",
    "            padding = shape[1] - len(row)\n",
    "            row = np.pad(row, (0, padding), mode='constant', constant_values=0)\n",
    "\n",
    "            # append the new row\n",
    "            placeholder = np.append(placeholder, row)\n",
    "        \n",
    "        # append the new unrolled matrix to the final result array\n",
    "        result = np.append(result, placeholder)\n",
    "    \n",
    "    # reshape into a 3-dim matrix\n",
    "    result = np.reshape(result, (len(list_of_paths), shape[0], shape[1]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the Mel spectrograms for all sample sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mel spectrogram:  (240, 128, 32)\n",
      "CV mel spectrogram:  (60, 128, 32)\n",
      "Test mel spectrogram:  (60, 128, 32)\n"
     ]
    }
   ],
   "source": [
    "train_X_mel_spectrogram = utils.get_X_mel_spectrogram(sample_train_wavs)\n",
    "cv_X_mel_spectrogram = utils.get_X_mel_spectrogram(sample_cv_wavs)\n",
    "test_X_mel_spectrogram = utils.get_X_mel_spectrogram(sample_test_wavs)\n",
    "\n",
    "print(\"Train mel spectrogram: \", train_X_mel_spectrogram.shape)\n",
    "print(\"CV mel spectrogram: \", cv_X_mel_spectrogram.shape)\n",
    "print(\"Test mel spectrogram: \",test_X_mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1457703.31342286,  427352.21837965,  201594.17632909,\n",
       "        238226.53307341,   65491.03447233,  178705.29710062,\n",
       "        412570.67348324,  378435.94871593,  258376.05295182,\n",
       "        187066.96734191,  239451.9017311 ,   56142.51095658,\n",
       "         42836.42147842,  139791.55001964,  102884.20436902,\n",
       "        167352.41037348,  321818.54914338,  559749.40569307,\n",
       "        989871.95717842,  918093.81247816, 1827327.90723131,\n",
       "       1677686.13316353,  673552.71678095,  419856.38671465,\n",
       "        384360.98528384,  454044.27309286,  670187.30942213,\n",
       "        427213.30004477,  395041.75416788,  548740.11881148,\n",
       "        294776.165019  ,  336066.61377942])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row is a 2D matrix (hence double-indexing)\n",
    "train_X_mel_spectrogram[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFT (Fast Fourier Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the FFT of our raw data too. For simplicity the utils.get_X_fft() function casts the complex numbers to the numpy float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here the shapes are the same\n",
    "x = utils.extract_fft(path_to_wav)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fft:  (240, 16000)\n",
      "CV fft:  (60, 16000)\n",
      "Test fft:  (60, 16000)\n"
     ]
    }
   ],
   "source": [
    "train_X_fft = utils.get_X_fft(sample_train_wavs)\n",
    "cv_X_fft = utils.get_X_fft(sample_cv_wavs)\n",
    "test_X_fft = utils.get_X_fft(sample_test_wavs)\n",
    "\n",
    "print(\"Train fft: \", train_X_fft.shape)\n",
    "print(\"CV fft: \", cv_X_fft.shape)\n",
    "print(\"Test fft: \",test_X_fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-11519.1640625 ,   -783.21451673,  -2758.71579679,   4396.02183472,\n",
       "         3284.95650186])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no longer complex numbers\n",
    "print(type(test_X_fft[0][0]))\n",
    "test_X_fft[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tempogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tempogram we have to do some reshaping to get a 3D matrix, just like with Mel spectrograms. We will also have to do a little bit of padding and trimming, to account for small differences in the length of the original sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempogram: (384, 32)\n"
     ]
    }
   ],
   "source": [
    "# let's see the difference in dimensions\n",
    "x = utils.extract_tempogram(path_to_wav)\n",
    "print(\"Tempogram: {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tempogram:  (240, 384, 32)\n",
      "CV tempogram:  (60, 384, 32)\n",
      "Test tempogram:  (60, 384, 32)\n"
     ]
    }
   ],
   "source": [
    "train_X_tempogram = utils.get_X_tempogram(sample_train_wavs)\n",
    "cv_X_tempogram = utils.get_X_tempogram(sample_cv_wavs)\n",
    "test_X_tempogram = utils.get_X_tempogram(sample_test_wavs)\n",
    "\n",
    "print(\"Train tempogram: \", train_X_tempogram.shape)\n",
    "print(\"CV tempogram: \", cv_X_tempogram.shape)\n",
    "print(\"Test tempogram: \",test_X_tempogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1764312 , 0.17643258, 0.17643404, 0.17643558, 0.17643721,\n",
       "       0.17643893, 0.17644072, 0.1764426 , 0.17644456, 0.17644661,\n",
       "       0.17644875, 0.17645096, 0.17645327, 0.17645565, 0.17645813,\n",
       "       0.17646069, 0.17646334, 0.17646607, 0.1764689 , 0.17647181,\n",
       "       0.17647481, 0.1764779 , 0.17648109, 0.17648437, 0.17648774,\n",
       "       0.1764912 , 0.17649476, 0.17649842, 0.17650217, 0.17650602,\n",
       "       0.17650997, 0.17651403])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row is a 2D matrix (hence double-indexing)\n",
    "train_X_tempogram[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the preprocessed X and y\n",
    "It's good practice to persist your preprocessed datasets so that we don't have to recalculate all of the preprocessing (which in large datasets can be time-consuming). \n",
    "\n",
    "A great library for this purpose is the bcolz library (for binary columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the bcolz array saving functions\n",
    "def bcolz_save(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def bcolz_load(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/tensorflow_speech_recognition\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sample/preprocessed'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_sample_preprocessed = os.path.join(path_to_sample, \"preprocessed\")\n",
    "path_to_sample_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory if it's not there already\n",
    "# !mkdir $path_to_sample_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the y\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_y\" + \".bc\", train_y)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_y\" + \".bc\", cv_y)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_y\" + \".bc\", test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the X\n",
    "# raw data\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X\" + \".bc\", train_X)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X\" + \".bc\", cv_X)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X\" + \".bc\", test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs (1dim and 2dim)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_mfccs_1D\" + \".bc\", train_X_mfccs_1D)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_mfccs_1D\" + \".bc\", cv_X_mfccs_1D)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_mfccs_1D\" + \".bc\", test_X_mfccs_1D)\n",
    "\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_mfccs_2D\" + \".bc\", train_X_mfccs_2D)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_mfccs_2D\" + \".bc\", cv_X_mfccs_2D)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_mfccs_2D\" + \".bc\", test_X_mfccs_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel spectrogram\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_mel_spectrogram\" + \".bc\", train_X_mel_spectrogram)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_mel_spectrogram\" + \".bc\", cv_X_mel_spectrogram)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_mel_spectrogram\" + \".bc\", test_X_mel_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_fft\" + \".bc\", train_X_fft)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_fft\" + \".bc\", cv_X_fft)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_fft\" + \".bc\", test_X_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempogram\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_tempogram\" + \".bc\", train_X_tempogram)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_tempogram\" + \".bc\", cv_X_tempogram)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_tempogram\" + \".bc\", test_X_tempogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the preprocessed X and y\n",
    "In order not to have to re-run the entire notebook to obtain the preprocessed X and the corresponding y matrices, let's reload them and then proceed to train simple models.\n",
    "\n",
    "If you're reloading the X & y after restarting the notebook you will also have to run the cells that define the bcolz functions and the path names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the y\n",
    "train_y = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_y\" + \".bc\")\n",
    "cv_y = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_y\" + \".bc\")\n",
    "test_y = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_y\" + \".bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 12)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload the X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 16000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the X\n",
    "# raw data\n",
    "train_X = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X\" + \".bc\")\n",
    "cv_X = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X\" + \".bc\")\n",
    "test_X = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X\" + \".bc\")\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 100)\n",
      "(240, 100, 32)\n"
     ]
    }
   ],
   "source": [
    "# MFCCs (1D and 2D)\n",
    "train_X_mfccs_1D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_mfccs_1D\" + \".bc\")\n",
    "cv_X_mfccs_1D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_mfccs_1D\" + \".bc\")\n",
    "test_X_mfccs_1D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_mfccs_1D\" + \".bc\")\n",
    "print(train_X_mfccs_1D.shape)\n",
    "\n",
    "train_X_mfccs_2D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_mfccs_2D\" + \".bc\")\n",
    "cv_X_mfccs_2D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_mfccs_2D\" + \".bc\")\n",
    "test_X_mfccs_2D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_mfccs_2D\" + \".bc\")\n",
    "print(train_X_mfccs_2D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 128, 32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mel spectrogram\n",
    "train_X_mel_spectrogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_mel_spectrogram\" + \".bc\")\n",
    "cv_X_mel_spectrogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_mel_spectrogram\" + \".bc\")\n",
    "test_X_mel_spectrogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_mel_spectrogram\" + \".bc\")\n",
    "train_X_mel_spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 16000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FFT\n",
    "train_X_fft = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_fft\" + \".bc\")\n",
    "cv_X_fft = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_fft\" + \".bc\")\n",
    "test_X_fft = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_fft\" + \".bc\")\n",
    "train_X_fft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 384, 32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tempogram\n",
    "train_X_tempogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_tempogram\" + \".bc\")\n",
    "cv_X_tempogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_tempogram\" + \".bc\")\n",
    "test_X_tempogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_tempogram\" + \".bc\")\n",
    "train_X_tempogram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train simple models\n",
    "We will start by training the simplest models and then try out more and more complex architectures, aiming for the highest possible accuracy and F1 score.\n",
    "\n",
    "The simplest model we can try is a linear model, which we can obtain by using the Keras Dense layer followed by an activation function such as softmax (as in our case categories are mutually exclusive).\n",
    "\n",
    "Since we have 12 mutually exclusive categories, we need to get an **accuracy of more than 0.833%** to beat random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model\n",
    "We'll need to keep track of the dimensions that we pass into our models, so lets assign their values to separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features: 16000\n",
      "Categories to predict: 12\n"
     ]
    }
   ],
   "source": [
    "# we'll need the number of parameters and the output categories\n",
    "num_features = train_X.shape[1]\n",
    "num_categories = train_y.shape[1]\n",
    "print(\"Input features: {}\\nCategories to predict: {}\".format(num_features, num_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design & compile the model\n",
    "linear_model = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = num_categories, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "linear_model.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on random weights initialization (values will change everytime you compile the model)\n",
      "Categorical crossentropy (loss): 14.7749\n",
      "Accuracy: 0.08\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate our loss before fitting the model\n",
    "initial_score = linear_model.evaluate(test_X, test_y, verbose=0)\n",
    "categorical_crossentropy = initial_score[0]\n",
    "accuracy = initial_score[1]\n",
    "\n",
    "print(\"Based on random weights initialization (values will change everytime you compile the model)\\nCategorical crossentropy (loss): {:.4f}\\nAccuracy: {:.2f}\".format(categorical_crossentropy, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our simple linear model for a couple of epochs and see the **F1 score** and **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/5\n",
      "240/240 [==============================] - 0s - loss: 14.2994 - acc: 0.1125 - val_loss: 15.0436 - val_acc: 0.0667\n",
      "Epoch 2/5\n",
      "240/240 [==============================] - 0s - loss: 14.1705 - acc: 0.1208 - val_loss: 15.0436 - val_acc: 0.0667\n",
      "Epoch 3/5\n",
      "240/240 [==============================] - 0s - loss: 14.1033 - acc: 0.1250 - val_loss: 15.0436 - val_acc: 0.0667\n",
      "Epoch 4/5\n",
      "240/240 [==============================] - 0s - loss: 14.1033 - acc: 0.1250 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 5/5\n",
      "240/240 [==============================] - 0s - loss: 14.0362 - acc: 0.1292 - val_loss: 14.7749 - val_acc: 0.0833\n"
     ]
    }
   ],
   "source": [
    "# we pass our training data and our cross-validation data to see if we're not overfitting\n",
    "history = linear_model.fit(train_X, train_y, batch_size=32, epochs=5, validation_data=(cv_X, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scores\n",
      "Train acc: 0.1292\n",
      "CV acc: 0.0833\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "best_training_accuracy = max(history.history[\"acc\"])\n",
    "best_validation_accuracy = max(history.history[\"val_acc\"])\n",
    "print(\"Best scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the random initialization of weights we should have an **accuracy** score within 0.05 and 0.15 on both the training and cross-validation set. Let's also calculate the **F1 score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first use the model to predict the labels\n",
    "pred_cv_y = linear_model.predict(cv_X, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if shape matches expectation (number of examples, number of categories to predict)\n",
    "pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 2.7456033e-25,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use softmax to get a result towards one-hot encoding, but not all rows will necessarily be just zeroes and one 1\n",
    "pred_cv_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before we pass our predictions to the sklearn's f1 score function we need to make sure that all of our rows are actually one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cv_y = utils.one_hot_encode(pred_cv_y)\n",
    "pred_cv_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final linear model CV accuracy via sklearn: 0.0833\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "sk_cv_accuracy = accuracy_score(cv_y, pred_cv_y)\n",
    "print(\"Final linear model CV accuracy via sklearn: {:.4f}\".format(sk_cv_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model f1 score (CV): 0.0789\n"
     ]
    }
   ],
   "source": [
    "# because we're dealing with a mutliclass classification challenge, we need to change the default value of average\n",
    "# (which is binary)\n",
    "cv_f1_score = f1_score(cv_y, pred_cv_y, average=\"weighted\")\n",
    "print(\"Linear model f1 score (CV): {:.4f}\".format(cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, our accuracy and F1 score for the simplest possible model fall within 0.05 - 0.15. This is our earliest benchmark to beat, and it's **not much better than random guessing**, which given 12 categories would give us an accuracy of 0.08333."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "It is also useful to try other ML methods before jumping into neural networks and deep learning. Random Forests are a simple but very often quite effective (and computationally inexpensive) method of obtaining a good benchmark.\n",
    "\n",
    "For the sklearn implementation of Random Forest we actually do not want our target to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the one-hot encoding\n",
    "rf_train_y = utils.reverse_one_hot_encoding(train_y)\n",
    "rf_cv_y = utils.reverse_one_hot_encoding(cv_y)\n",
    "rf_test_y = utils.reverse_one_hot_encoding(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "rand_forest.fit(train_X, rf_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  1.,  6.,  8.,  1.,  3.,  4.,  6.,  4.,  1.,  7., 11., 12.,\n",
       "        3.,  1.,  3.,  9.,  2.,  3.,  8.,  9.,  4.,  1.,  4.,  1.,  5.,\n",
       "        2.,  8.,  5.,  6.,  8.,  3.,  3.,  6.,  1.,  1.,  2.,  8.,  1.,\n",
       "        5.,  6.,  3.,  7.,  1.,  6., 10.,  5.,  2.,  9.,  6., 11.,  4.,\n",
       "       11.,  5.,  6.,  2.,  2.,  9.,  4.,  5.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predicted_cv_y = rand_forest.predict(cv_X)\n",
    "rf_predicted_cv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest f1 score (CV): 0.135\n",
      "Random forest accuracy (CV): 0.133\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and F1 for Random Forest\n",
    "rf_cv_f1_score = f1_score(rf_cv_y, rf_predicted_cv_y, average=\"weighted\")\n",
    "rf_cv_accuracy = accuracy_score(rf_cv_y, rf_predicted_cv_y)\n",
    "\n",
    "print(\"Random forest f1 score (CV): {:.3f}\".format(rf_cv_f1_score))\n",
    "print(\"Random forest accuracy (CV): {:.3f}\".format(rf_cv_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Random Forest method, using only default parameters (except for max depth), we are getting an **F1 score and accuracy around 0.10 - 0.15**.<br/> Slightly better than random, nowhere near good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set benchmark\n",
    "best_cv_acc = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Networks\n",
    "Now that we have a benchmark obtained via simple linear and Random Forest models we can proceed towards trying to outdo it with MLPs and deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP - multi-layer perceptron\n",
    "Let's start with the simplest possible neural network of just 2 dense layers. We'll be working only on the mfccs data from now on, as it tends to produce better results. We will also add **batch normalization** and **dropout** to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design & compile the model\n",
    "num_nodes = 2000\n",
    "mlp = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = num_nodes, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.95),\n",
    "    Dense(num_categories, activation='softmax')\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "mlp.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "240/240 [==============================] - 4s - loss: 8.0927 - acc: 0.0667 - val_loss: 6.1380 - val_acc: 0.1000\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 3s - loss: 6.2010 - acc: 0.1333 - val_loss: 3.4801 - val_acc: 0.1500\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 3s - loss: 6.4988 - acc: 0.1167 - val_loss: 3.1830 - val_acc: 0.1167\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 3s - loss: 6.0004 - acc: 0.1958 - val_loss: 3.0814 - val_acc: 0.1167\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 3s - loss: 5.3949 - acc: 0.2083 - val_loss: 3.0165 - val_acc: 0.1333\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 3s - loss: 4.9335 - acc: 0.1917 - val_loss: 2.9900 - val_acc: 0.1833\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 3s - loss: 4.6548 - acc: 0.2458 - val_loss: 2.9072 - val_acc: 0.1333\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 3s - loss: 4.5483 - acc: 0.3167 - val_loss: 2.9423 - val_acc: 0.1333\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 3s - loss: 4.2918 - acc: 0.2958 - val_loss: 2.9295 - val_acc: 0.1667\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 3s - loss: 4.3409 - acc: 0.3208 - val_loss: 2.8892 - val_acc: 0.1667\n"
     ]
    }
   ],
   "source": [
    "mlp_results = mlp.fit(train_X, train_y, batch_size=32, epochs=10, validation_data=(cv_X, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MLP scores\n",
      "Train acc: 0.3208\n",
      "CV acc: 0.1833\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "best_training_accuracy = max(mlp_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(mlp_results.history[\"val_acc\"])\n",
    "print(\"Best MLP scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "mlp_pred_cv_y = mlp.predict(cv_X, batch_size=32)\n",
    "mlp_pred_cv_y = utils.one_hot_encode(mlp_pred_cv_y)\n",
    "mlp_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy via sklearn (CV): 0.1667\n",
      "MLP f1 score (CV): 0.1654\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "mlp_cv_accuracy = accuracy_score(cv_y, mlp_pred_cv_y)\n",
    "mlp_cv_f1_score = f1_score(cv_y, mlp_pred_cv_y, average=\"weighted\")\n",
    "print(\"MLP accuracy via sklearn (CV): {:.4f}\".format(mlp_cv_accuracy))\n",
    "print(\"MLP f1 score (CV): {:.4f}\".format(mlp_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a simple MLP model reaches a very similar accuracy score to our previous benchmark of 0.15. Both this one and the previous ones can be tuned to reach approximately 0.25 but let's save fine-tuning for when we have a more promising approach - we are also already overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural Networks\n",
    "Let's try adding more layers to capture more complex interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = 4000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.8),\n",
    "    Dense(3000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.8),\n",
    "    Dense(2000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.8),\n",
    "    Dense(num_categories, activation='softmax')\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "dnn.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "240/240 [==============================] - 6s - loss: 5.5305 - acc: 0.0708 - val_loss: 5.9666 - val_acc: 0.0667\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 4s - loss: 4.6614 - acc: 0.1208 - val_loss: 3.6268 - val_acc: 0.0833\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 4s - loss: 4.3953 - acc: 0.0917 - val_loss: 3.5734 - val_acc: 0.0833\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 4s - loss: 4.2388 - acc: 0.1083 - val_loss: 3.6544 - val_acc: 0.1000\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 4s - loss: 4.2759 - acc: 0.1083 - val_loss: 3.3131 - val_acc: 0.1167\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 4s - loss: 4.2772 - acc: 0.1333 - val_loss: 3.3513 - val_acc: 0.1000\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 4s - loss: 3.9489 - acc: 0.1375 - val_loss: 3.4034 - val_acc: 0.1167\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 4s - loss: 3.6630 - acc: 0.1667 - val_loss: 3.2919 - val_acc: 0.1333\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 4s - loss: 3.5995 - acc: 0.2292 - val_loss: 3.1523 - val_acc: 0.1333\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 4s - loss: 3.5796 - acc: 0.2042 - val_loss: 3.0319 - val_acc: 0.1667\n"
     ]
    }
   ],
   "source": [
    "dnn_results = dnn.fit(train_X, train_y, batch_size=64, epochs=10, validation_data=(cv_X, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best DNN scores\n",
      "Train acc: 0.2292\n",
      "CV acc: 0.1667\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "best_training_accuracy = max(dnn_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(dnn_results.history[\"val_acc\"])\n",
    "print(\"Best DNN scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "dnn_pred_cv_y = dnn.predict(cv_X, batch_size=32)\n",
    "dnn_pred_cv_y = utils.one_hot_encode(dnn_pred_cv_y)\n",
    "dnn_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN accuracy via sklearn (CV): 0.1667\n",
      "DNN f1 score (CV): 0.1209\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "dnn_cv_accuracy = accuracy_score(cv_y, dnn_pred_cv_y)\n",
    "dnn_cv_f1_score = f1_score(cv_y, dnn_pred_cv_y, average=\"weighted\")\n",
    "print(\"DNN accuracy via sklearn (CV): {:.4f}\".format(dnn_cv_accuracy))\n",
    "print(\"DNN f1 score (CV): {:.4f}\".format(dnn_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Models\n",
    "Seems we're stuck around 0.15 accuracy. That makes sense because the actual \"no\" and other words may come at any place in the vector, we can't really keep being attached to specific indexes when training (which we currently are). Let's try convolutional layers, which can find certain patterns regardless of whether they appear at the start or end of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11606,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr, tmp = utils.get_wav_info(\"data/sample/train/stop/01b4757a_nohash_0.wav\")\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.91494527e+02,  7.97293842e+02,  7.96024977e+02,\n",
       "         8.13325323e+02,  8.29413353e+02,  8.38056295e+02,\n",
       "         8.33492077e+02,  8.20852905e+02,  8.92750655e+02,\n",
       "         9.64842738e+02,  9.80583510e+02,  9.79446237e+02,\n",
       "         9.71215749e+02,  9.69958534e+02,  9.64338481e+02,\n",
       "         9.51096909e+02,  8.99682175e+02,  8.45057228e+02,\n",
       "         8.39876141e+02,  8.52041665e+02,  8.64775350e+02,\n",
       "         9.02867648e+02,  8.82603551e+02],\n",
       "       [ 7.15329464e+01,  7.46569244e+01,  6.51241728e+01,\n",
       "         4.65793993e+01,  3.24022451e+01,  2.84170819e+01,\n",
       "         3.95254722e+01,  5.62807149e+01,  9.04809104e+01,\n",
       "         1.24341812e+02,  1.36372590e+02,  1.39704835e+02,\n",
       "         1.32579195e+02,  1.26472082e+02,  1.20161508e+02,\n",
       "         1.28909614e+02,  1.25371020e+02,  1.00925500e+02,\n",
       "         9.80818789e+01,  1.08411666e+02,  7.19331180e+01,\n",
       "         1.87766598e+01,  2.47276794e+01],\n",
       "       [ 6.33864938e+00,  5.12257677e+00,  5.64245337e-01,\n",
       "        -6.88541402e-01,  9.40372055e+00,  1.76124739e+01,\n",
       "         1.88578988e+01,  1.14818961e+00, -1.35409356e+01,\n",
       "        -4.82457893e+01, -7.65945234e+01, -8.01413770e+01,\n",
       "        -7.97954228e+01, -8.09569058e+01, -7.71250620e+01,\n",
       "        -6.01493152e+01, -2.98997537e+01, -1.29988902e+01,\n",
       "        -1.37089271e+01, -2.75775707e+00,  2.13203004e+01,\n",
       "         1.26704110e+01,  1.02475743e+01],\n",
       "       [-4.98627405e+00, -6.72064250e+00, -7.99470845e+00,\n",
       "         1.31216898e+00,  8.47553046e+00,  7.10933788e+00,\n",
       "         4.79326935e+00,  3.41340706e+00,  6.33753617e+00,\n",
       "         8.07797726e+00, -4.16940524e+00, -1.67807004e+01,\n",
       "        -1.59721120e+01, -1.37051816e+01, -1.29109814e+01,\n",
       "        -1.69343677e+01, -2.17056360e+01, -2.33022340e+01,\n",
       "        -1.94467002e+01, -2.91968111e+00,  1.05721276e+01,\n",
       "         1.09596215e+01,  7.46136672e+00],\n",
       "       [-1.92093627e+01, -1.77479439e+01, -1.99354654e+01,\n",
       "        -3.00961191e+01, -3.08623983e+01, -3.52324211e+01,\n",
       "        -3.64150229e+01, -3.18496313e+01, -3.73817543e+01,\n",
       "        -3.37212016e+01, -2.79828741e+01, -2.98356890e+01,\n",
       "        -3.12872593e+01, -4.08430273e+01, -4.28273518e+01,\n",
       "        -4.23537397e+01, -3.64700819e+01, -3.01866973e+01,\n",
       "        -2.17958915e+01, -8.33628142e+00, -1.87150653e+01,\n",
       "        -2.67516912e+01, -2.81682440e+01],\n",
       "       [ 4.16717881e+00,  5.27307839e+00,  6.08515682e+00,\n",
       "         9.94107628e+00,  1.43334004e+01,  1.09928191e+01,\n",
       "         8.66399527e+00,  8.36776331e+00,  7.20217802e+00,\n",
       "        -3.12400184e+00, -1.29801531e+01, -1.48075362e+01,\n",
       "        -1.35795938e+01, -1.30486840e+01, -1.33998135e+01,\n",
       "        -1.44880658e+01, -1.11641680e+01, -5.48267021e+00,\n",
       "        -4.64674417e-01,  1.02924580e+01,  1.72453207e+01,\n",
       "         1.68774021e+01,  1.43156035e+01],\n",
       "       [-8.24938375e+00, -1.38677871e+01, -1.51159525e+01,\n",
       "        -1.14884181e+01, -2.11988312e+01, -2.68820808e+01,\n",
       "        -2.33286343e+01, -2.02706264e+01, -3.82971470e+01,\n",
       "        -3.24179961e+01, -1.85615072e+01, -1.44263330e+01,\n",
       "        -1.50236464e+01, -9.55065508e+00, -1.16364660e+01,\n",
       "        -1.23017025e+01, -1.23784589e+01, -1.61609367e+01,\n",
       "        -2.10017172e+01, -2.36397154e+01, -2.09296052e+01,\n",
       "        -2.08515789e+01, -2.26119920e+01],\n",
       "       [ 9.53456995e+00,  2.84367655e+00, -6.06963061e-01,\n",
       "         6.75096306e-02, -6.55766845e+00, -8.80346174e+00,\n",
       "        -8.35778431e+00, -8.78412996e+00, -3.77899101e+01,\n",
       "        -4.76500193e+01, -3.85216729e+01, -2.75114612e+01,\n",
       "        -3.04075253e+01, -3.08627830e+01, -2.94394270e+01,\n",
       "        -2.92152522e+01, -1.91783806e+01, -5.78906175e+00,\n",
       "        -8.83887100e+00, -1.35419613e+01, -1.02481866e+01,\n",
       "        -1.14143581e+01, -1.18966999e+01],\n",
       "       [-2.96896584e+00, -6.34475796e+00, -6.60959957e+00,\n",
       "        -3.52461273e+00, -8.43177818e+00, -5.80336097e+00,\n",
       "        -1.82221396e+00, -4.83168011e+00, -1.40896261e+01,\n",
       "        -1.99406301e+01, -2.25844129e+01, -2.48534447e+01,\n",
       "        -2.63513910e+01, -2.44818139e+01, -2.43531201e+01,\n",
       "        -3.16182531e+01, -3.19385835e+01, -2.37005458e+01,\n",
       "        -1.65191930e+01, -1.13923285e+01, -8.48130064e+00,\n",
       "        -6.31352729e+00, -5.44635290e+00],\n",
       "       [ 8.05602832e+00,  8.01933964e+00,  1.98109057e+00,\n",
       "        -3.60115002e+00, -3.61108052e+00, -2.92287527e+00,\n",
       "        -4.77097195e+00, -2.67824190e+00,  1.40013355e+01,\n",
       "         2.58964209e+01,  3.18500505e+01,  2.92169310e+01,\n",
       "         2.67406130e+01,  2.43335569e+01,  2.22227253e+01,\n",
       "         1.57833855e+01,  9.47207558e+00,  4.09521695e-01,\n",
       "         6.44250138e+00,  1.94633021e+01,  1.47761737e+01,\n",
       "         1.88183458e+01,  2.03511724e+01],\n",
       "       [-6.31712616e+00, -1.08914920e+01, -1.95260269e+01,\n",
       "        -1.93869283e+01, -1.83774875e+01, -2.20791025e+01,\n",
       "        -1.91253284e+01, -1.90120947e+01, -1.43035209e+01,\n",
       "        -1.20192840e+01, -1.09448798e+01, -7.77161522e+00,\n",
       "        -7.19693864e+00, -7.55469404e+00, -7.05120004e+00,\n",
       "        -1.03163550e+01, -1.13100762e+01, -1.00643687e+01,\n",
       "        -1.25043886e+01, -1.00828756e+01, -1.35223312e+01,\n",
       "        -9.06987223e+00, -4.53305960e+00],\n",
       "       [ 2.18171811e+00,  4.54614471e+00,  1.58801182e+00,\n",
       "        -1.09452470e+00,  2.40902383e+00,  9.53162000e+00,\n",
       "         6.15178895e+00, -3.32991526e+00, -2.54928201e+00,\n",
       "         1.58612934e+00, -1.06133431e+00, -6.89252254e+00,\n",
       "        -6.46863008e+00, -5.09420457e+00, -6.51838454e+00,\n",
       "        -6.27483401e+00, -1.13190063e+00,  5.07191380e+00,\n",
       "        -1.13996175e+00, -3.27105205e+00,  2.09904550e+00,\n",
       "         5.53282251e+00,  1.25898590e+01],\n",
       "       [-6.24926873e+00, -4.53859226e+00, -9.47349631e+00,\n",
       "        -1.62023318e+01, -1.86316169e+01, -1.40454587e+01,\n",
       "        -1.04055066e+01, -9.90936786e+00, -7.89188764e+00,\n",
       "        -8.64236809e+00, -1.02771511e+01, -1.56610962e+01,\n",
       "        -1.55331106e+01, -1.33496522e+01, -1.33922347e+01,\n",
       "        -1.46621163e+01, -1.57072016e+01, -1.73389429e+01,\n",
       "        -1.10638728e+01, -6.18918918e+00, -1.02233788e+01,\n",
       "        -7.43820036e+00,  7.16721029e-01],\n",
       "       [ 4.51588735e+00,  4.36562794e+00,  2.93567907e+00,\n",
       "         5.30020601e+00,  4.14594700e+00,  2.36489829e+00,\n",
       "         5.07139083e+00,  8.42666096e+00,  1.39927494e-01,\n",
       "        -7.82885107e+00, -9.26745717e+00, -1.05333366e+01,\n",
       "        -1.04997757e+01, -7.49424033e+00, -5.00079101e+00,\n",
       "        -7.48369698e+00, -7.01898762e+00, -6.89582834e+00,\n",
       "         3.17322803e+00,  3.14135202e+00,  2.92590679e-02,\n",
       "         2.48137064e+00,  1.06989169e+01],\n",
       "       [-2.73521113e+00, -4.38444135e+00, -7.62212732e+00,\n",
       "        -1.43631537e+01, -1.49874241e+01, -1.40740374e+01,\n",
       "        -1.74640770e+01, -1.75180225e+01, -3.08397540e+01,\n",
       "        -2.84688641e+01, -2.05277786e+01, -2.31353923e+01,\n",
       "        -2.99919818e+01, -3.09331641e+01, -2.92772727e+01,\n",
       "        -2.89672209e+01, -2.24090196e+01, -1.48901250e+01,\n",
       "        -8.73364438e+00, -1.28110232e+01, -5.41585283e+00,\n",
       "         1.66303900e+00,  9.91927744e+00],\n",
       "       [ 6.07266018e+00,  6.75895519e+00,  6.40463872e+00,\n",
       "         6.37355994e+00,  6.52182644e+00,  9.75215094e+00,\n",
       "         5.09211047e+00,  3.09038030e-01,  1.21608161e-01,\n",
       "        -2.50283689e+00, -6.73134729e+00, -4.12496814e+00,\n",
       "        -5.37424372e+00, -4.25552821e+00, -4.09290120e+00,\n",
       "        -8.34246480e+00, -1.07751878e+01, -6.51802582e+00,\n",
       "         2.49968842e+00,  6.49628852e+00, -3.91732257e+00,\n",
       "        -7.34496504e+00,  2.01621195e+00],\n",
       "       [-5.77516401e+00, -7.19268412e+00, -1.16180016e+01,\n",
       "        -6.51975011e+00, -7.62157311e+00, -9.82562654e+00,\n",
       "        -1.15679250e+01, -9.45511489e+00, -1.59733635e+01,\n",
       "        -1.64949537e+01, -1.32518028e+01, -1.08857636e+01,\n",
       "        -1.14618877e+01, -6.89752076e+00, -7.09317966e+00,\n",
       "        -1.19763159e+01, -1.39177420e+01, -1.16210905e+01,\n",
       "        -1.10339600e+01, -1.02574868e+01, -9.45149237e+00,\n",
       "        -1.09752132e+01, -3.60375605e+00],\n",
       "       [ 8.99642895e+00,  3.20803294e+00, -3.70585281e+00,\n",
       "         1.61071879e+00,  5.80680840e-01, -7.09877205e-02,\n",
       "        -5.06583464e-01, -2.42991195e+00, -1.85199635e+01,\n",
       "        -1.12162988e+01,  8.12169187e-01,  7.05761932e+00,\n",
       "         4.42729613e+00,  1.13303311e+00,  2.80439784e+00,\n",
       "         1.62239541e+00,  8.63328946e-01, -1.89316101e+00,\n",
       "        -5.38256561e-01, -2.52599234e+00, -4.44946667e+00,\n",
       "        -4.50568931e+00,  6.36739156e-01],\n",
       "       [ 3.37162888e+00,  5.04023598e-01, -4.31547568e+00,\n",
       "         1.52374048e+00,  3.57741226e+00,  5.51436280e-01,\n",
       "        -2.67747364e+00, -1.00219502e+00, -5.57097467e+00,\n",
       "        -9.35756947e+00, -1.07317182e+01, -9.08926565e+00,\n",
       "        -9.90902240e+00, -9.76289133e+00, -9.87461527e+00,\n",
       "        -1.46257445e+01, -1.39728860e+01, -1.32953125e+01,\n",
       "        -1.01509977e+01, -1.54467202e+01, -1.62201633e+01,\n",
       "        -1.03037481e+01, -5.47296037e+00],\n",
       "       [ 9.99091001e+00,  9.62061861e+00,  1.02524877e+01,\n",
       "         1.01247311e+01,  1.23090845e+01,  8.21302957e+00,\n",
       "         8.08277710e+00,  7.68548182e+00, -2.44164399e+00,\n",
       "        -3.82266056e+00, -1.07991402e+00, -4.41490039e+00,\n",
       "        -9.32972649e+00, -9.98061509e+00, -9.21841240e+00,\n",
       "        -1.25877858e+01, -8.57756671e+00,  8.94100237e-01,\n",
       "         3.65753484e+00, -1.24513848e+00,  5.00961557e-01,\n",
       "         1.69225189e+00,  8.02072885e+00]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_tmp = librosa.feature.mfcc(tmp, sr)\n",
    "mfcc_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 16000, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to use convolutions we have reshape our X -> expand it to 3 dimensions\n",
    "conv_train_X_mfccs = np.expand_dims(train_X_mfccs, axis=2)\n",
    "conv_train_X_mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for cv & test\n",
    "conv_cv_X_mfccs = np.expand_dims(cv_X_mfccs, axis=2)\n",
    "conv_test_X_mfccs = np.expand_dims(test_X_mfccs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = Sequential([\n",
    "        Convolution1D(input_shape=(num_features, 1), kernel_size=32, filters=8, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling1D(),\n",
    "        Convolution1D(kernel_size=64, filters=16, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(500, activation=\"relu\"),\n",
    "        Dropout(.6),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "cnn1.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN architecture should get to 0.367 accuracy around the 35 epoch and then start to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/60\n",
      "240/240 [==============================] - 8s - loss: 4.8245 - acc: 0.0750 - val_loss: 2.5706 - val_acc: 0.1167\n",
      "Epoch 2/60\n",
      "240/240 [==============================] - 8s - loss: 2.5976 - acc: 0.1125 - val_loss: 2.4629 - val_acc: 0.1167\n",
      "Epoch 3/60\n",
      "240/240 [==============================] - 7s - loss: 2.4707 - acc: 0.1208 - val_loss: 2.4688 - val_acc: 0.0833\n",
      "Epoch 4/60\n",
      "128/240 [===============>..............] - ETA: 3s - loss: 2.4641 - acc: 0.1484"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-d58c45fd8bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn1_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_train_X_mfccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_cv_X_mfccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1193\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn1_results = cnn1.fit(conv_train_X_mfccs, train_y, batch_size=32, epochs=60, validation_data=(conv_cv_X_mfccs, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn1_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-60765ad6f073>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# show best results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_training_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn1_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbest_validation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn1_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best CNN 1 scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_training_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_validation_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn1_results' is not defined"
     ]
    }
   ],
   "source": [
    "# show best results\n",
    "best_training_accuracy = max(cnn1_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(cnn1_results.history[\"val_acc\"])\n",
    "print(\"Best CNN 1 scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "cnn1_pred_cv_y = cnn1.predict(conv_cv_X_mfccs, batch_size=32)\n",
    "cnn1_pred_cv_y = utils.one_hot_encode(cnn1_pred_cv_y)\n",
    "cnn1_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN 1 accuracy via sklearn (CV): 0.1500\n",
      "CNN 1 f1 score (CV): 0.0981\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "cnn1_cv_accuracy = accuracy_score(cv_y, cnn1_pred_cv_y)\n",
    "cnn1_cv_f1_score = f1_score(cv_y, cnn1_pred_cv_y, average=\"weighted\")\n",
    "print(\"CNN 1 accuracy via sklearn (CV): {:.4f}\".format(cnn1_cv_accuracy))\n",
    "print(\"CNN 1 f1 score (CV): {:.4f}\".format(cnn1_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the kernel size - patterns in speech mighr require more than e.g. 32 single samplings to be recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = Sequential([\n",
    "        Convolution1D(input_shape=(num_features, 1), kernel_size=256, filters=32, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(),\n",
    "        Convolution1D(kernel_size=512, filters=32, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(500, activation=\"relu\"),\n",
    "        Dropout(.6),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "cnn2.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/50\n",
      "240/240 [==============================] - 169s - loss: 6.7903 - acc: 0.0458 - val_loss: 2.4849 - val_acc: 0.0833\n",
      "Epoch 2/50\n",
      " 64/240 [=======>......................] - ETA: 115s - loss: 2.4854 - acc: 0.0469"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-991a5fec6adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn2_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_train_X_mfccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_cv_X_mfccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1193\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensor_voice_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn2_results = cnn2.fit(conv_train_X_mfccs, train_y, batch_size=32, epochs=50, validation_data=(conv_cv_X_mfccs, cv_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Models\n",
    "We can also try to take advantage of the architectures specifically designed for time sequences: RNNs. We will start with the basic keras implementations of simple RNN and then move on to GRUs & LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_1 = Sequential([\n",
    "        SimpleRNN(input_shape=(num_features, 1), units=100, activation='relu'),\n",
    "        Dense(500, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.7),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "rnn_1.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_1_results = rnn_1.fit(conv_train_X_mfccs, train_y, batch_size=32, epochs=50, validation_data=(conv_cv_X_mfccs, cv_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action plan\n",
    "X) turn the sample data into numpy arrays with X and y normally <br>\n",
    "X) turn sample data into numpy arrays with X and y via mfccs<br>\n",
    "X) Use linear model? (towards first benchmark)<br>\n",
    "X) Use random forest?<br>\n",
    "X) Use MLP<br>\n",
    "X) Use multiple dense layers<br>\n",
    "4c) Use convolutions (try the increased kernel sie that takes 400s per epoch)<br>\n",
    "4d) USE RNN -> like in Nietzsche [https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/]<br>\n",
    "5) Add preprocessing and test a couple of the best models<br>\n",
    "\n",
    "6) Consider splitting the work on images into separate notebook depending on how bulky this gets<br>\n",
    "7) Experiments on images without data augmentation<br>\n",
    "8) Experiments on images with data augmentation<br>\n",
    "\n",
    "9) Decide on e.g. 3 most promising methods<br>\n",
    "\n",
    "And then:<br>\n",
    "10) Move to writing the most promising models in tensorflow<br>\n",
    "11) Include tensorboard visualization of training & graph<br>\n",
    "12) Code for turning results into kaggle format of results to get score<br>\n",
    "13) Obtain a good score on kaggle<br>\n",
    "14) Re-read everything from start to finish and adjust<br>\n",
    "15) Write a good Readme for markdown<br>\n",
    "16) Add to CV<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
