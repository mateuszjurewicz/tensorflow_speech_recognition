{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model experiments - sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks we have separated a small subset of our data, called \"sample\", on which we can now experiment with simple models to assess the effectiveness of our preprocessing & data augmentation techniques.\n",
    "\n",
    "We do it this way to avoid spending too much time on training on the entire set, the assumption is that the methods which are effective on the sample will work well on a larger scale too. \n",
    "\n",
    "We will start by testing a couple of simple models on untouched sample data (as numpy arrays) and then proceed towards data augmentation and finally spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/mateusz/Documents/Mateusz/Career/Machine Learning & AI/tensorflow_speech_recognition/tensorflow_speech_recognition\n"
     ]
    }
   ],
   "source": [
    "# first make sure we're in the parent dictory of our data/sample folders.\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "We'll need a couple of additional libraries so let's import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'C:\\\\Users\\\\mateusz\\\\Documents\\\\Mateusz\\\\Career\\\\Machine Learning & AI\\\\tensorflow_speech_recognition\\\\tensorflow_speech_recognition\\\\utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow\n",
    "\n",
    "# keras as tensorflow backend\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, BatchNormalization, Dropout, Convolution1D, MaxPooling1D, Flatten\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "# F1 and accuracy score metric\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# utils\n",
    "from importlib import reload\n",
    "import utils; reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "The easiest way to work with data is by turning it into a list of numbers, in our case a numpy array. We can use one of the functions from utils to load the raw data or use the librosa.load() function. The difference lies in the fact that the former returns int16s whereas librosa returns float32s and uses its default sampling rate of 22050Hz, unless we explicitly tell it to use the file's original sampling rate of 16000Hz.\n",
    "\n",
    "We should also consider normalizing our data (so that it all falls within the same scale) and extracting a 1D mel-frequency cepstrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample = \"data\\\\sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to go through each of the folders in our sample/train, cv and test sets, one-hot encode their label and load the 16K long array of raw data. The y data will be of shape (m, 12), where m is the number of examples, and the X data will be of shape (m, 16000).\n",
    "\n",
    "Let's calculate **m** first. We will do that by using a function that create a list of all the .wav files within a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of paths\n",
    "We will use the glob module that we learned about in the very first notebook and a function from util.py which can, given a directory, return a list of paths to .wav files within it. We will repeat the process for all 3 sets within sample, and every category subdirectory within those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\sample\\\\train\\\\stop\\\\01bcfc0c_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\17cc40ee_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\2da58b32_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\2da58b32_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\311fde72_nohash_2.wav']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example we can grab all .wav files from sample/train/stop\n",
    "path_to_sample_train_stop = os.path.join(path_to_sample, \"train\", \"stop\")\n",
    "utils.grab_wavs(path_to_sample_train_stop)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need a list of all category folder names\n",
    "categories_to_predict = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"silence\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\sample\\\\train\\\\yes\\\\023a61ad_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\0f3f64d5_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\190821dc_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\28ed6bc9_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\324210dd_nohash_5.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\32561e9e_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\3fdafe25_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\48e8b82a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\493392c6_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\589bce2c_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\5c237956_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\65c73b55_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\89f680f3_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\953fe1ad_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\b43de700_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\b7669804_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\e48a80ed_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\f5c3de1b_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\f839238a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\yes\\\\f953e1af_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\0362539c_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\0362539c_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\0bd689d7_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\190821dc_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\1bb574f9_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\53fd1780_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\87728a88_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\9637f43f_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\a902ce9b_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\b80d6c5e_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\be7fa0a3_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\bf8d5617_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\c1d39ce8_nohash_6.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\cf8d91cf_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\d0426d63_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\d7559b2d_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\d78858d9_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\e41e41f7_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\e4a2cf79_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\no\\\\ee07dcb9_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\21307344_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\32561e9e_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\39999a0f_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\39ec87ac_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\3b8406c0_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\4a294341_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\5677ec77_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\569455ff_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\611d2b50_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\61d3e51e_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\7211390b_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\763188c4_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\7f74626f_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\8281a2a8_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\982babaf_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\b3bdded5_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\c0fb6812_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\f06190c1_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\f92e49f3_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\up\\\\ffbb695d_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\10ace7eb_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\1626bc5a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\29fb33da_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\3411cf4b_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\3c257192_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\3ea77ede_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\4954abe8_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\617de221_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\87014d40_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\a97017df_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\cb8f8307_nohash_5.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\d2f4f431_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\db7c95b0_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\e3e49931_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\eb6dab4a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\ec201020_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\f19c1390_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\f59d0771_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\f839238a_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\down\\\\fad7a69a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\1cec8d71_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\24694eb6_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\2df590cd_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\338dacf5_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\531a5b8a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\686d030b_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\6ef76186_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\743edf9d_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\80c17118_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\88120683_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\b2fbe484_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\bbbf4fbd_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\c79159aa_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\cd8b1781_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\dbb7723a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\e102119e_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\e41e41f7_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\e57d35bc_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\ec201020_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\left\\\\fb9d6d23_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\06a79a03_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\0c5027de_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\0d2bcf9d_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\3c257192_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\4beff0c5_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\62581901_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\6e74c582_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\784e281a_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\92a9c5e6_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\97e0c576_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\a4baac4e_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\aeb99b1c_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\b4aa9fef_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\c68cf200_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\cb8f8307_nohash_6.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\d2f4f431_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\d9b50b8b_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\dbb7723a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\dea820ce_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\right\\\\f810e799_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\21e8c417_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\2bd2cad5_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\34e8c726_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\3fb8c268_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\408de0a4_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\48a9f771_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\51055bda_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\617de221_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\8931f494_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\89ed36ab_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\8c3c4715_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\8eb4a1bf_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\96c66ab7_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\c9b653a0_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\cae62f38_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\cb8f8307_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\d33df435_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\e0344f60_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\f19d1738_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\on\\\\f618568f_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\300384f0_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\305776dd_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\35d1b6ee_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\51eefcc6_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\5e033479_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\6727b579_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\7096522d_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\78884794_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\89f3ab7d_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\918a2473_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\96ab6565_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\9f6fbdb4_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\a16b3102_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\b8897f1c_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\cb8f8307_nohash_6.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\d7ca14ef_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\dcc012ec_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\dd6c6806_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\edd8bfe3_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\off\\\\fe5c4a7a_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\01bcfc0c_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\17cc40ee_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\2da58b32_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\2da58b32_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\311fde72_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\36050ef3_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\5184ed3e_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\59fe87e6_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\686d030b_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\7014b07e_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\723efc4c_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\888a0c49_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\888a0c49_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\88a487ce_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\ac4b3fc3_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\d486fb84_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\e7117d00_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\ec201020_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\f06190c1_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\stop\\\\f8f60f59_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\012c8314_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\02746d24_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\10467b06_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\1bb574f9_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\36050ef3_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\364f979f_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\37e8db82_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\4fe01997_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\5628d7b7_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\57b68383_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\6fca237d_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\9a43b64b_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\c1e0e8e3_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\ceaadb24_nohash_0.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\e7ea8b76_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\eb3f7d82_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\f5733968_nohash_1.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\f5733968_nohash_3.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\f864cd4a_nohash_2.wav',\n",
       " 'data\\\\sample\\\\train\\\\go\\\\f92e49f3_nohash_4.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\doing_the_dishes_43.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\doing_the_dishes_46.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\doing_the_dishes_84.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\doing_the_dishes_94.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\exercise_bike_27.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\exercise_bike_35.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\exercise_bike_47.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\exercise_bike_50.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\exercise_bike_57.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\exercise_bike_60.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\pink_noise_45.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\pink_noise_8.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\running_tap_24.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\running_tap_35.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\running_tap_41.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\running_tap_57.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\running_tap_61.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\white_noise_26.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\white_noise_39.wav',\n",
       " 'data\\\\sample\\\\train\\\\silence\\\\white_noise_49.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\0137b3f4_nohash_1_eight.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\1365dd89_nohash_1_bird.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\18a1aab9_nohash_2_house.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\1e9b215e_nohash_0_sheila.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\2ad772d6_nohash_0_sheila.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\3ab9ba07_nohash_0_eight.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\3e31dffe_nohash_2_four.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\637c702a_nohash_1_two.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\64da5281_nohash_0_zero.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\7303215d_nohash_0_six.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\99e6cab8_nohash_0_seven.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\9aa21fa9_nohash_3_nine.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\b59fe16d_nohash_0_six.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\b959cd0c_nohash_2_four.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\c2aeb59d_nohash_0_wow.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\c2df23b2_nohash_0_sheila.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\cdbd6969_nohash_0_five.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\d430b3cc_nohash_1_eight.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\dfb6450b_nohash_0_two.wav',\n",
       " 'data\\\\sample\\\\train\\\\unknown\\\\eb0676ec_nohash_2_nine.wav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first grab the training set\n",
    "path_to_train = os.path.join(path_to_sample, \"train\")\n",
    "sample_train_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_train, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    \n",
    "    # we use extend instead of append to add all elements from the iterable\n",
    "    sample_train_wavs.extend(category_files)\n",
    "    \n",
    "sample_train_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for cv\n",
    "path_to_cv = os.path.join(path_to_sample, \"cv\")\n",
    "sample_cv_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_cv, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    sample_cv_wavs.extend(category_files)\n",
    "\n",
    "# repeat for test\n",
    "path_to_test = os.path.join(path_to_sample, \"test\")\n",
    "sample_test_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_test, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    sample_test_wavs.extend(category_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode the y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the 3 lists of files from each set (train, cv and test) we can construct our train_y, cv_y and test_y numpy arrays. These will be matrices of size (m, 12), one-hot encoded. E.g. if a row belongs to the category \"up\" it will take the form of an array of zeros, where the entry at index 2 (the third from the left) will become a 1.\n",
    "\n",
    "We will use a function from the utils that takes a path to a .wav, the index at which the category name starts within it (we want to control this because we will eventually use this for the main set, not just the sample) and a list of categories to predict. For our current example, the category name in the paths belonging to \"train\" starts at the 18th index (separators count as one char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's grab a single path (this one is an \"up\")\n",
    "a_wav = sample_train_wavs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see if the 1 is correctly placed\n",
    "utils.one_hot_encode_path(a_wav, 18, categories_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path belonged to the first category (\"up\") and the one-hot encoding correctly placed the 1 at index 0.\n",
    "\n",
    "We want to repeat this for all examples in each of the 3 subsets, adding each new one-hot encoded numpy array as a new row of the y matrix, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out the dimensions of train_y\n",
    "rows = len(sample_train_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape: 2880\n",
      "New shape: (240, 12)\n"
     ]
    }
   ],
   "source": [
    "# create train_y as empty array\n",
    "train_y = np.array([])\n",
    "\n",
    "# append each row to train_y\n",
    "for path_to_wav in sample_train_wavs:\n",
    "    row = utils.one_hot_encode_path(path_to_wav, 18, categories_to_predict)\n",
    "    \n",
    "    # append the new row\n",
    "    train_y = np.append(train_y, row)\n",
    "    \n",
    "# we currently have a flattened vector\n",
    "print(\"Current shape: {}\".format(*train_y.shape))\n",
    "\n",
    "# let's reshape it\n",
    "train_y = np.reshape(train_y, dimensions)\n",
    "print(\"New shape: {}\".format(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the train_y matrix to confirm\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first 3 entries have the 1 at 0th index, which means they belong to category \"up\" and the last three have the 1 at the last index, which is also correct given the fact that our list of paths was also ordered.\n",
    "\n",
    "We should bear in mind that by default the np.array contains float64s and our functions for loading a .wav return int16s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for **CV set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dimensions: (60, 12)\n",
      "Current shape: 720\n",
      "New shape: (60, 12)\n"
     ]
    }
   ],
   "source": [
    "# figure out the dimensions\n",
    "rows = len(sample_cv_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "print(\"Target dimensions: {}\".format(dimensions))\n",
    "\n",
    "# empy array\n",
    "cv_y = np.array([])\n",
    "\n",
    "for path_to_wav in sample_cv_wavs:\n",
    "    row = utils.one_hot_encode_path(path_to_wav, 15, categories_to_predict)\n",
    "    \n",
    "    # append the new row\n",
    "    cv_y = np.append(cv_y, row)\n",
    "    \n",
    "# we currently have a flattened vector\n",
    "print(\"Current shape: {}\".format(*cv_y.shape))\n",
    "\n",
    "# let's reshape it\n",
    "cv_y = np.reshape(cv_y, dimensions)\n",
    "print(\"New shape: {}\".format(cv_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for **Test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dimensions: (60, 12)\n",
      "Current shape: 720\n",
      "New shape: (60, 12)\n"
     ]
    }
   ],
   "source": [
    "# figure out the dimensions\n",
    "rows = len(sample_test_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "print(\"Target dimensions: {}\".format(dimensions))\n",
    "\n",
    "# empy array\n",
    "test_y = np.array([])\n",
    "\n",
    "for path_to_wav in sample_test_wavs:\n",
    "    row = utils.one_hot_encode_path(path_to_wav, 17, categories_to_predict)\n",
    "    \n",
    "    # append the new row\n",
    "    test_y = np.append(test_y, row)\n",
    "    \n",
    "# we currently have a flattened vector\n",
    "print(\"Current shape: {}\".format(*test_y.shape))\n",
    "\n",
    "# let's reshape it\n",
    "test_y = np.reshape(test_y, dimensions)\n",
    "print(\"New shape: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the X\n",
    "We have the y - the one-hot encoded vectors representing the category for each training, cv and test example in the sample set. We need the feature vectors, conventionally referred to as X. We will use both the simplest way of extracting the .wav data and the 1D mel frequency cepstrum (mfccs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(librosa.core.load(sample_train_wavs[0], sr=16000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple helper function\n",
    "def get_X_with_padding(list_of_paths, columns=16000):\n",
    "    \n",
    "    # get shape data\n",
    "    rows = len(list_of_paths)\n",
    "    dimensions = (rows, columns)\n",
    "    \n",
    "    # create placeholder\n",
    "    X = np.array([])\n",
    "    \n",
    "    # go through every file path in the list\n",
    "    for path_to_wav in list_of_paths:\n",
    "\n",
    "        # get raw array of signed ints\n",
    "        row = utils.get_wav_info(path_to_wav)[1]\n",
    "        \n",
    "        # some of our sample have less (or slightly more) than 16000 values, so let's adjust them\n",
    "        # trim to fixed length\n",
    "        row = row[:columns]\n",
    "        \n",
    "        # pad with zeros, calculating amount of padding needed\n",
    "        padding = columns - len(row)\n",
    "        row = np.pad(row, (0, padding), mode='constant', constant_values=0)\n",
    "\n",
    "        # append the new row\n",
    "        X = np.append(X, row)\n",
    "    \n",
    "    # reshape (unroll)\n",
    "    X = np.reshape(X, dimensions)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (240, 16000)\n",
      "CV:  (60, 16000)\n",
      "Test:  (60, 16000)\n"
     ]
    }
   ],
   "source": [
    "# get the X for each set\n",
    "train_X = get_X_with_padding(sample_train_wavs)\n",
    "cv_X = get_X_with_padding(sample_cv_wavs)\n",
    "test_X = get_X_with_padding(sample_test_wavs)\n",
    "\n",
    "print(\"Train: \", train_X.shape)\n",
    "print(\"CV: \", cv_X.shape)\n",
    "print(\"Test: \",test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same for the 1D mel frequency cepstrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mfccs:  (240, 16000)\n",
      "CV mfccs:  (60, 16000)\n",
      "Test mfccs:  (60, 16000)\n"
     ]
    }
   ],
   "source": [
    "train_X_mfccs = utils.get_X_with_padding_mfccs(sample_train_wavs)\n",
    "cv_X_mfccs = utils.get_X_with_padding_mfccs(sample_cv_wavs)\n",
    "test_X_mfccs = utils.get_X_with_padding_mfccs(sample_test_wavs)\n",
    "\n",
    "print(\"Train mfccs: \", train_X_mfccs.shape)\n",
    "print(\"CV mfccs: \", cv_X_mfccs.shape)\n",
    "print(\"Test mfccs: \",test_X_mfccs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train simple models\n",
    "We will start by training the simplest models and then try out more and more complex architectures, aiming for the highest possible accuracy and F1 score.\n",
    "\n",
    "The simplest model we can try is a linear model, which we can obtain by using the Keras Dense layer followed by an activation function such as softmax (as in our case categories are mutually exclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model\n",
    "We'll need to keep track of the dimensions that we pass into our models, so lets assign their values to separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features: 16000\n",
      "Categories to predict: 12\n"
     ]
    }
   ],
   "source": [
    "# we'll need the number of parameters and the output categories\n",
    "num_features = train_X.shape[1]\n",
    "num_categories = train_y.shape[1]\n",
    "print(\"Input features: {}\\nCategories to predict: {}\".format(num_features, num_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design & compile the model\n",
    "linear_model = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = num_categories, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "linear_model.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on random weights initialization (values will change everytime you compile the model)\n",
      "Categorical crossentropy (loss): 15.5808\n",
      "Accuracy: 0.03\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate our loss before fitting the model\n",
    "initial_score = linear_model.evaluate(test_X, test_y, verbose=0)\n",
    "categorical_crossentropy = initial_score[0]\n",
    "accuracy = initial_score[1]\n",
    "\n",
    "print(\"Based on random weights initialization (values will change everytime you compile the model)\\nCategorical crossentropy (loss): {:.4f}\\nAccuracy: {:.2f}\".format(categorical_crossentropy, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our simple linear model for a couple of epochs and see the **F1 score** and **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/5\n",
      "240/240 [==============================] - 0s - loss: 14.9092 - acc: 0.0750 - val_loss: 14.7809 - val_acc: 0.0833\n",
      "Epoch 2/5\n",
      "240/240 [==============================] - 0s - loss: 14.9764 - acc: 0.0708 - val_loss: 15.0477 - val_acc: 0.0667\n",
      "Epoch 3/5\n",
      "240/240 [==============================] - 0s - loss: 14.9092 - acc: 0.0750 - val_loss: 15.0466 - val_acc: 0.0667\n",
      "Epoch 4/5\n",
      "240/240 [==============================] - 0s - loss: 14.6659 - acc: 0.0875 - val_loss: 14.7863 - val_acc: 0.0833\n",
      "Epoch 5/5\n",
      "240/240 [==============================] - 0s - loss: 14.3081 - acc: 0.1083 - val_loss: 14.8111 - val_acc: 0.0667\n"
     ]
    }
   ],
   "source": [
    "# we pass our training data and our cross-validation data to see if we're not overfitting\n",
    "history = linear_model.fit(train_X, train_y, batch_size=32, epochs=5, validation_data=(cv_X, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scores\n",
      "Train acc: 0.1083\n",
      "CV acc: 0.0833\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "best_training_accuracy = max(history.history[\"acc\"])\n",
    "best_validation_accuracy = max(history.history[\"val_acc\"])\n",
    "print(\"Best scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the random initialization of weights we should have an **accuracy** score within 0.05 and 0.15 on both the training and cross-validation set. Let's also calculate the **F1 score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first use the model to predict the labels\n",
    "pred_cv_y = linear_model.predict(cv_X, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if shape matches expectation (number of examples, number of categories to predict)\n",
    "pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use softmax to get a result towards one-hot encoding, but not all rows will be just zeroes and one 1\n",
    "pred_cv_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before we pass our predictions to the sklearn's f1 score function we need to make sure that all of our rows are actually one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cv_y = utils.one_hot_encode(pred_cv_y)\n",
    "pred_cv_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final linear model CV accuracy via sklearn: 0.0667\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "sk_cv_accuracy = accuracy_score(cv_y, pred_cv_y)\n",
    "print(\"Final linear model CV accuracy via sklearn: {:.4f}\".format(sk_cv_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model f1 score (CV): 0.0601\n"
     ]
    }
   ],
   "source": [
    "# because we're dealing with a mutliclass classification challenge, we need to change the default value of average\n",
    "# (which is binary)\n",
    "cv_f1_score = f1_score(cv_y, pred_cv_y, average=\"weighted\")\n",
    "print(\"Linear model f1 score (CV): {:.4f}\".format(cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, our accuracy and F1 score for the simplest possible model fall within 0.5 - 0.15. This is our earliest benchmark to beat, and it's not much better than **random guessing**, which given 12 categories would give us an accuracy of 0.08333."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "It is also useful to try other ML methods before jumping into neural networks and deep learning. Random Forests are a simple but very often quite effective (and computationally inexpensive) method of obtaining a good benchmark.\n",
    "\n",
    "For the sklearn implementation of Random Forest we actually do not want our target to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the one-hot encoding\n",
    "rf_train_y = utils.reverse_one_hot_encoding(train_y)\n",
    "rf_cv_y = utils.reverse_one_hot_encoding(cv_y)\n",
    "rf_test_y = utils.reverse_one_hot_encoding(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier(max_depth=7, random_state=0)\n",
    "rand_forest.fit(train_X, rf_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.,  6., 11., 11., 11.,  7.,  2.,  6., 12.,  2.,  1., 12., 10.,\n",
       "        2.,  1.,  1., 11., 10., 11.,  1., 12., 10.,  4.,  1.,  3.,  6.,\n",
       "        4.,  4.,  5.,  1.,  8.,  5.,  7.,  5.,  4.,  1.,  4.,  1.,  6.,\n",
       "        4.,  1.,  4.,  3.,  6.,  3.,  6.,  4.,  3., 12., 12.,  3.,  6.,\n",
       "        7., 11., 11.,  2., 12., 10.,  1.,  5.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predicted_cv_y = rand_forest.predict(cv_X)\n",
    "rf_predicted_cv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest f1 score (CV): 0.115\n",
      "Random forest accuracy (CV): 0.117\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and F1 for Random Forest\n",
    "rf_cv_f1_score = f1_score(rf_cv_y, rf_predicted_cv_y, average=\"weighted\")\n",
    "rf_cv_accuracy = accuracy_score(rf_cv_y, rf_predicted_cv_y)\n",
    "\n",
    "print(\"Random forest f1 score (CV): {:.3f}\".format(rf_cv_f1_score))\n",
    "print(\"Random forest accuracy (CV): {:.3f}\".format(rf_cv_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Random Forest method, using only default parameters (except for a max depth of 7), we are getting an **F1 score and accuracy around 0.11**. Slightly better than random, nowhere near good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCCS Linear Model & Random Forest\n",
    "Let's see if our methods result in a higher score for the 1D mel frequency cepstrum coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design & compile the model\n",
    "mfcc_linear_model = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = num_categories, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "mfcc_linear_model.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCCs\n",
      "Based on random weights initialization (values will change everytime you compile the model)\n",
      "Categorical crossentropy (loss): 7.8995\n",
      "Accuracy: 0.10\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate our loss before fitting the model\n",
    "initial_score = mfcc_linear_model.evaluate(cv_X_mfccs, cv_y, verbose=0)\n",
    "categorical_crossentropy = initial_score[0]\n",
    "accuracy = initial_score[1]\n",
    "\n",
    "print(\"MFCCs\\nBased on random weights initialization (values will change everytime you compile the model)\\nCategorical crossentropy (loss): {:.4f}\\nAccuracy: {:.2f}\".format(categorical_crossentropy, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "240/240 [==============================] - 0s - loss: 5.1898 - acc: 0.1042 - val_loss: 3.9808 - val_acc: 0.0833\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 0s - loss: 2.9077 - acc: 0.1792 - val_loss: 3.0066 - val_acc: 0.1500\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 0s - loss: 2.6075 - acc: 0.2208 - val_loss: 2.6951 - val_acc: 0.1000\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 0s - loss: 2.3432 - acc: 0.2083 - val_loss: 2.5878 - val_acc: 0.1667\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 0s - loss: 2.1040 - acc: 0.2792 - val_loss: 2.4360 - val_acc: 0.1667\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 0s - loss: 1.9938 - acc: 0.3083 - val_loss: 2.3730 - val_acc: 0.1833\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 0s - loss: 1.8965 - acc: 0.3375 - val_loss: 2.2976 - val_acc: 0.1500\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 0s - loss: 1.8188 - acc: 0.3958 - val_loss: 2.2569 - val_acc: 0.2000\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 0s - loss: 1.7680 - acc: 0.4292 - val_loss: 2.2488 - val_acc: 0.2833\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 0s - loss: 1.7073 - acc: 0.4583 - val_loss: 2.2079 - val_acc: 0.2333\n"
     ]
    }
   ],
   "source": [
    "# we pass our training data and our cross-validation data to see if we're not overfitting\n",
    "history = mfcc_linear_model.fit(train_X_mfccs, train_y, batch_size=32, epochs=10, validation_data=(cv_X_mfccs, cv_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly observe that the linear model based on the mfcc data is much better at fitting the training data - getting to a train accuracy of 0.44 after 10 epochs and a validation accuracy of around 0.233 (compared to the raw data linear model not progressing beyond train and cv accuracy of 0.15).\n",
    "\n",
    "Let's run the linear model for 10 more epochs to see if we can get a better cv accuracy, despite clearly already overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "240/240 [==============================] - 0s - loss: 1.6920 - acc: 0.4542 - val_loss: 2.1916 - val_acc: 0.2833\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 0s - loss: 1.6242 - acc: 0.4667 - val_loss: 2.1824 - val_acc: 0.2167\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 0s - loss: 1.6366 - acc: 0.4667 - val_loss: 2.1783 - val_acc: 0.2167\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 0s - loss: 1.5649 - acc: 0.5042 - val_loss: 2.1738 - val_acc: 0.3667\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 0s - loss: 1.5458 - acc: 0.5042 - val_loss: 2.1543 - val_acc: 0.2833\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 0s - loss: 1.5096 - acc: 0.5500 - val_loss: 2.1882 - val_acc: 0.3333\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 0s - loss: 1.5095 - acc: 0.5250 - val_loss: 2.1833 - val_acc: 0.2833\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 0s - loss: 1.4727 - acc: 0.5583 - val_loss: 2.1735 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 0s - loss: 1.4491 - acc: 0.5667 - val_loss: 2.1902 - val_acc: 0.3167\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 0s - loss: 1.4481 - acc: 0.5708 - val_loss: 2.1983 - val_acc: 0.2833\n"
     ]
    }
   ],
   "source": [
    "# 10 more epochs\n",
    "history = mfcc_linear_model.fit(train_X_mfccs, train_y, batch_size=32, epochs=10, validation_data=(cv_X_mfccs, cv_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we managed to reach a cv accuracy of over 0.3. If you do further experiments you can also stop on an epoch where the cv accuracy was around 0.35.\n",
    "\n",
    "Let's see what accuracy and F1 score we can obtain from this model on the cv set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first use the model to predict the labels\n",
    "mfccs_pred_cv_y = mfcc_linear_model.predict(cv_X_mfccs, batch_size=32)\n",
    "mfccs_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it's one-hot encoded\n",
    "mfccs_pred_cv_y = utils.one_hot_encode(mfccs_pred_cv_y)\n",
    "mfccs_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCCs Linear model accuracy via sklearn (CV): 0.2833\n",
      "MFCCs Linear model f1 score (CV): 0.2476\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "mfccs_cv_accuracy = accuracy_score(cv_y, mfccs_pred_cv_y)\n",
    "mfccs_cv_f1_score = f1_score(cv_y, mfccs_pred_cv_y, average=\"weighted\")\n",
    "print(\"MFCCs Linear model accuracy via sklearn (CV): {:.4f}\".format(mfccs_cv_accuracy))\n",
    "print(\"MFCCs Linear model f1 score (CV): {:.4f}\".format(mfccs_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a new benchmark - the **linear model based on the mfccs** has an accuracy and F1 score of around **0.3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the random forest approach on the mfccs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the random forest and fit it to mfcc X\n",
    "mfccs_rand_forest = RandomForestClassifier(max_depth=7, random_state=0)\n",
    "mfccs_rand_forest.fit(train_X_mfccs, rf_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  4.,  8.,  4.,  5., 12.,  9.,  2., 11.,  4.,  6., 11.,  9.,\n",
       "        7.,  5.,  4.,  4.,  3.,  2., 11., 10.,  1.,  1.,  7.,  5.,  9.,\n",
       "        5.,  4.,  4., 10.,  1.,  7.,  3.,  8.,  4.,  8.,  8.,  8.,  2.,\n",
       "        9., 12., 12., 12.,  4.,  5., 12., 10.,  7.,  2.,  5., 11., 11.,\n",
       "       11., 11., 11.,  2., 12.,  9., 12.,  3.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs_rf_predicted_cv_y = mfccs_rand_forest.predict(cv_X_mfccs)\n",
    "mfccs_rf_predicted_cv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest f1 score (CV): 0.235\n",
      "Random forest accuracy (CV): 0.267\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and F1 for Random Forest\n",
    "mfccs_rf_cv_f1_score = f1_score(rf_cv_y, mfccs_rf_predicted_cv_y, average=\"weighted\")\n",
    "mfccs_rf_cv_accuracy = accuracy_score(rf_cv_y, mfccs_rf_predicted_cv_y)\n",
    "\n",
    "print(\"Random forest f1 score (CV): {:.3f}\".format(mfccs_rf_cv_f1_score))\n",
    "print(\"Random forest accuracy (CV): {:.3f}\".format(mfccs_rf_cv_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model with all default parameters and a max_depth of 7 is able to make more accurate predictions based on the mfccs data than on the raw data, but not more accuracte than the linear model.\n",
    "\n",
    "Let's experiment a little with some of the other parameters of our Random Forest to see if we can get a better result than our mfccs linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features=3000, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the random forest and fit it to mfcc X\n",
    "optimized_mfccs_rand_forest = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=7,\n",
    "    max_features = 3000,\n",
    "    random_state=0)\n",
    "optimized_mfccs_rand_forest.fit(train_X_mfccs, rf_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.,  4.,  7.,  7.,  1.,  5., 12., 10.,  3.,  4.,  8., 10.,  6.,\n",
       "        5.,  3.,  4.,  2.,  6.,  2.,  9.,  5.,  7.,  5., 10., 10., 12.,\n",
       "        1.,  4., 10.,  1.,  7.,  7.,  2.,  2.,  6.,  8.,  2.,  7.,  4.,\n",
       "        9., 10.,  9.,  6.,  9.,  2.,  2., 10.,  2.,  7.,  2., 11., 11.,\n",
       "       11., 11., 11., 10.,  7., 12.,  1.,  5.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_mfccs_rf_predicted_cv_y = optimized_mfccs_rand_forest.predict(cv_X_mfccs)\n",
    "optimized_mfccs_rf_predicted_cv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest f1 score (CV): 0.293\n",
      "Random forest accuracy (CV): 0.283\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and F1 for Random Forest\n",
    "optimized_mfccs_rf_cv_f1_score = f1_score(rf_cv_y, optimized_mfccs_rf_predicted_cv_y, average=\"weighted\")\n",
    "optimized_mfccs_rf_cv_accuracy = accuracy_score(rf_cv_y, optimized_mfccs_rf_predicted_cv_y)\n",
    "\n",
    "print(\"Random forest f1 score (CV): {:.3f}\".format(optimized_mfccs_rf_cv_f1_score))\n",
    "print(\"Random forest accuracy (CV): {:.3f}\".format(optimized_mfccs_rf_cv_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a little bit of tweaking we can get a Random Forest with accuracy and F1 score approaching 0.3, right around our current benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set benchmark\n",
    "best_cv_acc = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Networks\n",
    "Now that we have a benchmark obtained via simple linear and Random Forest models we can proceed towards trying to outdo it with MLPs and deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP - multi-layer perceptron\n",
    "Let's start with the simplest possible neural network of just 2 dense layers. We'll be working only on the mfccs data from now on, as it tends to produce better results. We will also add **batch normalization** and **dropout** to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design & compile the model\n",
    "num_nodes = 1000\n",
    "mlp = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = num_nodes, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.8),\n",
    "    Dense(num_categories, activation='softmax')\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "mlp.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual epoch: 1\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 3.9123 - acc: 0.1000 - val_loss: 4.1950 - val_acc: 0.2167\n",
      "Actual epoch: 2\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 2s - loss: 3.4404 - acc: 0.1875 - val_loss: 3.4460 - val_acc: 0.2167\n",
      "Actual epoch: 3\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 2s - loss: 2.8681 - acc: 0.2292 - val_loss: 5.3622 - val_acc: 0.1333\n",
      "Actual epoch: 4\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 2s - loss: 2.6828 - acc: 0.2292 - val_loss: 4.3822 - val_acc: 0.1667\n",
      "Actual epoch: 5\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.3916 - acc: 0.3208 - val_loss: 3.5390 - val_acc: 0.2500\n",
      "Actual epoch: 6\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.4419 - acc: 0.3167 - val_loss: 3.1560 - val_acc: 0.2667\n",
      "Actual epoch: 7\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.5454 - acc: 0.2958 - val_loss: 3.2238 - val_acc: 0.2000\n",
      "Actual epoch: 8\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.1615 - acc: 0.3333 - val_loss: 3.1211 - val_acc: 0.2500\n",
      "Actual epoch: 9\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.1457 - acc: 0.3375 - val_loss: 2.9087 - val_acc: 0.2333\n",
      "Actual epoch: 10\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.2247 - acc: 0.3542 - val_loss: 3.0325 - val_acc: 0.2167\n",
      "Actual epoch: 11\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.3361 - acc: 0.3500 - val_loss: 3.0582 - val_acc: 0.2167\n",
      "Actual epoch: 12\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.1633 - acc: 0.3958 - val_loss: 2.8327 - val_acc: 0.2333\n",
      "Actual epoch: 13\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.0095 - acc: 0.3583 - val_loss: 2.6981 - val_acc: 0.2333\n",
      "Actual epoch: 14\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.1458 - acc: 0.3958 - val_loss: 2.8480 - val_acc: 0.2167\n",
      "Actual epoch: 15\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 1.8871 - acc: 0.4000 - val_loss: 2.6870 - val_acc: 0.2667\n",
      "Actual epoch: 16\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.0715 - acc: 0.3750 - val_loss: 2.7678 - val_acc: 0.2500\n",
      "Actual epoch: 17\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 2.0624 - acc: 0.4042 - val_loss: 2.6114 - val_acc: 0.2500\n",
      "Actual epoch: 18\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 1.7950 - acc: 0.4542 - val_loss: 2.5724 - val_acc: 0.2500\n",
      "Actual epoch: 19\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 1.9419 - acc: 0.4208 - val_loss: 2.4827 - val_acc: 0.2500\n",
      "Actual epoch: 20\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 1.8617 - acc: 0.4000 - val_loss: 2.6070 - val_acc: 0.2500\n",
      "Actual epoch: 21\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 3s - loss: 1.7555 - acc: 0.4583 - val_loss: 2.7958 - val_acc: 0.3000\n"
     ]
    }
   ],
   "source": [
    "# let's train \n",
    "for i in range(30):\n",
    "    print(\"Actual epoch: {}\".format(i + 1))\n",
    "    mlp_results = mlp.fit(train_X_mfccs, train_y, batch_size=32, epochs=1, validation_data=(cv_X_mfccs, cv_y))\n",
    "    # stop if we exceed previous best (benchmark)\n",
    "    current_cv_acc = mlp_results.history[\"val_acc\"][0] \n",
    "    if current_cv_acc > best_cv_acc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last MLP scores\n",
      "Train acc: 0.4583\n",
      "CV acc: 0.3000\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "last_training_accuracy = max(mlp_results.history[\"acc\"])\n",
    "last_validation_accuracy = max(mlp_results.history[\"val_acc\"])\n",
    "print(\"Last MLP scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(last_training_accuracy, last_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "mlp_pred_cv_y = mlp.predict(cv_X_mfccs, batch_size=32)\n",
    "mlp_pred_cv_y = utils.one_hot_encode(mlp_pred_cv_y)\n",
    "mlp_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy via sklearn (CV): 0.3000\n",
      "MLP f1 score (CV): 0.2717\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "mlp_cv_accuracy = accuracy_score(cv_y, mlp_pred_cv_y)\n",
    "mlp_cv_f1_score = f1_score(cv_y, mlp_pred_cv_y, average=\"weighted\")\n",
    "print(\"MLP accuracy via sklearn (CV): {:.4f}\".format(mlp_cv_accuracy))\n",
    "print(\"MLP f1 score (CV): {:.4f}\".format(mlp_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a simple MLP model reaches a very similar accuracy score to our previous benchmark of 0.3. Both this one and the previous ones can be tuned to reach approximately 0.35 but let's save fine-tuning for when we have a more promising approach - we are also already overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural Networks\n",
    "Let's try adding more layers to capture more complex interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = 1500, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.87),\n",
    "    Dense(1000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.87),\n",
    "    Dense(num_categories, activation='softmax')\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "dnn.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual epoch: 1\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 6s - loss: 5.6303 - acc: 0.0917 - val_loss: 3.7513 - val_acc: 0.1333\n",
      "Actual epoch: 2\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 5.4301 - acc: 0.1042 - val_loss: 4.4812 - val_acc: 0.1167\n",
      "Actual epoch: 3\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 7s - loss: 4.9806 - acc: 0.1500 - val_loss: 5.2225 - val_acc: 0.1167\n",
      "Actual epoch: 4\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 6s - loss: 5.0200 - acc: 0.1875 - val_loss: 6.6632 - val_acc: 0.1333\n",
      "Actual epoch: 5\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 6s - loss: 4.9083 - acc: 0.1417 - val_loss: 6.2239 - val_acc: 0.1333\n",
      "Actual epoch: 6\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 7s - loss: 4.7389 - acc: 0.1625 - val_loss: 5.8977 - val_acc: 0.1667\n",
      "Actual epoch: 7\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 7s - loss: 4.2021 - acc: 0.1917 - val_loss: 4.8229 - val_acc: 0.1833\n",
      "Actual epoch: 8\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 4.1677 - acc: 0.2250 - val_loss: 4.4357 - val_acc: 0.1000\n",
      "Actual epoch: 9\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 6s - loss: 4.5420 - acc: 0.1667 - val_loss: 3.9467 - val_acc: 0.1833\n",
      "Actual epoch: 10\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 8s - loss: 4.4031 - acc: 0.1625 - val_loss: 3.4467 - val_acc: 0.2167\n",
      "Actual epoch: 11\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 6s - loss: 4.2834 - acc: 0.1833 - val_loss: 3.3865 - val_acc: 0.2000\n",
      "Actual epoch: 12\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 6s - loss: 4.1474 - acc: 0.2000 - val_loss: 3.6475 - val_acc: 0.2167\n",
      "Actual epoch: 13\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 6s - loss: 4.1857 - acc: 0.1958 - val_loss: 3.8914 - val_acc: 0.1833\n",
      "Actual epoch: 14\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 3.7022 - acc: 0.2042 - val_loss: 3.4260 - val_acc: 0.2000\n",
      "Actual epoch: 15\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 4.0620 - acc: 0.2167 - val_loss: 2.9560 - val_acc: 0.2667\n",
      "Actual epoch: 16\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 3.8453 - acc: 0.2125 - val_loss: 2.9036 - val_acc: 0.2500\n",
      "Actual epoch: 17\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 4.4116 - acc: 0.1708 - val_loss: 3.0488 - val_acc: 0.2500\n",
      "Actual epoch: 18\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 3.8452 - acc: 0.1833 - val_loss: 3.2456 - val_acc: 0.2333\n",
      "Actual epoch: 19\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 6s - loss: 3.9315 - acc: 0.2250 - val_loss: 2.9589 - val_acc: 0.2000\n",
      "Actual epoch: 20\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 4s - loss: 3.4136 - acc: 0.2542 - val_loss: 2.9242 - val_acc: 0.2167\n",
      "Actual epoch: 21\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 3.4761 - acc: 0.2500 - val_loss: 2.8618 - val_acc: 0.2167\n",
      "Actual epoch: 22\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 4s - loss: 3.6389 - acc: 0.2292 - val_loss: 2.8402 - val_acc: 0.2167\n",
      "Actual epoch: 23\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 4s - loss: 3.8278 - acc: 0.1875 - val_loss: 2.7928 - val_acc: 0.2500\n",
      "Actual epoch: 24\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 4s - loss: 3.3953 - acc: 0.2417 - val_loss: 2.7078 - val_acc: 0.2333\n",
      "Actual epoch: 25\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 4s - loss: 3.2757 - acc: 0.2625 - val_loss: 2.5028 - val_acc: 0.2167\n",
      "Actual epoch: 26\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 4s - loss: 3.6631 - acc: 0.2125 - val_loss: 2.3921 - val_acc: 0.2000\n",
      "Actual epoch: 27\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 3.5564 - acc: 0.2208 - val_loss: 2.4018 - val_acc: 0.2000\n",
      "Actual epoch: 28\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 3.3632 - acc: 0.2375 - val_loss: 2.4006 - val_acc: 0.2333\n",
      "Actual epoch: 29\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 5s - loss: 3.6876 - acc: 0.2000 - val_loss: 2.3613 - val_acc: 0.2500\n",
      "Actual epoch: 30\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/1\n",
      "240/240 [==============================] - 4s - loss: 3.1480 - acc: 0.2458 - val_loss: 2.3474 - val_acc: 0.2667\n"
     ]
    }
   ],
   "source": [
    "# let's train \n",
    "for i in range(30):\n",
    "    print(\"Actual epoch: {}\".format(i + 1))\n",
    "    dnn_results = dnn.fit(train_X_mfccs, train_y, batch_size=32, epochs=1, validation_data=(cv_X_mfccs, cv_y))\n",
    "    # stop if we exceed or meet previous best (benchmark) - you can then re-run to see if we're overfitting or not\n",
    "    current_cv_acc = dnn_results.history[\"val_acc\"][0] \n",
    "    if current_cv_acc >= best_cv_acc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last DNN scores\n",
      "Train acc: 0.2458\n",
      "CV acc: 0.2667\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "last_training_accuracy = max(dnn_results.history[\"acc\"])\n",
    "last_validation_accuracy = max(dnn_results.history[\"val_acc\"])\n",
    "print(\"Last DNN scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(last_training_accuracy, last_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "dnn_pred_cv_y = dnn.predict(cv_X_mfccs, batch_size=32)\n",
    "dnn_pred_cv_y = utils.one_hot_encode(dnn_pred_cv_y)\n",
    "dnn_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN accuracy via sklearn (CV): 0.2667\n",
      "DNN f1 score (CV): 0.2264\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "dnn_cv_accuracy = accuracy_score(cv_y, dnn_pred_cv_y)\n",
    "dnn_cv_f1_score = f1_score(cv_y, dnn_pred_cv_y, average=\"weighted\")\n",
    "print(\"DNN accuracy via sklearn (CV): {:.4f}\".format(dnn_cv_accuracy))\n",
    "print(\"DNN f1 score (CV): {:.4f}\".format(dnn_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Models\n",
    "Seems we're stuck around 0.3 accuracy. That makes sense because the actual \"no\" and other words may come at any place in the vector, we can't really keep being attached to specific indexes when training (which we currently are). Let's try convolutional layers, which can find certain patterns regardless of whether they appear at the start or end of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 16000, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to use convolutions we have reshape our X -> expand it to 3 dimensions\n",
    "conv_train_X_mfccs = np.expand_dims(train_X_mfccs, axis=2)\n",
    "conv_train_X_mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for cv & test\n",
    "conv_cv_X_mfccs = np.expand_dims(cv_X_mfccs, axis=2)\n",
    "conv_test_X_mfccs = np.expand_dims(test_X_mfccs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = Sequential([\n",
    "        Convolution1D(input_shape=(num_features, 1), kernel_size=32, filters=8, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling1D(),\n",
    "        Convolution1D(kernel_size=64, filters=16, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(500, activation=\"relu\"),\n",
    "        Dropout(.6),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "cnn1.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN architecture should get to 0.367 accuracy around the 35 epoch and then start to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/50\n",
      "240/240 [==============================] - 23s - loss: 4.1719 - acc: 0.1125 - val_loss: 2.5335 - val_acc: 0.0833\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - 22s - loss: 2.5723 - acc: 0.0667 - val_loss: 2.4793 - val_acc: 0.1333\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - 22s - loss: 2.4780 - acc: 0.0875 - val_loss: 2.4808 - val_acc: 0.1167\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - 18s - loss: 2.4764 - acc: 0.0958 - val_loss: 2.4799 - val_acc: 0.1333\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - 18s - loss: 2.4746 - acc: 0.1417 - val_loss: 2.4753 - val_acc: 0.1167\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - 20s - loss: 2.4563 - acc: 0.1500 - val_loss: 2.4628 - val_acc: 0.1333\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - 17s - loss: 2.4597 - acc: 0.1583 - val_loss: 2.4511 - val_acc: 0.1000\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - 17s - loss: 2.4303 - acc: 0.1708 - val_loss: 2.4370 - val_acc: 0.0667\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - 18s - loss: 2.4096 - acc: 0.1375 - val_loss: 2.4143 - val_acc: 0.1000\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - 19s - loss: 2.3690 - acc: 0.1458 - val_loss: 2.3861 - val_acc: 0.1167\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - 17s - loss: 2.3989 - acc: 0.1625 - val_loss: 2.3799 - val_acc: 0.1167\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - 20s - loss: 2.3633 - acc: 0.1583 - val_loss: 2.3652 - val_acc: 0.1167\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - 21s - loss: 2.3376 - acc: 0.1958 - val_loss: 2.3629 - val_acc: 0.1667\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - 22s - loss: 2.3017 - acc: 0.2167 - val_loss: 2.3405 - val_acc: 0.2000\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - 21s - loss: 2.3563 - acc: 0.1708 - val_loss: 2.3476 - val_acc: 0.2333\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - 19s - loss: 2.2728 - acc: 0.1917 - val_loss: 2.3291 - val_acc: 0.1833\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - 20s - loss: 2.2948 - acc: 0.1875 - val_loss: 2.3584 - val_acc: 0.2167\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - 20s - loss: 2.2415 - acc: 0.2417 - val_loss: 2.3060 - val_acc: 0.2333\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - 23s - loss: 2.2262 - acc: 0.2208 - val_loss: 2.3125 - val_acc: 0.2333\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - 21s - loss: 2.1659 - acc: 0.2083 - val_loss: 2.3096 - val_acc: 0.2500\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - 21s - loss: 2.1620 - acc: 0.2417 - val_loss: 2.3133 - val_acc: 0.2167\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - 20s - loss: 2.1157 - acc: 0.2792 - val_loss: 2.3019 - val_acc: 0.2667\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - 20s - loss: 2.1526 - acc: 0.2375 - val_loss: 2.2592 - val_acc: 0.2833\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - 20s - loss: 2.0778 - acc: 0.2833 - val_loss: 2.2553 - val_acc: 0.2167\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - 20s - loss: 2.0383 - acc: 0.3042 - val_loss: 2.2191 - val_acc: 0.3333\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - 25s - loss: 2.0517 - acc: 0.2625 - val_loss: 2.2372 - val_acc: 0.3000\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - 19s - loss: 2.0760 - acc: 0.2958 - val_loss: 2.1996 - val_acc: 0.2000\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - 17s - loss: 1.9186 - acc: 0.3708 - val_loss: 2.1704 - val_acc: 0.3000\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - 20s - loss: 1.9479 - acc: 0.3250 - val_loss: 2.2244 - val_acc: 0.3000\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - 20s - loss: 1.9530 - acc: 0.3292 - val_loss: 2.1614 - val_acc: 0.2500\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - 20s - loss: 1.9315 - acc: 0.3333 - val_loss: 2.1773 - val_acc: 0.2667\n",
      "Epoch 32/50\n",
      "240/240 [==============================] - 18s - loss: 1.8307 - acc: 0.3667 - val_loss: 2.1951 - val_acc: 0.3000\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - 18s - loss: 1.7678 - acc: 0.3958 - val_loss: 2.1575 - val_acc: 0.2667\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - 21s - loss: 1.7650 - acc: 0.3542 - val_loss: 2.1520 - val_acc: 0.2667\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - 19s - loss: 1.7803 - acc: 0.3583 - val_loss: 2.1517 - val_acc: 0.3667\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - 19s - loss: 1.6829 - acc: 0.4125 - val_loss: 2.1391 - val_acc: 0.2833\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - 20s - loss: 1.6655 - acc: 0.4167 - val_loss: 2.1886 - val_acc: 0.3500\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - 22s - loss: 1.6752 - acc: 0.4167 - val_loss: 2.2246 - val_acc: 0.2500\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - 20s - loss: 1.6516 - acc: 0.4250 - val_loss: 2.1718 - val_acc: 0.3167\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - 22s - loss: 1.6125 - acc: 0.4375 - val_loss: 2.1584 - val_acc: 0.3000\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - 22s - loss: 1.6046 - acc: 0.4667 - val_loss: 2.1787 - val_acc: 0.3500\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - 20s - loss: 1.5512 - acc: 0.4958 - val_loss: 2.2574 - val_acc: 0.2333\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - 17s - loss: 1.5715 - acc: 0.4458 - val_loss: 2.2282 - val_acc: 0.3167\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - 17s - loss: 1.4483 - acc: 0.5000 - val_loss: 2.2525 - val_acc: 0.2667\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - 17s - loss: 1.4717 - acc: 0.5125 - val_loss: 2.2919 - val_acc: 0.3167\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - 17s - loss: 1.4245 - acc: 0.5042 - val_loss: 2.2740 - val_acc: 0.3333\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - 17s - loss: 1.3500 - acc: 0.5583 - val_loss: 2.2408 - val_acc: 0.3000\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - 17s - loss: 1.2946 - acc: 0.5667 - val_loss: 2.2571 - val_acc: 0.3167\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - 17s - loss: 1.2529 - acc: 0.5750 - val_loss: 2.2429 - val_acc: 0.2833\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - 17s - loss: 1.3307 - acc: 0.5583 - val_loss: 2.2113 - val_acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "cnn1_results = cnn1.fit(conv_train_X_mfccs, train_y, batch_size=32, epochs=60, validation_data=(conv_cv_X_mfccs, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last DNN scores\n",
      "Train acc: 0.5750\n",
      "CV acc: 0.3667\n"
     ]
    }
   ],
   "source": [
    "# show best results\n",
    "best_training_accuracy = max(cnn1_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(cnn1_results.history[\"val_acc\"])\n",
    "print(\"Best CNN 1 scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "cnn1_pred_cv_y = cnn1.predict(conv_cv_X_mfccs, batch_size=32)\n",
    "cnn1_pred_cv_y = utils.one_hot_encode(cnn1_pred_cv_y)\n",
    "cnn1_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN accuracy via sklearn (CV): 0.3167\n",
      "DNN f1 score (CV): 0.3039\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "cnn1_cv_accuracy = accuracy_score(cv_y, cnn1_pred_cv_y)\n",
    "cnn1_cv_f1_score = f1_score(cv_y, cnn1_pred_cv_y, average=\"weighted\")\n",
    "print(\"CNN 1 accuracy via sklearn (CV): {:.4f}\".format(cnn1_cv_accuracy))\n",
    "print(\"CNN 1 f1 score (CV): {:.4f}\".format(cnn1_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the kernel size - patterns in speech mighr require more than e.g. 32 single samplings to be recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = Sequential([\n",
    "        Convolution1D(input_shape=(num_features, 1), kernel_size=256, filters=32, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(),\n",
    "        Convolution1D(kernel_size=512, filters=32, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(500, activation=\"relu\"),\n",
    "        Dropout(.6),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "cnn2.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/50\n",
      "240/240 [==============================] - 378s - loss: 5.1550 - acc: 0.0667 - val_loss: 2.4849 - val_acc: 0.0833\n",
      "Epoch 2/50\n",
      "224/240 [===========================>..] - ETA: 23s - loss: 2.4853 - acc: 0.0804"
     ]
    }
   ],
   "source": [
    "cnn2_results = cnn2.fit(conv_train_X_mfccs, train_y, batch_size=32, epochs=50, validation_data=(conv_cv_X_mfccs, cv_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action plan\n",
    "X) turn the sample data into numpy arrays with X and y normally <br>\n",
    "X) turn sample data into numpy arrays with X and y via mfccs<br>\n",
    "X) Use linear model? (towards first benchmark)<br>\n",
    "X) Use random forest?<br>\n",
    "X) Use MLP<br>\n",
    "X) Use multiple dense layers<br>\n",
    "4c) Use convolutions (try the increased kernel sie that takes 400s per epoch)<br>\n",
    "4d) USE RNN -> like in Nietzsche [https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/]<br>\n",
    "5) Add preprocessing and test a couple of the best models<br>\n",
    "\n",
    "6) Consider splitting the work on images into separate notebook depending on how bulky this gets<br>\n",
    "7) Experiments on images without data augmentation<br>\n",
    "8) Experiments on images with data augmentation<br>\n",
    "\n",
    "9) Decide on e.g. 3 most promising methods<br>\n",
    "\n",
    "And then:<br>\n",
    "10) Move to writing the most promising models in tensorflow<br>\n",
    "11) Include tensorboard visualization of training & graph<br>\n",
    "12) Code for turning results into kaggle format of results to get score<br>\n",
    "13) Obtain a good score on kaggle<br>\n",
    "14) Re-read everything from start to finish and adjust<br>\n",
    "15) Write a good Readme for markdown<br>\n",
    "16) Add to CV<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start by trying a simple model on the 1D mfccs -> even a linear model,then maybe 1D convolutions on keras, then move on to actual 2D stuff.\n",
    "\n",
    "**If we work on 1D data (like mfccs/waveforms) we can use the data augmentation done by the guy here:https://www.kaggle.com/CVxTz/audio-data-augmentation when passing our files into the Keras DataGenerator, but if we decide to work with the MEL images we can just use the same image augmentation as in fastai**\n",
    "\n",
    "se very simple linear model / keras network to see how we do on current sample, then experiment with different preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_mfccs(wav_file):\n",
    "    \"\"\"\n",
    "    Take a file and return the mel-frequency cepstrum.\n",
    "    \"\"\"\n",
    "    X, sample_rate = librosa.load(wav_file, res_type='kaiser_fast', sr=None)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample = \"data\\\\sample\"\n",
    "path_to_a_wav = os.path.join(path_to_sample, \"cv\\\\unknown\\\\9db2bfe9_nohash_4_five.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.06296364e+02,  6.31600698e+01, -2.38641127e+01, -4.86630969e+00,\n",
       "       -3.53521586e+01, -3.80595467e+00, -1.06260360e+01, -5.45357225e+00,\n",
       "       -5.38032267e-01, -3.13763738e+00, -1.61412864e+00, -3.92968492e+00,\n",
       "       -5.57078467e+00, -4.21382641e+00, -8.39318905e+00,  2.59598676e+00,\n",
       "       -1.21718174e+01,  6.58169994e+00, -6.52752377e+00,  2.20022835e+00,\n",
       "       -4.70370097e+00, -7.75634867e-01, -2.45838166e+00, -1.27684907e+00,\n",
       "       -9.24384769e-01, -2.84166555e+00, -2.06350172e+00, -8.51055474e-01,\n",
       "       -6.62192168e-01, -1.39785145e+00, -1.65039538e+00,  3.35274945e-03,\n",
       "        1.09041363e+00, -5.96439092e-01,  5.99651357e-01, -2.19326520e+00,\n",
       "        7.19763870e-01,  1.33843908e+00,  1.59644506e-01, -9.80777004e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_mfccs(path_to_a_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -3.0517578e-05, -3.0517578e-05, -3.0517578e-05], dtype=float32), 16000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and that's what the waveform uses, I think\n",
    "librosa.core.load(path_to_a_wav, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mateusz\\docume~1\\mateusz\\career\\machin~1\\tensor~2\\trf_venv\\lib\\site-packages\\pydub\\utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from utils import get_wav_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_wav_info(path_to_a_wav)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(librosa.core.load(path_to_a_wav)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
