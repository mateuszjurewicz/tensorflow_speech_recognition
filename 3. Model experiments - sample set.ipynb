{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model experiments - sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks we have separated a small subset of our data, called \"sample\", on which we can now experiment with simple models to assess the effectiveness of our preprocessing & data augmentation techniques.\n",
    "\n",
    "We do it this way to avoid spending too much time on training on the entire set, the assumption is that the methods which are effective on the sample will work well on a larger scale too. \n",
    "\n",
    "We will start by testing a couple of simple models on untouched sample data (as numpy arrays) and then proceed towards data augmentation and finally spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/tensorflow_speech_recognition\r\n"
     ]
    }
   ],
   "source": [
    "# first make sure we're in the parent dictory of our data/sample folders.\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "We'll need a couple of additional libraries so let's import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import glob\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow\n",
    "\n",
    "# utils\n",
    "from importlib import reload\n",
    "import utils; reload(utils)\n",
    "\n",
    "# keras as tensorflow backend\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, BatchNormalization, Dropout, Convolution1D, MaxPooling1D, Flatten\n",
    "from tensorflow.python.keras.layers import SimpleRNN, GRU, ConvLSTM2D\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "# F1 and accuracy score metric\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "The easiest way to work with data is by turning it into a list of numbers, in our case a numpy array. We can use one of the functions from utils to load the raw data or use the librosa.load() function. The difference lies in the fact that the former returns int16s whereas librosa returns float32s and uses its default sampling rate of 22050Hz, unless we explicitly tell it to use the file's original sampling rate of 16000Hz.\n",
    "\n",
    "We should also consider normalizing our data (so that it all falls within the same scale) and using the preprocessing methods explored in the previous notebook (MFCCs, Mel spectrogram, fast fourier transform and tempogram). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample = \"data/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to go through each of the folders in our sample/train, cv and test sets, one-hot encode their label and load the 16K long array of raw data. The y data will be of shape (m, 12), where m is the number of examples, and the X data will be of shape (m, 16000) - at least for the raw .wav input.\n",
    "\n",
    "Let's calculate **m** first. We will do that by using a function that create a list of all the .wav files within a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of paths\n",
    "We will use the glob module that we learned about in the very first notebook and a function from util.py which can, given a directory, return a list of paths to .wav files within it. We will repeat the process for all 3 sets within sample, and every category subdirectory within those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/sample/train/stop/01b4757a_nohash_0.wav',\n",
       " 'data/sample/train/stop/3ac2e76f_nohash_0.wav',\n",
       " 'data/sample/train/stop/3e31dffe_nohash_3.wav',\n",
       " 'data/sample/train/stop/37bd115d_nohash_1.wav',\n",
       " 'data/sample/train/stop/6c2dd2d5_nohash_0.wav']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example we can grab all .wav files from sample/train/stop\n",
    "path_to_sample_train_stop = os.path.join(path_to_sample, \"train\", \"stop\")\n",
    "utils.grab_wavs(path_to_sample_train_stop)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need a list of all category folder names\n",
    "categories_to_predict = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"silence\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/sample/train/yes/0f3f64d5_nohash_0.wav',\n",
       " 'data/sample/train/yes/8a28231e_nohash_3.wav',\n",
       " 'data/sample/train/yes/d3f22f0e_nohash_0.wav',\n",
       " 'data/sample/train/yes/2d3c8dcb_nohash_1.wav',\n",
       " 'data/sample/train/yes/d486fb84_nohash_0.wav',\n",
       " 'data/sample/train/yes/61d3e51e_nohash_0.wav',\n",
       " 'data/sample/train/yes/8f811bbc_nohash_0.wav',\n",
       " 'data/sample/train/yes/b43de700_nohash_0.wav',\n",
       " 'data/sample/train/yes/66a412a7_nohash_0.wav',\n",
       " 'data/sample/train/yes/92e17cc4_nohash_0.wav',\n",
       " 'data/sample/train/yes/6c9223bd_nohash_0.wav',\n",
       " 'data/sample/train/yes/d5356b9a_nohash_0.wav',\n",
       " 'data/sample/train/yes/e7d0eb3f_nohash_1.wav',\n",
       " 'data/sample/train/yes/712e4d58_nohash_2.wav',\n",
       " 'data/sample/train/yes/a2fefcb4_nohash_0.wav',\n",
       " 'data/sample/train/yes/324210dd_nohash_1.wav',\n",
       " 'data/sample/train/yes/742d6431_nohash_0.wav',\n",
       " 'data/sample/train/yes/70a00e98_nohash_1.wav',\n",
       " 'data/sample/train/yes/28e47b1a_nohash_4.wav',\n",
       " 'data/sample/train/yes/7014b07e_nohash_1.wav',\n",
       " 'data/sample/train/no/f0ebef1b_nohash_0.wav',\n",
       " 'data/sample/train/no/7e783e3f_nohash_0.wav',\n",
       " 'data/sample/train/no/65040d9b_nohash_0.wav',\n",
       " 'data/sample/train/no/fcb25a78_nohash_0.wav',\n",
       " 'data/sample/train/no/28ce0c58_nohash_6.wav',\n",
       " 'data/sample/train/no/3c257192_nohash_1.wav',\n",
       " 'data/sample/train/no/106a6183_nohash_2.wav',\n",
       " 'data/sample/train/no/eb0676ec_nohash_1.wav',\n",
       " 'data/sample/train/no/6add0595_nohash_0.wav',\n",
       " 'data/sample/train/no/3efa7ec4_nohash_1.wav',\n",
       " 'data/sample/train/no/e4a2cf79_nohash_0.wav',\n",
       " 'data/sample/train/no/3c257192_nohash_3.wav',\n",
       " 'data/sample/train/no/b36c27c2_nohash_0.wav',\n",
       " 'data/sample/train/no/73dda36a_nohash_0.wav',\n",
       " 'data/sample/train/no/7ab5b8f7_nohash_0.wav',\n",
       " 'data/sample/train/no/c661be6e_nohash_2.wav',\n",
       " 'data/sample/train/no/7f74626f_nohash_1.wav',\n",
       " 'data/sample/train/no/ab5b211a_nohash_1.wav',\n",
       " 'data/sample/train/no/86cb59b2_nohash_3.wav',\n",
       " 'data/sample/train/no/caa7feaf_nohash_1.wav',\n",
       " 'data/sample/train/up/d3badc9a_nohash_1.wav',\n",
       " 'data/sample/train/up/37e8db82_nohash_0.wav',\n",
       " 'data/sample/train/up/079dfce3_nohash_1.wav',\n",
       " 'data/sample/train/up/fb9d6d23_nohash_0.wav',\n",
       " 'data/sample/train/up/48463cbc_nohash_2.wav',\n",
       " 'data/sample/train/up/229978fd_nohash_4.wav',\n",
       " 'data/sample/train/up/7846fd85_nohash_1.wav',\n",
       " 'data/sample/train/up/f33660af_nohash_1.wav',\n",
       " 'data/sample/train/up/13d7b8c9_nohash_0.wav',\n",
       " 'data/sample/train/up/cb8f8307_nohash_3.wav',\n",
       " 'data/sample/train/up/b5552931_nohash_1.wav',\n",
       " 'data/sample/train/up/9dcfba4b_nohash_0.wav',\n",
       " 'data/sample/train/up/e649aa92_nohash_0.wav',\n",
       " 'data/sample/train/up/8281a2a8_nohash_3.wav',\n",
       " 'data/sample/train/up/6078eb0d_nohash_0.wav',\n",
       " 'data/sample/train/up/10ace7eb_nohash_1.wav',\n",
       " 'data/sample/train/up/422f8cf1_nohash_2.wav',\n",
       " 'data/sample/train/up/1ba20be2_nohash_0.wav',\n",
       " 'data/sample/train/up/742d6431_nohash_1.wav',\n",
       " 'data/sample/train/up/37a3cd33_nohash_0.wav',\n",
       " 'data/sample/train/down/a1cff772_nohash_2.wav',\n",
       " 'data/sample/train/down/4ec7d027_nohash_0.wav',\n",
       " 'data/sample/train/down/742d6431_nohash_4.wav',\n",
       " 'data/sample/train/down/0819edb0_nohash_2.wav',\n",
       " 'data/sample/train/down/884ae8e1_nohash_1.wav',\n",
       " 'data/sample/train/down/834f03fe_nohash_1.wav',\n",
       " 'data/sample/train/down/ced835d3_nohash_1.wav',\n",
       " 'data/sample/train/down/0cd323ec_nohash_0.wav',\n",
       " 'data/sample/train/down/3bfd30e6_nohash_2.wav',\n",
       " 'data/sample/train/down/0e5193e6_nohash_1.wav',\n",
       " 'data/sample/train/down/c5570933_nohash_0.wav',\n",
       " 'data/sample/train/down/b7a0754f_nohash_3.wav',\n",
       " 'data/sample/train/down/ced4e2a1_nohash_0.wav',\n",
       " 'data/sample/train/down/b4bef564_nohash_1.wav',\n",
       " 'data/sample/train/down/dabf67d9_nohash_0.wav',\n",
       " 'data/sample/train/down/3d53244b_nohash_0.wav',\n",
       " 'data/sample/train/down/778a4a01_nohash_0.wav',\n",
       " 'data/sample/train/down/35d1b6ee_nohash_3.wav',\n",
       " 'data/sample/train/down/06076b6b_nohash_2.wav',\n",
       " 'data/sample/train/down/38d78313_nohash_1.wav',\n",
       " 'data/sample/train/left/4ec7d027_nohash_0.wav',\n",
       " 'data/sample/train/left/a74f3917_nohash_0.wav',\n",
       " 'data/sample/train/left/c2bc4489_nohash_1.wav',\n",
       " 'data/sample/train/left/e102119e_nohash_0.wav',\n",
       " 'data/sample/train/left/3cdecb0b_nohash_1.wav',\n",
       " 'data/sample/train/left/51eefcc6_nohash_0.wav',\n",
       " 'data/sample/train/left/743edf9d_nohash_1.wav',\n",
       " 'data/sample/train/left/190821dc_nohash_2.wav',\n",
       " 'data/sample/train/left/813b82a6_nohash_0.wav',\n",
       " 'data/sample/train/left/833a0279_nohash_0.wav',\n",
       " 'data/sample/train/left/763188c4_nohash_0.wav',\n",
       " 'data/sample/train/left/da5dadb9_nohash_0.wav',\n",
       " 'data/sample/train/left/6e2ef3d5_nohash_0.wav',\n",
       " 'data/sample/train/left/d8521ea0_nohash_0.wav',\n",
       " 'data/sample/train/left/834f03fe_nohash_0.wav',\n",
       " 'data/sample/train/left/1b63157b_nohash_0.wav',\n",
       " 'data/sample/train/left/824e8ce5_nohash_0.wav',\n",
       " 'data/sample/train/left/408de0a4_nohash_0.wav',\n",
       " 'data/sample/train/left/9f4098cb_nohash_0.wav',\n",
       " 'data/sample/train/left/a2b16113_nohash_0.wav',\n",
       " 'data/sample/train/right/ece1a95a_nohash_0.wav',\n",
       " 'data/sample/train/right/7e783e3f_nohash_1.wav',\n",
       " 'data/sample/train/right/5ba724a7_nohash_0.wav',\n",
       " 'data/sample/train/right/93f30cc4_nohash_1.wav',\n",
       " 'data/sample/train/right/cb2929ce_nohash_4.wav',\n",
       " 'data/sample/train/right/7846fd85_nohash_1.wav',\n",
       " 'data/sample/train/right/e7ea8b76_nohash_1.wav',\n",
       " 'data/sample/train/right/57b68383_nohash_0.wav',\n",
       " 'data/sample/train/right/340c8b10_nohash_0.wav',\n",
       " 'data/sample/train/right/8781f4c1_nohash_0.wav',\n",
       " 'data/sample/train/right/9735764a_nohash_0.wav',\n",
       " 'data/sample/train/right/02fcd241_nohash_0.wav',\n",
       " 'data/sample/train/right/7f74626f_nohash_3.wav',\n",
       " 'data/sample/train/right/48a9f771_nohash_1.wav',\n",
       " 'data/sample/train/right/b06c19b0_nohash_1.wav',\n",
       " 'data/sample/train/right/be91a00a_nohash_1.wav',\n",
       " 'data/sample/train/right/9080f6d3_nohash_1.wav',\n",
       " 'data/sample/train/right/dc6e9c04_nohash_0.wav',\n",
       " 'data/sample/train/right/e91d173e_nohash_0.wav',\n",
       " 'data/sample/train/right/0137b3f4_nohash_0.wav',\n",
       " 'data/sample/train/on/ffd2ba2f_nohash_0.wav',\n",
       " 'data/sample/train/on/3565ca83_nohash_2.wav',\n",
       " 'data/sample/train/on/14775481_nohash_0.wav',\n",
       " 'data/sample/train/on/b9515bf3_nohash_1.wav',\n",
       " 'data/sample/train/on/5efb758c_nohash_0.wav',\n",
       " 'data/sample/train/on/eb3f7d82_nohash_2.wav',\n",
       " 'data/sample/train/on/9d050657_nohash_0.wav',\n",
       " 'data/sample/train/on/0132a06d_nohash_3.wav',\n",
       " 'data/sample/train/on/15d83b54_nohash_0.wav',\n",
       " 'data/sample/train/on/611d2b50_nohash_4.wav',\n",
       " 'data/sample/train/on/ab353673_nohash_0.wav',\n",
       " 'data/sample/train/on/c79159aa_nohash_1.wav',\n",
       " 'data/sample/train/on/c2e08f08_nohash_1.wav',\n",
       " 'data/sample/train/on/eb3f7d82_nohash_4.wav',\n",
       " 'data/sample/train/on/28ed6bc9_nohash_0.wav',\n",
       " 'data/sample/train/on/4f086393_nohash_0.wav',\n",
       " 'data/sample/train/on/cce17a61_nohash_1.wav',\n",
       " 'data/sample/train/on/8eb4a1bf_nohash_3.wav',\n",
       " 'data/sample/train/on/7799c9cd_nohash_0.wav',\n",
       " 'data/sample/train/on/2579e514_nohash_0.wav',\n",
       " 'data/sample/train/off/a759efbc_nohash_1.wav',\n",
       " 'data/sample/train/off/b16f2d0d_nohash_1.wav',\n",
       " 'data/sample/train/off/611d2b50_nohash_1.wav',\n",
       " 'data/sample/train/off/b59fe16d_nohash_0.wav',\n",
       " 'data/sample/train/off/37bd115d_nohash_0.wav',\n",
       " 'data/sample/train/off/a8f45bdc_nohash_0.wav',\n",
       " 'data/sample/train/off/5f1b1051_nohash_0.wav',\n",
       " 'data/sample/train/off/8fce59ff_nohash_2.wav',\n",
       " 'data/sample/train/off/89f3ab7d_nohash_2.wav',\n",
       " 'data/sample/train/off/d7529c0c_nohash_0.wav',\n",
       " 'data/sample/train/off/ef2578c0_nohash_1.wav',\n",
       " 'data/sample/train/off/c2e08f08_nohash_0.wav',\n",
       " 'data/sample/train/off/1bb6ed89_nohash_0.wav',\n",
       " 'data/sample/train/off/8b39e36f_nohash_2.wav',\n",
       " 'data/sample/train/off/4cb874bb_nohash_0.wav',\n",
       " 'data/sample/train/off/e8d562ca_nohash_1.wav',\n",
       " 'data/sample/train/off/c93d5e22_nohash_2.wav',\n",
       " 'data/sample/train/off/a3fc7884_nohash_0.wav',\n",
       " 'data/sample/train/off/0137b3f4_nohash_0.wav',\n",
       " 'data/sample/train/off/ab353673_nohash_1.wav',\n",
       " 'data/sample/train/stop/01b4757a_nohash_0.wav',\n",
       " 'data/sample/train/stop/3ac2e76f_nohash_0.wav',\n",
       " 'data/sample/train/stop/3e31dffe_nohash_3.wav',\n",
       " 'data/sample/train/stop/37bd115d_nohash_1.wav',\n",
       " 'data/sample/train/stop/6c2dd2d5_nohash_0.wav',\n",
       " 'data/sample/train/stop/cf68fad2_nohash_1.wav',\n",
       " 'data/sample/train/stop/f35eedd7_nohash_0.wav',\n",
       " 'data/sample/train/stop/12c206ea_nohash_1.wav',\n",
       " 'data/sample/train/stop/ae82c78c_nohash_0.wav',\n",
       " 'data/sample/train/stop/824e8ce5_nohash_1.wav',\n",
       " 'data/sample/train/stop/f632210f_nohash_1.wav',\n",
       " 'data/sample/train/stop/d0858dce_nohash_0.wav',\n",
       " 'data/sample/train/stop/0b56bcfe_nohash_0.wav',\n",
       " 'data/sample/train/stop/28ed6bc9_nohash_0.wav',\n",
       " 'data/sample/train/stop/229978fd_nohash_1.wav',\n",
       " 'data/sample/train/stop/f8f60f59_nohash_2.wav',\n",
       " 'data/sample/train/stop/012c8314_nohash_0.wav',\n",
       " 'data/sample/train/stop/a1cff772_nohash_0.wav',\n",
       " 'data/sample/train/stop/834f03fe_nohash_2.wav',\n",
       " 'data/sample/train/stop/479e64cc_nohash_1.wav',\n",
       " 'data/sample/train/go/42c6fff1_nohash_0.wav',\n",
       " 'data/sample/train/go/31583d30_nohash_0.wav',\n",
       " 'data/sample/train/go/324210dd_nohash_4.wav',\n",
       " 'data/sample/train/go/71904de3_nohash_0.wav',\n",
       " 'data/sample/train/go/ced835d3_nohash_1.wav',\n",
       " 'data/sample/train/go/9efe5140_nohash_1.wav',\n",
       " 'data/sample/train/go/c0c701f1_nohash_0.wav',\n",
       " 'data/sample/train/go/0cd323ec_nohash_0.wav',\n",
       " 'data/sample/train/go/126a31d2_nohash_1.wav',\n",
       " 'data/sample/train/go/afd53389_nohash_0.wav',\n",
       " 'data/sample/train/go/b4604db4_nohash_0.wav',\n",
       " 'data/sample/train/go/c1e0e8e3_nohash_3.wav',\n",
       " 'data/sample/train/go/bc065a17_nohash_1.wav',\n",
       " 'data/sample/train/go/ea9f8e11_nohash_1.wav',\n",
       " 'data/sample/train/go/590750e8_nohash_2.wav',\n",
       " 'data/sample/train/go/e41e41f7_nohash_0.wav',\n",
       " 'data/sample/train/go/59fe87e6_nohash_1.wav',\n",
       " 'data/sample/train/go/6414258b_nohash_0.wav',\n",
       " 'data/sample/train/go/26e573a9_nohash_1.wav',\n",
       " 'data/sample/train/go/483e2a6f_nohash_1.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_32.wav',\n",
       " 'data/sample/train/silence/pink_noise_4.wav',\n",
       " 'data/sample/train/silence/pink_noise_25.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_16.wav',\n",
       " 'data/sample/train/silence/doing_the_dishes_53.wav',\n",
       " 'data/sample/train/silence/pink_noise_51.wav',\n",
       " 'data/sample/train/silence/white_noise_10.wav',\n",
       " 'data/sample/train/silence/doing_the_dishes_9.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_17.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_43.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_37.wav',\n",
       " 'data/sample/train/silence/doing_the_dishes_86.wav',\n",
       " 'data/sample/train/silence/running_tap_9.wav',\n",
       " 'data/sample/train/silence/running_tap_51.wav',\n",
       " 'data/sample/train/silence/exercise_bike_57.wav',\n",
       " 'data/sample/train/silence/pink_noise_14.wav',\n",
       " 'data/sample/train/silence/dude_miaowing_38.wav',\n",
       " 'data/sample/train/silence/pink_noise_20.wav',\n",
       " 'data/sample/train/silence/white_noise_1.wav',\n",
       " 'data/sample/train/silence/running_tap_52.wav',\n",
       " 'data/sample/train/unknown/4def68db_nohash_0_seven.wav',\n",
       " 'data/sample/train/unknown/5705a0f9_nohash_0_eight.wav',\n",
       " 'data/sample/train/unknown/238c112c_nohash_0_two.wav',\n",
       " 'data/sample/train/unknown/b55a09be_nohash_1_wow.wav',\n",
       " 'data/sample/train/unknown/421ed23f_nohash_0_zero.wav',\n",
       " 'data/sample/train/unknown/d37e4bf1_nohash_0_nine.wav',\n",
       " 'data/sample/train/unknown/ab5ae445_nohash_0_five.wav',\n",
       " 'data/sample/train/unknown/a527cb3c_nohash_0_sheila.wav',\n",
       " 'data/sample/train/unknown/4a1e736b_nohash_1_seven.wav',\n",
       " 'data/sample/train/unknown/44f68a83_nohash_0_six.wav',\n",
       " 'data/sample/train/unknown/b9515bf3_nohash_3_six.wav',\n",
       " 'data/sample/train/unknown/8f4c551f_nohash_0_four.wav',\n",
       " 'data/sample/train/unknown/10ace7eb_nohash_2_six.wav',\n",
       " 'data/sample/train/unknown/179a61b7_nohash_0_bed.wav',\n",
       " 'data/sample/train/unknown/9f63152b_nohash_0_six.wav',\n",
       " 'data/sample/train/unknown/4d9e07cf_nohash_2_two.wav',\n",
       " 'data/sample/train/unknown/e7ea8b76_nohash_0_cat.wav',\n",
       " 'data/sample/train/unknown/c0f8f4df_nohash_0_two.wav',\n",
       " 'data/sample/train/unknown/c1d39ce8_nohash_6_zero.wav',\n",
       " 'data/sample/train/unknown/1ecfb537_nohash_4_four.wav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first grab the training set\n",
    "path_to_train = os.path.join(path_to_sample, \"train\")\n",
    "sample_train_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_train, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    \n",
    "    # we use extend instead of append to add all elements from the iterable\n",
    "    sample_train_wavs.extend(category_files)\n",
    "    \n",
    "sample_train_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for cv\n",
    "path_to_cv = os.path.join(path_to_sample, \"cv\")\n",
    "sample_cv_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_cv, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    sample_cv_wavs.extend(category_files)\n",
    "\n",
    "# repeat for test\n",
    "path_to_test = os.path.join(path_to_sample, \"test\")\n",
    "sample_test_wavs = []\n",
    "\n",
    "for category in categories_to_predict:\n",
    "    path_to_category = os.path.join(path_to_test, category)\n",
    "    category_files = utils.grab_wavs(path_to_category)\n",
    "    sample_test_wavs.extend(category_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode the y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the 3 lists of files from each set (train, cv and test) we can construct our train_y, cv_y and test_y numpy arrays. These will be matrices of size (m, 12), one-hot encoded. E.g. if a row belongs to the category \"up\" it will take the form of an array of zeros, where the entry at index 2 (the third from the left) will become a 1.\n",
    "\n",
    "We will use a function from the utils that takes a path to a .wav, the index at which the category name starts within it (we want to control this because we will eventually use this for the main set, not just the sample) and a list of categories to predict. For our current example, the category name in the paths belonging to \"train\" starts at the 18th index (separators count as one char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sample/train/left/4ec7d027_nohash_0.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's grab a single path (this one is an \"left\")\n",
    "a_wav = sample_train_wavs[80]\n",
    "a_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see if the 1 is correctly placed\n",
    "utils.one_hot_encode_path(a_wav, 18, categories_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path belonged to the fifth category (\"left\") and the one-hot encoding correctly placed the 1 at index 4 (zero-indexed).\n",
    "\n",
    "We want to repeat this for all examples in each of the 3 subsets, adding each new one-hot encoded numpy array as a new row of the y matrix, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out the dimensions of train_y\n",
    "rows = len(sample_train_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape: 2880\n",
      "New shape: (240, 12)\n"
     ]
    }
   ],
   "source": [
    "# create train_y as empty array\n",
    "train_y = np.array([])\n",
    "\n",
    "# append each row to train_y\n",
    "for path_to_wav in sample_train_wavs:\n",
    "    row = utils.one_hot_encode_path(path_to_wav, 18, categories_to_predict)\n",
    "    \n",
    "    # append the new row\n",
    "    train_y = np.append(train_y, row)\n",
    "    \n",
    "# we currently have a flattened vector\n",
    "print(\"Current shape: {}\".format(*train_y.shape))\n",
    "\n",
    "# let's reshape it\n",
    "train_y = np.reshape(train_y, dimensions)\n",
    "print(\"New shape: {}\".format(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the train_y matrix to confirm\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first 3 entries have the 1 at 0th index, which means they belong to category \"up\" and the last three have the 1 at the last index, which is also correct given the fact that our list of paths was also ordered.\n",
    "\n",
    "We should bear in mind that by default the np.array contains float64s and our functions for loading a .wav return int16s.\n",
    "\n",
    "Since this is a highly-repetitive task we'll want to use the utils function for obtaining the y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for **CV set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dimensions: (60, 12)\n",
      "Received shape: (60, 12)\n"
     ]
    }
   ],
   "source": [
    "# figure out the dimensions\n",
    "rows = len(sample_cv_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "print(\"Target dimensions: {}\".format(dimensions))\n",
    "\n",
    "# get the y\n",
    "cv_y = utils.get_y(sample_cv_wavs, 15, categories_to_predict)\n",
    "print(\"Received shape: {}\".format(cv_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for **Test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dimensions: (60, 12)\n",
      "Received shape: (60, 12)\n"
     ]
    }
   ],
   "source": [
    "# figure out the dimensions\n",
    "rows = len(sample_test_wavs)\n",
    "columns = len(categories_to_predict)\n",
    "dimensions = (rows, columns)\n",
    "print(\"Target dimensions: {}\".format(dimensions))\n",
    "\n",
    "# get the y\n",
    "test_y = utils.get_y(sample_test_wavs, 17, categories_to_predict)\n",
    "print(\"Received shape: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the X\n",
    "We have the y - the one-hot encoded vectors representing the category for each training, cv and test example in the sample set. We need the feature vectors, conventionally referred to as X. We will use both the simplest way of extracting the .wav data and the preprocessing techniques - MFCCs, Mel spectrogram, FFT and tempogram.\n",
    "\n",
    "Let's start by defining a simple helper function for just the raw .wav data. Since our samples are of slightly differing lengths but each row of our X always has to have the same length, we will **add padding by default.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the desired number of columns (n)\n",
    "n = len(utils.get_wav_info(path_to_wav)[1])\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw .wav data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple helper function\n",
    "def get_X_with_padding(list_of_paths, columns=16000):\n",
    "    \n",
    "    # get shape data\n",
    "    rows = len(list_of_paths)\n",
    "    dimensions = (rows, columns)\n",
    "    \n",
    "    # create placeholder\n",
    "    X = np.array([])\n",
    "    \n",
    "    # go through every file path in the list\n",
    "    for path_to_wav in list_of_paths:\n",
    "\n",
    "        # get raw array of signed ints\n",
    "        row = utils.get_wav_info(path_to_wav)[1]\n",
    "        \n",
    "        # some of our sample have less (or slightly more) than 16000 values, so let's adjust them\n",
    "        # trim to fixed length\n",
    "        row = row[:columns]\n",
    "        \n",
    "        # pad with zeros, calculating amount of padding needed\n",
    "        padding = columns - len(row)\n",
    "        row = np.pad(row, (0, padding), mode='constant', constant_values=0)\n",
    "\n",
    "        # append the new row\n",
    "        X = np.append(X, row)\n",
    "    \n",
    "    # reshape (unroll)\n",
    "    X = np.reshape(X, dimensions)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (240, 16000)\n",
      "CV:  (60, 16000)\n",
      "Test:  (60, 16000)\n"
     ]
    }
   ],
   "source": [
    "# get the X for each set\n",
    "train_X = utils.get_X(sample_train_wavs, n)\n",
    "cv_X = utils.get_X(sample_cv_wavs, n)\n",
    "test_X = utils.get_X(sample_test_wavs, n)\n",
    "\n",
    "print(\"Train: \", train_X.shape)\n",
    "print(\"CV: \", cv_X.shape)\n",
    "print(\"Test: \",test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-11., -21., -25., -42., -33.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same for the MFCCs. We have a choice of whether or not we want to get returned only the mean value (1D) for the MFCCs. For now let's obtain both the 1D (mean) and 2D version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with a reasonable number of mfccs to return\n",
    "n_mfcc = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mfccs:  (240, 100)\n",
      "CV mfccs:  (60, 100)\n",
      "Test mfccs:  (60, 100)\n"
     ]
    }
   ],
   "source": [
    "train_X_mfccs_1D = utils.get_X_mfccs(sample_train_wavs, shape=(n_mfcc, 32), mean=True)\n",
    "cv_X_mfccs_1D = utils.get_X_mfccs(sample_cv_wavs, shape=(n_mfcc, 32), mean=True)\n",
    "test_X_mfccs_1D = utils.get_X_mfccs(sample_test_wavs, shape=(n_mfcc, 32), mean=True)\n",
    "\n",
    "print(\"Train mfccs: \", train_X_mfccs_1D.shape)\n",
    "print(\"CV mfccs: \", cv_X_mfccs_1D.shape)\n",
    "print(\"Test mfccs: \",test_X_mfccs_1D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-445.35308297,   31.80853315,   -3.24210645,   18.580648  ,\n",
       "          0.94357346])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_mfccs_1D[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the 2-dim output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mfccs:  (240, 100, 32)\n",
      "CV mfccs:  (60, 100, 32)\n",
      "Test mfccs:  (60, 100, 32)\n"
     ]
    }
   ],
   "source": [
    "train_X_mfccs_2D = utils.get_X_mfccs(sample_train_wavs, shape=(n_mfcc, 32), mean=False)\n",
    "cv_X_mfccs_2D = utils.get_X_mfccs(sample_cv_wavs, shape=(n_mfcc, 32), mean=False)\n",
    "test_X_mfccs_2D = utils.get_X_mfccs(sample_test_wavs, shape=(n_mfcc, 32), mean=False)\n",
    "\n",
    "print(\"Train mfccs: \", train_X_mfccs_2D.shape)\n",
    "print(\"CV mfccs: \", cv_X_mfccs_2D.shape)\n",
    "print(\"Test mfccs: \",test_X_mfccs_2D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-576.51036871, -570.90724179, -544.55853427, -553.88026846,\n",
       "       -578.52544577])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_mfccs_2D[0][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel spectrogam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of Mel spectrograms we expect to get a matrix from a vector, therefore our final X will be 3 dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (16000,)\n",
      "Mel spectrogram shape: (128, 32)\n"
     ]
    }
   ],
   "source": [
    "# let's see the difference in dimensions\n",
    "sr, raw_data = utils.get_wav_info(path_to_wav)\n",
    "print(\"Raw data shape: {}\".format(raw_data.shape))\n",
    "x = librosa.feature.melspectrogram(raw_data, sr)\n",
    "print(\"Mel spectrogram shape: {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's the function we'll use (via utils.py)\n",
    "def get_X_mel_spectrogram(list_of_paths, shape=(128, 32)):\n",
    "\n",
    "    # get shape data\n",
    "    rows = len(list_of_paths)\n",
    "\n",
    "    # create placeholder\n",
    "    result = np.array([])\n",
    "\n",
    "    # go through every file path in the list\n",
    "    for path_to_wav in list_of_paths:\n",
    "        \n",
    "        # get raw array of signed ints\n",
    "        sr, raw_data = utils.get_wav_info(path_to_wav)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(raw_data, sr)\n",
    "\n",
    "        # some of our samples have less (or slightly more) than the expected amount of values,\n",
    "        # so let's adjust them\n",
    "        placeholder = np.array([])\n",
    "        for row in mel_spectrogram:\n",
    "            \n",
    "            # trim to fixed length\n",
    "            row = row[:shape[1]]\n",
    "\n",
    "            # pad with zeros, calculating amount of padding needed\n",
    "            padding = shape[1] - len(row)\n",
    "            row = np.pad(row, (0, padding), mode='constant', constant_values=0)\n",
    "\n",
    "            # append the new row\n",
    "            placeholder = np.append(placeholder, row)\n",
    "        \n",
    "        # append the new unrolled matrix to the final result array\n",
    "        result = np.append(result, placeholder)\n",
    "    \n",
    "    # reshape into a 3-dim matrix\n",
    "    result = np.reshape(result, (len(list_of_paths), shape[0], shape[1]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the Mel spectrograms for all sample sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mel spectrogram:  (240, 128, 32)\n",
      "CV mel spectrogram:  (60, 128, 32)\n",
      "Test mel spectrogram:  (60, 128, 32)\n"
     ]
    }
   ],
   "source": [
    "train_X_mel_spectrogram = utils.get_X_mel_spectrogram(sample_train_wavs)\n",
    "cv_X_mel_spectrogram = utils.get_X_mel_spectrogram(sample_cv_wavs)\n",
    "test_X_mel_spectrogram = utils.get_X_mel_spectrogram(sample_test_wavs)\n",
    "\n",
    "print(\"Train mel spectrogram: \", train_X_mel_spectrogram.shape)\n",
    "print(\"CV mel spectrogram: \", cv_X_mel_spectrogram.shape)\n",
    "print(\"Test mel spectrogram: \",test_X_mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1457703.31342286,  427352.21837965,  201594.17632909,\n",
       "        238226.53307341,   65491.03447233,  178705.29710062,\n",
       "        412570.67348324,  378435.94871593,  258376.05295182,\n",
       "        187066.96734191,  239451.9017311 ,   56142.51095658,\n",
       "         42836.42147842,  139791.55001964,  102884.20436902,\n",
       "        167352.41037348,  321818.54914338,  559749.40569307,\n",
       "        989871.95717842,  918093.81247816, 1827327.90723131,\n",
       "       1677686.13316353,  673552.71678095,  419856.38671465,\n",
       "        384360.98528384,  454044.27309286,  670187.30942213,\n",
       "        427213.30004477,  395041.75416788,  548740.11881148,\n",
       "        294776.165019  ,  336066.61377942])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row is a 2D matrix (hence double-indexing)\n",
    "train_X_mel_spectrogram[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFT (Fast Fourier Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the FFT of our raw data too. For simplicity the utils.get_X_fft() function casts the complex numbers to the numpy float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here the shapes are the same\n",
    "x = utils.extract_fft(path_to_wav)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fft:  (240, 16000)\n",
      "CV fft:  (60, 16000)\n",
      "Test fft:  (60, 16000)\n"
     ]
    }
   ],
   "source": [
    "train_X_fft = utils.get_X_fft(sample_train_wavs)\n",
    "cv_X_fft = utils.get_X_fft(sample_cv_wavs)\n",
    "test_X_fft = utils.get_X_fft(sample_test_wavs)\n",
    "\n",
    "print(\"Train fft: \", train_X_fft.shape)\n",
    "print(\"CV fft: \", cv_X_fft.shape)\n",
    "print(\"Test fft: \",test_X_fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-11519.1640625 ,   -783.21451673,  -2758.71579679,   4396.02183472,\n",
       "         3284.95650186])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no longer complex numbers\n",
    "print(type(test_X_fft[0][0]))\n",
    "test_X_fft[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tempogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tempogram we have to do some reshaping to get a 3D matrix, just like with Mel spectrograms. We will also have to do a little bit of padding and trimming, to account for small differences in the length of the original sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempogram: (384, 32)\n"
     ]
    }
   ],
   "source": [
    "# let's see the difference in dimensions\n",
    "x = utils.extract_tempogram(path_to_wav)\n",
    "print(\"Tempogram: {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tempogram:  (240, 384, 32)\n",
      "CV tempogram:  (60, 384, 32)\n",
      "Test tempogram:  (60, 384, 32)\n"
     ]
    }
   ],
   "source": [
    "train_X_tempogram = utils.get_X_tempogram(sample_train_wavs)\n",
    "cv_X_tempogram = utils.get_X_tempogram(sample_cv_wavs)\n",
    "test_X_tempogram = utils.get_X_tempogram(sample_test_wavs)\n",
    "\n",
    "print(\"Train tempogram: \", train_X_tempogram.shape)\n",
    "print(\"CV tempogram: \", cv_X_tempogram.shape)\n",
    "print(\"Test tempogram: \",test_X_tempogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1764312 , 0.17643258, 0.17643404, 0.17643558, 0.17643721,\n",
       "       0.17643893, 0.17644072, 0.1764426 , 0.17644456, 0.17644661,\n",
       "       0.17644875, 0.17645096, 0.17645327, 0.17645565, 0.17645813,\n",
       "       0.17646069, 0.17646334, 0.17646607, 0.1764689 , 0.17647181,\n",
       "       0.17647481, 0.1764779 , 0.17648109, 0.17648437, 0.17648774,\n",
       "       0.1764912 , 0.17649476, 0.17649842, 0.17650217, 0.17650602,\n",
       "       0.17650997, 0.17651403])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row is a 2D matrix (hence double-indexing)\n",
    "train_X_tempogram[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the preprocessed X and y\n",
    "It's good practice to persist your preprocessed datasets so that we don't have to recalculate all of the preprocessing (which in large datasets can be time-consuming). \n",
    "\n",
    "A great library for this purpose is the bcolz library (for binary columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the bcolz array saving functions\n",
    "def bcolz_save(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def bcolz_load(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/tensorflow_speech_recognition\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sample/preprocessed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_sample_preprocessed = os.path.join(path_to_sample, \"preprocessed\")\n",
    "path_to_sample_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory if it's not there already\n",
    "# !mkdir $path_to_sample_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the y\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_y\" + \".bc\", train_y)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_y\" + \".bc\", cv_y)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_y\" + \".bc\", test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the X\n",
    "# raw data\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X\" + \".bc\", train_X)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X\" + \".bc\", cv_X)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X\" + \".bc\", test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCCs (1dim and 2dim)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_mfccs_1D\" + \".bc\", train_X_mfccs_1D)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_mfccs_1D\" + \".bc\", cv_X_mfccs_1D)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_mfccs_1D\" + \".bc\", test_X_mfccs_1D)\n",
    "\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_mfccs_2D\" + \".bc\", train_X_mfccs_2D)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_mfccs_2D\" + \".bc\", cv_X_mfccs_2D)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_mfccs_2D\" + \".bc\", test_X_mfccs_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel spectrogram\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_mel_spectrogram\" + \".bc\", train_X_mel_spectrogram)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_mel_spectrogram\" + \".bc\", cv_X_mel_spectrogram)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_mel_spectrogram\" + \".bc\", test_X_mel_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_fft\" + \".bc\", train_X_fft)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_fft\" + \".bc\", cv_X_fft)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_fft\" + \".bc\", test_X_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempogram\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"train_X_tempogram\" + \".bc\", train_X_tempogram)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"cv_X_tempogram\" + \".bc\", cv_X_tempogram)\n",
    "bcolz_save(path_to_sample_preprocessed + os.path.sep + \"test_X_tempogram\" + \".bc\", test_X_tempogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the preprocessed X and y\n",
    "In order not to have to re-run the entire notebook to obtain the preprocessed X and the corresponding y matrices, let's reload them and then proceed to train simple models.\n",
    "\n",
    "If you're reloading the X & y after restarting the notebook you will also have to run the cells that define the bcolz functions and the path names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the y\n",
    "train_y = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_y\" + \".bc\")\n",
    "cv_y = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_y\" + \".bc\")\n",
    "test_y = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_y\" + \".bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload the X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 16000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the X\n",
    "# raw data\n",
    "train_X = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X\" + \".bc\")\n",
    "cv_X = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X\" + \".bc\")\n",
    "test_X = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X\" + \".bc\")\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 100)\n",
      "(240, 100, 32)\n"
     ]
    }
   ],
   "source": [
    "# MFCCs (1D and 2D)\n",
    "train_X_mfccs_1D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_mfccs_1D\" + \".bc\")\n",
    "cv_X_mfccs_1D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_mfccs_1D\" + \".bc\")\n",
    "test_X_mfccs_1D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_mfccs_1D\" + \".bc\")\n",
    "print(train_X_mfccs_1D.shape)\n",
    "\n",
    "train_X_mfccs_2D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_mfccs_2D\" + \".bc\")\n",
    "cv_X_mfccs_2D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_mfccs_2D\" + \".bc\")\n",
    "test_X_mfccs_2D = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_mfccs_2D\" + \".bc\")\n",
    "print(train_X_mfccs_2D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 128, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mel spectrogram\n",
    "train_X_mel_spectrogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_mel_spectrogram\" + \".bc\")\n",
    "cv_X_mel_spectrogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_mel_spectrogram\" + \".bc\")\n",
    "test_X_mel_spectrogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_mel_spectrogram\" + \".bc\")\n",
    "train_X_mel_spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 16000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FFT\n",
    "train_X_fft = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_fft\" + \".bc\")\n",
    "cv_X_fft = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_fft\" + \".bc\")\n",
    "test_X_fft = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_fft\" + \".bc\")\n",
    "train_X_fft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 384, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tempogram\n",
    "train_X_tempogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"train_X_tempogram\" + \".bc\")\n",
    "cv_X_tempogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"cv_X_tempogram\" + \".bc\")\n",
    "test_X_tempogram = bcolz_load(path_to_sample_preprocessed + os.path.sep + \"test_X_tempogram\" + \".bc\")\n",
    "train_X_tempogram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train simple models\n",
    "We will start by training the simplest models and then try out more and more complex architectures, aiming for the highest possible accuracy and F1 score.\n",
    "\n",
    "The simplest model we can try is a linear model, which we can obtain by using the Keras Dense layer followed by an activation function such as softmax (as in our case categories are mutually exclusive).\n",
    "\n",
    "Since we have 12 mutually exclusive categories, we need to get an **accuracy of more than 0.833%** to beat random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model\n",
    "We'll need to keep track of the dimensions that we pass into our models, so lets assign their values to separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features: 16000\n",
      "Categories to predict: 12\n"
     ]
    }
   ],
   "source": [
    "# we'll need the number of parameters and the output categories\n",
    "num_features = train_X.shape[1]\n",
    "num_categories = train_y.shape[1]\n",
    "print(\"Input features: {}\\nCategories to predict: {}\".format(num_features, num_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design & compile the model\n",
    "linear_model = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = num_categories, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "linear_model.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on random weights initialization (values will change everytime you compile the model)\n",
      "Categorical crossentropy (loss): 15.0436\n",
      "Accuracy: 0.07\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate our loss before fitting the model\n",
    "initial_score = linear_model.evaluate(test_X, test_y, verbose=0)\n",
    "categorical_crossentropy = initial_score[0]\n",
    "accuracy = initial_score[1]\n",
    "\n",
    "print(\"Based on random weights initialization (values will change everytime you compile the model)\\nCategorical crossentropy (loss): {:.4f}\\nAccuracy: {:.2f}\".format(categorical_crossentropy, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our simple linear model for a couple of epochs and see the **F1 score** and **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/5\n",
      "240/240 [==============================] - 0s - loss: 14.8369 - acc: 0.0792 - val_loss: 15.0436 - val_acc: 0.0667\n",
      "Epoch 2/5\n",
      "240/240 [==============================] - 0s - loss: 14.9092 - acc: 0.0750 - val_loss: 15.0436 - val_acc: 0.0667\n",
      "Epoch 3/5\n",
      "240/240 [==============================] - 0s - loss: 14.8421 - acc: 0.0792 - val_loss: 15.0436 - val_acc: 0.0667\n",
      "Epoch 4/5\n",
      "240/240 [==============================] - 0s - loss: 14.7749 - acc: 0.0833 - val_loss: 15.0436 - val_acc: 0.0667\n",
      "Epoch 5/5\n",
      "240/240 [==============================] - 0s - loss: 14.7749 - acc: 0.0833 - val_loss: 15.0436 - val_acc: 0.0667\n"
     ]
    }
   ],
   "source": [
    "# we pass our training data and our cross-validation data to see if we're not overfitting\n",
    "history = linear_model.fit(train_X, train_y, batch_size=32, epochs=5, validation_data=(cv_X, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scores\n",
      "Train acc: 0.0833\n",
      "CV acc: 0.0667\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "best_training_accuracy = max(history.history[\"acc\"])\n",
    "best_validation_accuracy = max(history.history[\"val_acc\"])\n",
    "print(\"Best scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the random initialization of weights we should have an **accuracy** score within 0.05 and 0.15 on both the training and cross-validation set. Let's also calculate the **F1 score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first use the model to predict the labels\n",
    "pred_cv_y = linear_model.predict(cv_X, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if shape matches expectation (number of examples, number of categories to predict)\n",
    "pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use softmax to get a result towards one-hot encoding, but not all rows will necessarily be just zeroes and one 1\n",
    "pred_cv_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before we pass our predictions to the sklearn's f1 score function we need to make sure that all of our rows are actually one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cv_y = utils.one_hot_encode(pred_cv_y)\n",
    "pred_cv_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final linear model CV accuracy via sklearn: 0.0667\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "sk_cv_accuracy = accuracy_score(cv_y, pred_cv_y)\n",
    "print(\"Final linear model CV accuracy via sklearn: {:.4f}\".format(sk_cv_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model f1 score (CV): 0.0513\n"
     ]
    }
   ],
   "source": [
    "# because we're dealing with a mutliclass classification challenge, we need to change the default value of average\n",
    "# (which is binary)\n",
    "cv_f1_score = f1_score(cv_y, pred_cv_y, average=\"weighted\")\n",
    "print(\"Linear model f1 score (CV): {:.4f}\".format(cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, our accuracy and F1 score for the simplest possible model fall within 0.05 - 0.15. This is our earliest benchmark to beat, and it's **not much better than random guessing**, which given 12 categories would give us an accuracy of 0.08333."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "It is also useful to try other ML methods before jumping into neural networks and deep learning. Random Forests are a simple but very often quite effective (and computationally inexpensive) method of obtaining a good benchmark.\n",
    "\n",
    "For the sklearn implementation of Random Forest we actually do not want our target to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the one-hot encoding\n",
    "rf_train_y = utils.reverse_one_hot_encoding(train_y)\n",
    "rf_cv_y = utils.reverse_one_hot_encoding(cv_y)\n",
    "rf_test_y = utils.reverse_one_hot_encoding(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "rand_forest.fit(train_X, rf_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  1.,  6.,  8.,  1.,  3.,  4.,  6.,  4.,  1.,  7., 11., 12.,\n",
       "        3.,  1.,  3.,  9.,  2.,  3.,  8.,  9.,  4.,  1.,  4.,  1.,  5.,\n",
       "        2.,  8.,  5.,  6.,  8.,  3.,  3.,  6.,  1.,  1.,  2.,  8.,  1.,\n",
       "        5.,  6.,  3.,  7.,  1.,  6., 10.,  5.,  2.,  9.,  6., 11.,  4.,\n",
       "       11.,  5.,  6.,  2.,  2.,  9.,  4.,  5.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predicted_cv_y = rand_forest.predict(cv_X)\n",
    "rf_predicted_cv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest f1 score (CV): 0.135\n",
      "Random forest accuracy (CV): 0.133\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and F1 for Random Forest\n",
    "rf_cv_f1_score = f1_score(rf_cv_y, rf_predicted_cv_y, average=\"weighted\")\n",
    "rf_cv_accuracy = accuracy_score(rf_cv_y, rf_predicted_cv_y)\n",
    "\n",
    "print(\"Random forest f1 score (CV): {:.3f}\".format(rf_cv_f1_score))\n",
    "print(\"Random forest accuracy (CV): {:.3f}\".format(rf_cv_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Random Forest method, using only default parameters (except for max depth), we are getting an **F1 score and accuracy around 0.10 - 0.15**.<br/> Slightly better than random, nowhere near good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set benchmark\n",
    "best_cv_acc = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Networks\n",
    "Now that we have a benchmark obtained via simple linear and Random Forest models we can proceed towards trying to outdo it with MLPs and deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP - multi-layer perceptron\n",
    "Let's start with the simplest possible neural network of just 2 dense layers. We'll be working only on the mfccs data from now on, as it tends to produce better results. We will also add **batch normalization** and **dropout** to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design & compile the model\n",
    "num_nodes = 2000\n",
    "mlp = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = num_nodes, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.96),\n",
    "    Dense(num_categories, activation='softmax')\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "mlp.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "240/240 [==============================] - 3s - loss: 8.2435 - acc: 0.0958 - val_loss: 6.3584 - val_acc: 0.0833\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 2s - loss: 7.8047 - acc: 0.1000 - val_loss: 3.9879 - val_acc: 0.0667\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 2s - loss: 6.5026 - acc: 0.1292 - val_loss: 3.5573 - val_acc: 0.1000\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 2s - loss: 6.4385 - acc: 0.1167 - val_loss: 3.2334 - val_acc: 0.0833\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 2s - loss: 6.0878 - acc: 0.1583 - val_loss: 2.9742 - val_acc: 0.1333\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 2s - loss: 5.5775 - acc: 0.2042 - val_loss: 2.9128 - val_acc: 0.1500\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 2s - loss: 6.0301 - acc: 0.1750 - val_loss: 2.8771 - val_acc: 0.1500\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 2s - loss: 5.5120 - acc: 0.1917 - val_loss: 2.8346 - val_acc: 0.1500\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 2s - loss: 5.1921 - acc: 0.2375 - val_loss: 2.8122 - val_acc: 0.1333\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 2s - loss: 4.7076 - acc: 0.3000 - val_loss: 2.7891 - val_acc: 0.1500\n"
     ]
    }
   ],
   "source": [
    "mlp_results = mlp.fit(train_X, train_y, batch_size=32, epochs=10, validation_data=(cv_X, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MLP scores\n",
      "Train acc: 0.3000\n",
      "CV acc: 0.1500\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "best_training_accuracy = max(mlp_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(mlp_results.history[\"val_acc\"])\n",
    "print(\"Best MLP scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "mlp_pred_cv_y = mlp.predict(cv_X, batch_size=32)\n",
    "mlp_pred_cv_y = utils.one_hot_encode(mlp_pred_cv_y)\n",
    "mlp_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy via sklearn (CV): 0.1500\n",
      "MLP f1 score (CV): 0.1499\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "mlp_cv_accuracy = accuracy_score(cv_y, mlp_pred_cv_y)\n",
    "mlp_cv_f1_score = f1_score(cv_y, mlp_pred_cv_y, average=\"weighted\")\n",
    "print(\"MLP accuracy via sklearn (CV): {:.4f}\".format(mlp_cv_accuracy))\n",
    "print(\"MLP f1 score (CV): {:.4f}\".format(mlp_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a simple MLP model reaches a very similar accuracy score to our previous benchmark of 0.15. Both this one and the previous ones can be tuned to reach approximately 0.25 but let's save fine-tuning for when we have a more promising approach - we are also already overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural Networks\n",
    "Let's try adding more layers to capture more complex interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = Sequential([\n",
    "    Dense(input_shape=(num_features,), units = 4000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.8),\n",
    "    Dense(3000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.85),\n",
    "    Dense(2000, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.9),\n",
    "    Dense(num_categories, activation='softmax')\n",
    "])\n",
    "\n",
    "# we choose the Adam optimizer with a specific learning rate\n",
    "dnn.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "240/240 [==============================] - 5s - loss: 6.7286 - acc: 0.0958 - val_loss: 7.9826 - val_acc: 0.0833\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 3s - loss: 6.5261 - acc: 0.0917 - val_loss: 4.9535 - val_acc: 0.1000\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 3s - loss: 6.0344 - acc: 0.1333 - val_loss: 4.2601 - val_acc: 0.0833\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 3s - loss: 5.7909 - acc: 0.0875 - val_loss: 3.7598 - val_acc: 0.0500\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 3s - loss: 6.0803 - acc: 0.1000 - val_loss: 3.2002 - val_acc: 0.0500\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 3s - loss: 5.9077 - acc: 0.0958 - val_loss: 3.0013 - val_acc: 0.0667\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 3s - loss: 5.6614 - acc: 0.0792 - val_loss: 2.9356 - val_acc: 0.1167\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 3s - loss: 5.8123 - acc: 0.0958 - val_loss: 2.8923 - val_acc: 0.1500\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 3s - loss: 5.5146 - acc: 0.1375 - val_loss: 2.9007 - val_acc: 0.1500\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 3s - loss: 5.2759 - acc: 0.1500 - val_loss: 2.9153 - val_acc: 0.1500\n"
     ]
    }
   ],
   "source": [
    "dnn_results = dnn.fit(train_X, train_y, batch_size=64, epochs=10, validation_data=(cv_X, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best DNN scores\n",
      "Train acc: 0.1500\n",
      "CV acc: 0.1500\n"
     ]
    }
   ],
   "source": [
    "# show latest results\n",
    "best_training_accuracy = max(dnn_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(dnn_results.history[\"val_acc\"])\n",
    "print(\"Best DNN scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "dnn_pred_cv_y = dnn.predict(cv_X, batch_size=32)\n",
    "dnn_pred_cv_y = utils.one_hot_encode(dnn_pred_cv_y)\n",
    "dnn_pred_cv_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN accuracy via sklearn (CV): 0.1500\n",
      "DNN f1 score (CV): 0.1133\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "dnn_cv_accuracy = accuracy_score(cv_y, dnn_pred_cv_y)\n",
    "dnn_cv_f1_score = f1_score(cv_y, dnn_pred_cv_y, average=\"weighted\")\n",
    "print(\"DNN accuracy via sklearn (CV): {:.4f}\".format(dnn_cv_accuracy))\n",
    "print(\"DNN f1 score (CV): {:.4f}\".format(dnn_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Models\n",
    "Seems we're stuck around 0.15 accuracy. That makes sense because the actual \"no\" and other words may come at any place in the vector, we can't really keep being attached to specific indexes when training (which we currently are). Let's try convolutional layers, which can find certain patterns regardless of whether they appear at the start or end of the file.\n",
    "\n",
    "We will also move towards using our preprocessed data as convolutions work better with data that conveys dimensionality, beginning with mean MFCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 100, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to use convolutions we have to reshape our X -> expand it to 3 dimensions\n",
    "conv_train_X_mfccs_1D = np.expand_dims(train_X_mfccs_1D, axis=2)\n",
    "conv_train_X_mfccs_1D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for cv & test\n",
    "conv_cv_X_mfccs_1D = np.expand_dims(cv_X_mfccs_1D, axis=2)\n",
    "conv_test_X_mfccs_1D = np.expand_dims(test_X_mfccs_1D, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = Sequential([\n",
    "        Convolution1D(input_shape=(conv_train_X_mfccs_1D.shape[1], 1), kernel_size=64, filters=64, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.12),\n",
    "        MaxPooling1D(),\n",
    "        Convolution1D(kernel_size=64, filters=64, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.12),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(2000, activation=\"relu\"),\n",
    "        Dropout(.7),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "cnn1.compile(Adam(lr=0.0001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/80\n",
      "240/240 [==============================] - 2s - loss: 9.0299 - acc: 0.0917 - val_loss: 4.7122 - val_acc: 0.1167\n",
      "Epoch 2/80\n",
      "240/240 [==============================] - 0s - loss: 7.4429 - acc: 0.1208 - val_loss: 2.8284 - val_acc: 0.1167\n",
      "Epoch 3/80\n",
      "240/240 [==============================] - 0s - loss: 5.9821 - acc: 0.0750 - val_loss: 3.1201 - val_acc: 0.1500\n",
      "Epoch 4/80\n",
      "240/240 [==============================] - 0s - loss: 5.2001 - acc: 0.0958 - val_loss: 2.5003 - val_acc: 0.1833\n",
      "Epoch 5/80\n",
      "240/240 [==============================] - 0s - loss: 4.1264 - acc: 0.1000 - val_loss: 2.5581 - val_acc: 0.1167\n",
      "Epoch 6/80\n",
      "240/240 [==============================] - 0s - loss: 3.8282 - acc: 0.0833 - val_loss: 2.4932 - val_acc: 0.1500\n",
      "Epoch 7/80\n",
      "240/240 [==============================] - 0s - loss: 3.1335 - acc: 0.1375 - val_loss: 2.4176 - val_acc: 0.1833\n",
      "Epoch 8/80\n",
      "240/240 [==============================] - 0s - loss: 2.8177 - acc: 0.1375 - val_loss: 2.4147 - val_acc: 0.1500\n",
      "Epoch 9/80\n",
      "240/240 [==============================] - 0s - loss: 2.6024 - acc: 0.1750 - val_loss: 2.4203 - val_acc: 0.1833\n",
      "Epoch 10/80\n",
      "240/240 [==============================] - 0s - loss: 2.6968 - acc: 0.1542 - val_loss: 2.4164 - val_acc: 0.1333\n",
      "Epoch 11/80\n",
      "240/240 [==============================] - 0s - loss: 2.6537 - acc: 0.1208 - val_loss: 2.4124 - val_acc: 0.1000\n",
      "Epoch 12/80\n",
      "240/240 [==============================] - 0s - loss: 2.5441 - acc: 0.1292 - val_loss: 2.4133 - val_acc: 0.1167\n",
      "Epoch 13/80\n",
      "240/240 [==============================] - 0s - loss: 2.5055 - acc: 0.1667 - val_loss: 2.4194 - val_acc: 0.1500\n",
      "Epoch 14/80\n",
      "240/240 [==============================] - 0s - loss: 2.4240 - acc: 0.1333 - val_loss: 2.4230 - val_acc: 0.1333\n",
      "Epoch 15/80\n",
      "240/240 [==============================] - 0s - loss: 2.5024 - acc: 0.1292 - val_loss: 2.4231 - val_acc: 0.1167\n",
      "Epoch 16/80\n",
      "240/240 [==============================] - 0s - loss: 2.4374 - acc: 0.1292 - val_loss: 2.4203 - val_acc: 0.1500\n",
      "Epoch 17/80\n",
      "240/240 [==============================] - 0s - loss: 2.4345 - acc: 0.1250 - val_loss: 2.4158 - val_acc: 0.1833\n",
      "Epoch 18/80\n",
      "240/240 [==============================] - 0s - loss: 2.3647 - acc: 0.1625 - val_loss: 2.4073 - val_acc: 0.1667\n",
      "Epoch 19/80\n",
      "240/240 [==============================] - 0s - loss: 2.3628 - acc: 0.1917 - val_loss: 2.4035 - val_acc: 0.1833\n",
      "Epoch 20/80\n",
      "240/240 [==============================] - 0s - loss: 2.3965 - acc: 0.1333 - val_loss: 2.3932 - val_acc: 0.1500\n",
      "Epoch 21/80\n",
      "240/240 [==============================] - 0s - loss: 2.4011 - acc: 0.1667 - val_loss: 2.3862 - val_acc: 0.1667\n",
      "Epoch 22/80\n",
      "240/240 [==============================] - 0s - loss: 2.3722 - acc: 0.1750 - val_loss: 2.3813 - val_acc: 0.1500\n",
      "Epoch 23/80\n",
      "240/240 [==============================] - 0s - loss: 2.3800 - acc: 0.1750 - val_loss: 2.3693 - val_acc: 0.1833\n",
      "Epoch 24/80\n",
      "240/240 [==============================] - 0s - loss: 2.3764 - acc: 0.1583 - val_loss: 2.3634 - val_acc: 0.2000\n",
      "Epoch 25/80\n",
      "240/240 [==============================] - 0s - loss: 2.3949 - acc: 0.1750 - val_loss: 2.3640 - val_acc: 0.2000\n",
      "Epoch 26/80\n",
      "240/240 [==============================] - 0s - loss: 2.3283 - acc: 0.1792 - val_loss: 2.3643 - val_acc: 0.1833\n",
      "Epoch 27/80\n",
      "240/240 [==============================] - 0s - loss: 2.3960 - acc: 0.1667 - val_loss: 2.3627 - val_acc: 0.2167\n",
      "Epoch 28/80\n",
      "240/240 [==============================] - 0s - loss: 2.3812 - acc: 0.1792 - val_loss: 2.3500 - val_acc: 0.2167\n",
      "Epoch 29/80\n",
      "240/240 [==============================] - 0s - loss: 2.3713 - acc: 0.1833 - val_loss: 2.3450 - val_acc: 0.2167\n",
      "Epoch 30/80\n",
      "240/240 [==============================] - 0s - loss: 2.3721 - acc: 0.1750 - val_loss: 2.3437 - val_acc: 0.1667\n",
      "Epoch 31/80\n",
      "240/240 [==============================] - 0s - loss: 2.3109 - acc: 0.1792 - val_loss: 2.3467 - val_acc: 0.1500\n",
      "Epoch 32/80\n",
      "240/240 [==============================] - 0s - loss: 2.3025 - acc: 0.2167 - val_loss: 2.3400 - val_acc: 0.1167\n",
      "Epoch 33/80\n",
      "240/240 [==============================] - 0s - loss: 2.3430 - acc: 0.1583 - val_loss: 2.3277 - val_acc: 0.0833\n",
      "Epoch 34/80\n",
      "240/240 [==============================] - 0s - loss: 2.3321 - acc: 0.1625 - val_loss: 2.3122 - val_acc: 0.1500\n",
      "Epoch 35/80\n",
      "240/240 [==============================] - 0s - loss: 2.3233 - acc: 0.2208 - val_loss: 2.2990 - val_acc: 0.2167\n",
      "Epoch 36/80\n",
      "240/240 [==============================] - 0s - loss: 2.3479 - acc: 0.1708 - val_loss: 2.2911 - val_acc: 0.2000\n",
      "Epoch 37/80\n",
      "240/240 [==============================] - 0s - loss: 2.3217 - acc: 0.1625 - val_loss: 2.2967 - val_acc: 0.2000\n",
      "Epoch 38/80\n",
      "240/240 [==============================] - 0s - loss: 2.2992 - acc: 0.1833 - val_loss: 2.3089 - val_acc: 0.2000\n",
      "Epoch 39/80\n",
      "240/240 [==============================] - 0s - loss: 2.2897 - acc: 0.2042 - val_loss: 2.3225 - val_acc: 0.2000\n",
      "Epoch 40/80\n",
      "240/240 [==============================] - 0s - loss: 2.2893 - acc: 0.2083 - val_loss: 2.3205 - val_acc: 0.2000\n",
      "Epoch 41/80\n",
      "240/240 [==============================] - 0s - loss: 2.2908 - acc: 0.2000 - val_loss: 2.3127 - val_acc: 0.2000\n",
      "Epoch 42/80\n",
      "240/240 [==============================] - 0s - loss: 2.3441 - acc: 0.1500 - val_loss: 2.3110 - val_acc: 0.1833\n",
      "Epoch 43/80\n",
      "240/240 [==============================] - 0s - loss: 2.2787 - acc: 0.2167 - val_loss: 2.3102 - val_acc: 0.1500\n",
      "Epoch 44/80\n",
      "240/240 [==============================] - 0s - loss: 2.3171 - acc: 0.1958 - val_loss: 2.3090 - val_acc: 0.1833\n",
      "Epoch 45/80\n",
      "240/240 [==============================] - 0s - loss: 2.2361 - acc: 0.2375 - val_loss: 2.3033 - val_acc: 0.2000\n",
      "Epoch 46/80\n",
      "240/240 [==============================] - 0s - loss: 2.3483 - acc: 0.1667 - val_loss: 2.2933 - val_acc: 0.2167\n",
      "Epoch 47/80\n",
      "240/240 [==============================] - 0s - loss: 2.2224 - acc: 0.2167 - val_loss: 2.2827 - val_acc: 0.2000\n",
      "Epoch 48/80\n",
      "240/240 [==============================] - 0s - loss: 2.2371 - acc: 0.1833 - val_loss: 2.2773 - val_acc: 0.2500\n",
      "Epoch 49/80\n",
      "240/240 [==============================] - 0s - loss: 2.2331 - acc: 0.2208 - val_loss: 2.2763 - val_acc: 0.3000\n",
      "Epoch 50/80\n",
      "240/240 [==============================] - 0s - loss: 2.2674 - acc: 0.2542 - val_loss: 2.2832 - val_acc: 0.2333\n",
      "Epoch 51/80\n",
      "240/240 [==============================] - 0s - loss: 2.2463 - acc: 0.2000 - val_loss: 2.2698 - val_acc: 0.2000\n",
      "Epoch 52/80\n",
      "240/240 [==============================] - 0s - loss: 2.2260 - acc: 0.2292 - val_loss: 2.2522 - val_acc: 0.2333\n",
      "Epoch 53/80\n",
      "240/240 [==============================] - 0s - loss: 2.2428 - acc: 0.2833 - val_loss: 2.2446 - val_acc: 0.2167\n",
      "Epoch 54/80\n",
      "240/240 [==============================] - 0s - loss: 2.1467 - acc: 0.2583 - val_loss: 2.2426 - val_acc: 0.1833\n",
      "Epoch 55/80\n",
      "240/240 [==============================] - 0s - loss: 2.2054 - acc: 0.2542 - val_loss: 2.2521 - val_acc: 0.2167\n",
      "Epoch 56/80\n",
      "240/240 [==============================] - 0s - loss: 2.2014 - acc: 0.2667 - val_loss: 2.2558 - val_acc: 0.1667\n",
      "Epoch 57/80\n",
      "240/240 [==============================] - 0s - loss: 2.2168 - acc: 0.2250 - val_loss: 2.2482 - val_acc: 0.2167\n",
      "Epoch 58/80\n",
      "240/240 [==============================] - 0s - loss: 2.1396 - acc: 0.2750 - val_loss: 2.2372 - val_acc: 0.2667\n",
      "Epoch 59/80\n",
      "240/240 [==============================] - 0s - loss: 2.1468 - acc: 0.2458 - val_loss: 2.2271 - val_acc: 0.2333\n",
      "Epoch 60/80\n",
      "240/240 [==============================] - 0s - loss: 2.1570 - acc: 0.2542 - val_loss: 2.2444 - val_acc: 0.2167\n",
      "Epoch 61/80\n",
      "240/240 [==============================] - 0s - loss: 2.1209 - acc: 0.2708 - val_loss: 2.2364 - val_acc: 0.2000\n",
      "Epoch 62/80\n",
      "240/240 [==============================] - 0s - loss: 2.1236 - acc: 0.2208 - val_loss: 2.2220 - val_acc: 0.2000\n",
      "Epoch 63/80\n",
      "240/240 [==============================] - 0s - loss: 2.1445 - acc: 0.2750 - val_loss: 2.2162 - val_acc: 0.2333\n",
      "Epoch 64/80\n",
      "240/240 [==============================] - 0s - loss: 2.1275 - acc: 0.2542 - val_loss: 2.2072 - val_acc: 0.2000\n",
      "Epoch 65/80\n",
      "240/240 [==============================] - 0s - loss: 2.1148 - acc: 0.3125 - val_loss: 2.2010 - val_acc: 0.1667\n",
      "Epoch 66/80\n",
      "240/240 [==============================] - 0s - loss: 2.1534 - acc: 0.2625 - val_loss: 2.1916 - val_acc: 0.2000\n",
      "Epoch 67/80\n",
      "240/240 [==============================] - 0s - loss: 2.1514 - acc: 0.2375 - val_loss: 2.1942 - val_acc: 0.2000\n",
      "Epoch 68/80\n",
      "240/240 [==============================] - 0s - loss: 2.1022 - acc: 0.2542 - val_loss: 2.2061 - val_acc: 0.1500\n",
      "Epoch 69/80\n",
      "240/240 [==============================] - 0s - loss: 2.0887 - acc: 0.3167 - val_loss: 2.2183 - val_acc: 0.1833\n",
      "Epoch 70/80\n",
      "240/240 [==============================] - 0s - loss: 2.0922 - acc: 0.2625 - val_loss: 2.2088 - val_acc: 0.2833\n",
      "Epoch 71/80\n",
      "240/240 [==============================] - 0s - loss: 2.1104 - acc: 0.2750 - val_loss: 2.1778 - val_acc: 0.2500\n",
      "Epoch 72/80\n",
      "240/240 [==============================] - 0s - loss: 2.0773 - acc: 0.2875 - val_loss: 2.1739 - val_acc: 0.2333\n",
      "Epoch 73/80\n",
      "240/240 [==============================] - 0s - loss: 2.0442 - acc: 0.3000 - val_loss: 2.1663 - val_acc: 0.2333\n",
      "Epoch 74/80\n",
      "240/240 [==============================] - 0s - loss: 2.0186 - acc: 0.2958 - val_loss: 2.1616 - val_acc: 0.2333\n",
      "Epoch 75/80\n",
      "240/240 [==============================] - 0s - loss: 2.0527 - acc: 0.2708 - val_loss: 2.1622 - val_acc: 0.2167\n",
      "Epoch 76/80\n",
      "240/240 [==============================] - 0s - loss: 2.0472 - acc: 0.2792 - val_loss: 2.1686 - val_acc: 0.2167\n",
      "Epoch 77/80\n",
      "240/240 [==============================] - 0s - loss: 2.0385 - acc: 0.2833 - val_loss: 2.1529 - val_acc: 0.2000\n",
      "Epoch 78/80\n",
      "240/240 [==============================] - 0s - loss: 2.0519 - acc: 0.2958 - val_loss: 2.1458 - val_acc: 0.1833\n",
      "Epoch 79/80\n",
      "240/240 [==============================] - 0s - loss: 1.9960 - acc: 0.2667 - val_loss: 2.1593 - val_acc: 0.2000\n",
      "Epoch 80/80\n",
      "240/240 [==============================] - 0s - loss: 1.9519 - acc: 0.3500 - val_loss: 2.1585 - val_acc: 0.2167\n"
     ]
    }
   ],
   "source": [
    "cnn1_results = cnn1.fit(conv_train_X_mfccs_1D, train_y, batch_size=64, epochs=80, \n",
    "                        validation_data=(conv_cv_X_mfccs_1D, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CNN 1 scores\n",
      "Train acc: 0.3500\n",
      "CV acc: 0.3000\n"
     ]
    }
   ],
   "source": [
    "# show best results\n",
    "best_training_accuracy = max(cnn1_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(cnn1_results.history[\"val_acc\"])\n",
    "print(\"Best CNN 1 scores\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CNN architecture should get to 0.3 accuracy within 50-70 epochs and then start to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "cnn1_pred_cv_y_mfccs_1D = cnn1.predict(conv_cv_X_mfccs_1D, batch_size=32)\n",
    "cnn1_pred_cv_y_mfccs_1D = utils.one_hot_encode(cnn1_pred_cv_y_mfccs_1D)\n",
    "cnn1_pred_cv_y_mfccs_1D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN 1 accuracy via sklearn (CV): 0.2167\n",
      "CNN 1 f1 score (CV): 0.2299\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "cnn1_cv_accuracy = accuracy_score(cv_y, cnn1_pred_cv_y_mfccs_1D)\n",
    "cnn1_cv_f1_score = f1_score(cv_y, cnn1_pred_cv_y_mfccs_1D, average=\"weighted\")\n",
    "print(\"CNN 1 accuracy via sklearn (CV): {:.4f}\".format(cnn1_cv_accuracy))\n",
    "print(\"CNN 1 f1 score (CV): {:.4f}\".format(cnn1_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional models (1D) with FFT\n",
    "We have another form of preprocessing that results in a 1D vector - the Fast Fourier Transform. Let's see how our convolutional model might perform in that area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 16000, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to use convolutions we have to reshape our X -> expand it to 3 dimensions\n",
    "conv_train_X_fft = np.expand_dims(train_X_fft, axis=2)\n",
    "conv_train_X_fft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the FFT results in a 16K column vector - which will also require a lot more computational resources to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for cv & test\n",
    "conv_cv_X_fft = np.expand_dims(cv_X_fft, axis=2)\n",
    "conv_test_X_fft = np.expand_dims(test_X_fft, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_fft = Sequential([\n",
    "    Convolution1D(input_shape=(conv_train_X_fft.shape[1], 1), kernel_size=64, filters=64, padding=\"same\", activation=\"relu\"),\n",
    "    Dropout(0.12),\n",
    "    MaxPooling1D(),\n",
    "    Convolution1D(kernel_size=64, filters=64, padding=\"same\", activation=\"relu\"),\n",
    "    Dropout(0.12),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(2000, activation=\"relu\"),\n",
    "    Dropout(.7),\n",
    "    Dense(num_categories, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "cnn_fft.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/20\n",
      "240/240 [==============================] - 89s - loss: 14.9422 - acc: 0.0667 - val_loss: 13.9690 - val_acc: 0.1333\n",
      "Epoch 2/20\n",
      "240/240 [==============================] - 84s - loss: 14.6406 - acc: 0.0917 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 3/20\n",
      "240/240 [==============================] - 77s - loss: 14.5734 - acc: 0.0958 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 4/20\n",
      "240/240 [==============================] - 76s - loss: 14.9092 - acc: 0.0750 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 5/20\n",
      "240/240 [==============================] - 76s - loss: 14.9764 - acc: 0.0708 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 6/20\n",
      "240/240 [==============================] - 77s - loss: 14.9092 - acc: 0.0750 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 7/20\n",
      "240/240 [==============================] - 81s - loss: 14.7749 - acc: 0.0833 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 8/20\n",
      "240/240 [==============================] - 80s - loss: 14.7749 - acc: 0.0833 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 9/20\n",
      "240/240 [==============================] - 84s - loss: 14.7078 - acc: 0.0875 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 10/20\n",
      "240/240 [==============================] - 80s - loss: 14.5734 - acc: 0.0958 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 11/20\n",
      "240/240 [==============================] - 84s - loss: 14.7749 - acc: 0.0833 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 12/20\n",
      "240/240 [==============================] - 79s - loss: 15.0436 - acc: 0.0667 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 13/20\n",
      "240/240 [==============================] - 75s - loss: 15.1779 - acc: 0.0583 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 14/20\n",
      "240/240 [==============================] - 76s - loss: 14.7749 - acc: 0.0833 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 15/20\n",
      "240/240 [==============================] - 74s - loss: 15.0436 - acc: 0.0667 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 16/20\n",
      "240/240 [==============================] - 76s - loss: 14.6406 - acc: 0.0917 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 17/20\n",
      "240/240 [==============================] - 77s - loss: 14.8421 - acc: 0.0792 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 18/20\n",
      "240/240 [==============================] - 84s - loss: 15.1107 - acc: 0.0625 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 19/20\n",
      "240/240 [==============================] - 82s - loss: 14.7749 - acc: 0.0833 - val_loss: 14.7749 - val_acc: 0.0833\n",
      "Epoch 20/20\n",
      "240/240 [==============================] - 83s - loss: 14.9092 - acc: 0.0750 - val_loss: 14.7749 - val_acc: 0.0833\n"
     ]
    }
   ],
   "source": [
    "cnn_fft_results = cnn_fft.fit(conv_train_X_fft, train_y, batch_size=64, epochs=20, \n",
    "                        validation_data=(conv_cv_X_fft, cv_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the number of columns in the 1D vector results in a significant increase in the training time per epoch and we don't seem to be converging on better predictions (can't even fit the training set well with a fairly basic setup of the layers - most probably our kernel sizes are too small given the size of the vector). Let's leave this approach for now. Important practical aspect of ML: human time is ultimately the most valuable resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional models (2D) withMFCCs\n",
    "Let's try a similar model on the _2D MFCC data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 100, 32)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our data is already 2D, we don't need to expand dimensions\n",
    "train_X_mfccs_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = Sequential([\n",
    "        Convolution1D(input_shape=(train_X_mfccs_2D.shape[1], train_X_mfccs_2D.shape[2]), \n",
    "                      kernel_size=12, filters=128, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.11),\n",
    "        MaxPooling1D(),\n",
    "        Convolution1D(kernel_size=12, filters=128, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.13),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(2000, activation=\"relu\"),\n",
    "        Dropout(.7),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "cnn2.compile(Adam(lr=0.0001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 5s - loss: 13.8843 - acc: 0.0625 - val_loss: 12.1183 - val_acc: 0.1500\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 0s - loss: 13.3157 - acc: 0.1208 - val_loss: 12.3101 - val_acc: 0.1667\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 0s - loss: 13.7101 - acc: 0.1000 - val_loss: 13.8039 - val_acc: 0.1333\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 0s - loss: 13.7622 - acc: 0.0917 - val_loss: 13.9723 - val_acc: 0.1333\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 0s - loss: 13.8633 - acc: 0.1125 - val_loss: 13.7956 - val_acc: 0.1333\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 0s - loss: 13.6308 - acc: 0.1250 - val_loss: 12.3735 - val_acc: 0.1667\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 0s - loss: 13.6375 - acc: 0.1167 - val_loss: 12.7424 - val_acc: 0.1167\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 0s - loss: 13.4899 - acc: 0.1125 - val_loss: 12.7566 - val_acc: 0.1167\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 0s - loss: 12.8714 - acc: 0.1542 - val_loss: 12.3217 - val_acc: 0.1167\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 0s - loss: 12.5884 - acc: 0.1542 - val_loss: 11.3360 - val_acc: 0.1833\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 0s - loss: 13.5296 - acc: 0.1167 - val_loss: 12.1098 - val_acc: 0.1500\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 0s - loss: 13.2045 - acc: 0.1167 - val_loss: 12.9681 - val_acc: 0.1500\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 0s - loss: 12.8995 - acc: 0.1625 - val_loss: 13.2857 - val_acc: 0.1500\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 0s - loss: 12.8826 - acc: 0.1667 - val_loss: 13.2962 - val_acc: 0.1500\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 0s - loss: 12.6220 - acc: 0.1792 - val_loss: 13.0474 - val_acc: 0.1500\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 0s - loss: 12.2717 - acc: 0.1917 - val_loss: 12.4257 - val_acc: 0.1833\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 0s - loss: 12.4744 - acc: 0.1917 - val_loss: 11.8660 - val_acc: 0.1167\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 0s - loss: 12.8951 - acc: 0.1542 - val_loss: 12.3967 - val_acc: 0.1167\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 0s - loss: 12.2048 - acc: 0.1833 - val_loss: 12.3090 - val_acc: 0.1333\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 0s - loss: 12.3146 - acc: 0.1875 - val_loss: 12.0581 - val_acc: 0.1667\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 0s - loss: 12.9351 - acc: 0.1500 - val_loss: 12.3656 - val_acc: 0.1500\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 0s - loss: 12.0926 - acc: 0.1792 - val_loss: 12.1894 - val_acc: 0.1667\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 0s - loss: 12.4511 - acc: 0.1875 - val_loss: 11.5998 - val_acc: 0.2000\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 0s - loss: 12.6979 - acc: 0.1708 - val_loss: 11.3153 - val_acc: 0.1667\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 0s - loss: 12.2806 - acc: 0.1708 - val_loss: 12.5651 - val_acc: 0.1667\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 0s - loss: 12.5864 - acc: 0.1833 - val_loss: 12.2928 - val_acc: 0.2000\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 0s - loss: 12.4074 - acc: 0.1833 - val_loss: 12.4509 - val_acc: 0.1500\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 0s - loss: 11.8533 - acc: 0.2000 - val_loss: 12.4813 - val_acc: 0.1333\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 0s - loss: 12.5673 - acc: 0.1667 - val_loss: 10.8042 - val_acc: 0.1333\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 0s - loss: 11.6499 - acc: 0.2042 - val_loss: 10.6129 - val_acc: 0.2000\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 0s - loss: 11.5741 - acc: 0.2042 - val_loss: 11.6958 - val_acc: 0.2000\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 0s - loss: 11.6571 - acc: 0.2125 - val_loss: 11.9330 - val_acc: 0.1833\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 0s - loss: 12.3656 - acc: 0.1833 - val_loss: 12.6795 - val_acc: 0.1833\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 0s - loss: 12.2452 - acc: 0.1792 - val_loss: 13.2241 - val_acc: 0.1667\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 0s - loss: 11.5600 - acc: 0.2333 - val_loss: 12.6342 - val_acc: 0.1500\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 0s - loss: 11.8864 - acc: 0.2000 - val_loss: 10.4108 - val_acc: 0.1500\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 0s - loss: 11.0030 - acc: 0.2583 - val_loss: 10.8800 - val_acc: 0.1833\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 0s - loss: 11.6391 - acc: 0.2167 - val_loss: 10.4483 - val_acc: 0.2000\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 0s - loss: 10.2718 - acc: 0.2917 - val_loss: 10.2877 - val_acc: 0.1833\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 0s - loss: 10.9473 - acc: 0.2542 - val_loss: 10.0632 - val_acc: 0.1667\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 0s - loss: 10.8718 - acc: 0.2708 - val_loss: 9.7471 - val_acc: 0.2000\n",
      "Epoch 42/100\n",
      "240/240 [==============================] - 0s - loss: 10.1419 - acc: 0.3000 - val_loss: 9.7513 - val_acc: 0.1833\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 0s - loss: 10.4198 - acc: 0.2583 - val_loss: 9.9384 - val_acc: 0.2167\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 0s - loss: 10.0839 - acc: 0.3000 - val_loss: 10.2707 - val_acc: 0.2333\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 0s - loss: 9.9772 - acc: 0.2875 - val_loss: 11.1794 - val_acc: 0.1500\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 0s - loss: 9.8849 - acc: 0.2875 - val_loss: 11.0415 - val_acc: 0.1500\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 0s - loss: 9.6395 - acc: 0.2875 - val_loss: 9.7267 - val_acc: 0.1833\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 0s - loss: 8.7303 - acc: 0.3542 - val_loss: 10.0210 - val_acc: 0.2000\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 0s - loss: 9.2594 - acc: 0.3250 - val_loss: 11.1557 - val_acc: 0.1833\n",
      "Epoch 50/100\n",
      "240/240 [==============================] - 0s - loss: 9.2203 - acc: 0.2875 - val_loss: 9.3382 - val_acc: 0.2667\n",
      "Epoch 51/100\n",
      "240/240 [==============================] - 0s - loss: 8.5293 - acc: 0.3458 - val_loss: 8.3143 - val_acc: 0.2500\n",
      "Epoch 52/100\n",
      "240/240 [==============================] - 0s - loss: 8.8413 - acc: 0.3167 - val_loss: 7.7694 - val_acc: 0.2167\n",
      "Epoch 53/100\n",
      "240/240 [==============================] - 0s - loss: 8.5381 - acc: 0.3333 - val_loss: 7.3499 - val_acc: 0.2333\n",
      "Epoch 54/100\n",
      "240/240 [==============================] - 0s - loss: 7.4057 - acc: 0.3875 - val_loss: 6.6551 - val_acc: 0.2167\n",
      "Epoch 55/100\n",
      "240/240 [==============================] - 0s - loss: 7.3161 - acc: 0.3958 - val_loss: 6.0489 - val_acc: 0.2333\n",
      "Epoch 56/100\n",
      "240/240 [==============================] - 0s - loss: 7.1368 - acc: 0.3500 - val_loss: 5.6989 - val_acc: 0.2667\n",
      "Epoch 57/100\n",
      "240/240 [==============================] - 0s - loss: 6.2476 - acc: 0.4000 - val_loss: 5.5383 - val_acc: 0.2500\n",
      "Epoch 58/100\n",
      "240/240 [==============================] - 0s - loss: 5.6294 - acc: 0.4292 - val_loss: 5.0392 - val_acc: 0.2000\n",
      "Epoch 59/100\n",
      "240/240 [==============================] - 0s - loss: 4.9004 - acc: 0.4792 - val_loss: 4.7864 - val_acc: 0.2167\n",
      "Epoch 60/100\n",
      "240/240 [==============================] - 0s - loss: 5.0654 - acc: 0.4417 - val_loss: 4.5202 - val_acc: 0.2667\n",
      "Epoch 61/100\n",
      "240/240 [==============================] - 0s - loss: 4.8065 - acc: 0.4833 - val_loss: 4.0323 - val_acc: 0.2167\n",
      "Epoch 62/100\n",
      "240/240 [==============================] - 0s - loss: 3.6952 - acc: 0.5375 - val_loss: 3.6849 - val_acc: 0.2000\n",
      "Epoch 63/100\n",
      "240/240 [==============================] - 0s - loss: 3.2297 - acc: 0.5458 - val_loss: 3.6648 - val_acc: 0.1833\n",
      "Epoch 64/100\n",
      "240/240 [==============================] - 0s - loss: 3.3110 - acc: 0.5292 - val_loss: 3.4994 - val_acc: 0.1833\n",
      "Epoch 65/100\n",
      "240/240 [==============================] - 0s - loss: 2.6696 - acc: 0.5875 - val_loss: 3.2769 - val_acc: 0.2000\n",
      "Epoch 66/100\n",
      "240/240 [==============================] - 0s - loss: 2.7242 - acc: 0.5583 - val_loss: 3.1359 - val_acc: 0.2333\n",
      "Epoch 67/100\n",
      "240/240 [==============================] - 0s - loss: 2.3156 - acc: 0.5792 - val_loss: 2.8586 - val_acc: 0.2500\n",
      "Epoch 68/100\n",
      "240/240 [==============================] - 0s - loss: 1.9998 - acc: 0.5792 - val_loss: 2.6515 - val_acc: 0.2833\n",
      "Epoch 69/100\n",
      "240/240 [==============================] - 0s - loss: 1.5813 - acc: 0.6375 - val_loss: 2.5454 - val_acc: 0.3000\n",
      "Epoch 70/100\n",
      "240/240 [==============================] - 0s - loss: 1.5449 - acc: 0.6625 - val_loss: 2.4688 - val_acc: 0.3167\n",
      "Epoch 71/100\n",
      "240/240 [==============================] - 0s - loss: 1.4038 - acc: 0.6625 - val_loss: 2.4216 - val_acc: 0.3333\n",
      "Epoch 72/100\n",
      "240/240 [==============================] - 0s - loss: 1.2442 - acc: 0.6750 - val_loss: 2.3688 - val_acc: 0.3500\n",
      "Epoch 73/100\n",
      "240/240 [==============================] - 0s - loss: 0.8931 - acc: 0.7083 - val_loss: 2.3199 - val_acc: 0.3500\n",
      "Epoch 74/100\n",
      "240/240 [==============================] - 0s - loss: 0.9994 - acc: 0.6875 - val_loss: 2.3150 - val_acc: 0.3500\n",
      "Epoch 75/100\n",
      "240/240 [==============================] - 0s - loss: 0.7931 - acc: 0.7792 - val_loss: 2.3201 - val_acc: 0.3667\n",
      "Epoch 76/100\n",
      "240/240 [==============================] - 0s - loss: 1.1013 - acc: 0.7042 - val_loss: 2.3143 - val_acc: 0.3500\n",
      "Epoch 77/100\n",
      "240/240 [==============================] - 0s - loss: 0.7515 - acc: 0.7375 - val_loss: 2.3095 - val_acc: 0.3667\n",
      "Epoch 78/100\n",
      "240/240 [==============================] - 0s - loss: 0.7794 - acc: 0.8083 - val_loss: 2.3081 - val_acc: 0.3667\n",
      "Epoch 79/100\n",
      "240/240 [==============================] - 0s - loss: 0.6406 - acc: 0.8000 - val_loss: 2.2990 - val_acc: 0.3500\n",
      "Epoch 80/100\n",
      "240/240 [==============================] - 0s - loss: 0.5782 - acc: 0.8167 - val_loss: 2.2906 - val_acc: 0.3500\n",
      "Epoch 81/100\n",
      "240/240 [==============================] - 0s - loss: 0.6192 - acc: 0.8083 - val_loss: 2.2948 - val_acc: 0.3167\n",
      "Epoch 82/100\n",
      "240/240 [==============================] - 0s - loss: 0.4767 - acc: 0.8583 - val_loss: 2.3005 - val_acc: 0.3500\n",
      "Epoch 83/100\n",
      "240/240 [==============================] - 0s - loss: 0.5202 - acc: 0.8417 - val_loss: 2.3202 - val_acc: 0.3167\n",
      "Epoch 84/100\n",
      "240/240 [==============================] - 0s - loss: 0.5177 - acc: 0.8292 - val_loss: 2.3436 - val_acc: 0.3000\n",
      "Epoch 85/100\n",
      "240/240 [==============================] - 0s - loss: 0.3719 - acc: 0.8875 - val_loss: 2.3619 - val_acc: 0.3167\n",
      "Epoch 86/100\n",
      "240/240 [==============================] - 0s - loss: 0.3652 - acc: 0.8792 - val_loss: 2.3715 - val_acc: 0.3333\n",
      "Epoch 87/100\n",
      "240/240 [==============================] - 0s - loss: 0.3251 - acc: 0.8958 - val_loss: 2.3881 - val_acc: 0.3333\n",
      "Epoch 88/100\n",
      "240/240 [==============================] - 0s - loss: 0.3631 - acc: 0.8625 - val_loss: 2.3999 - val_acc: 0.3833\n",
      "Epoch 89/100\n",
      "240/240 [==============================] - 0s - loss: 0.3473 - acc: 0.8708 - val_loss: 2.4111 - val_acc: 0.3833\n",
      "Epoch 90/100\n",
      "240/240 [==============================] - 0s - loss: 0.3708 - acc: 0.8708 - val_loss: 2.4294 - val_acc: 0.3333\n",
      "Epoch 91/100\n",
      "240/240 [==============================] - 0s - loss: 0.3701 - acc: 0.8708 - val_loss: 2.4398 - val_acc: 0.3000\n",
      "Epoch 92/100\n",
      "240/240 [==============================] - 0s - loss: 0.2602 - acc: 0.9083 - val_loss: 2.4338 - val_acc: 0.2833\n",
      "Epoch 93/100\n",
      "240/240 [==============================] - 0s - loss: 0.2491 - acc: 0.8958 - val_loss: 2.4188 - val_acc: 0.3667\n",
      "Epoch 94/100\n",
      "240/240 [==============================] - 0s - loss: 0.3462 - acc: 0.9083 - val_loss: 2.4111 - val_acc: 0.3833\n",
      "Epoch 95/100\n",
      "240/240 [==============================] - 0s - loss: 0.2006 - acc: 0.9375 - val_loss: 2.3988 - val_acc: 0.3833\n",
      "Epoch 96/100\n",
      "240/240 [==============================] - 0s - loss: 0.2515 - acc: 0.9042 - val_loss: 2.3900 - val_acc: 0.3667\n",
      "Epoch 97/100\n",
      "240/240 [==============================] - 0s - loss: 0.2246 - acc: 0.9333 - val_loss: 2.3940 - val_acc: 0.3667\n",
      "Epoch 98/100\n",
      "240/240 [==============================] - 0s - loss: 0.1624 - acc: 0.9458 - val_loss: 2.4119 - val_acc: 0.3667\n",
      "Epoch 99/100\n",
      "240/240 [==============================] - 0s - loss: 0.1707 - acc: 0.9333 - val_loss: 2.4460 - val_acc: 0.3500\n",
      "Epoch 100/100\n",
      "240/240 [==============================] - 0s - loss: 0.1978 - acc: 0.9458 - val_loss: 2.4600 - val_acc: 0.3500\n"
     ]
    }
   ],
   "source": [
    "cnn2_results = cnn2.fit(train_X_mfccs_2D, train_y, batch_size=64, \n",
    "                        epochs=100, validation_data=(cv_X_mfccs_2D, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CNN 2 results\n",
      "Train acc: 0.9458\n",
      "CV acc: 0.3833\n"
     ]
    }
   ],
   "source": [
    "# show best results\n",
    "best_training_accuracy = max(cnn2_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(cnn2_results.history[\"val_acc\"])\n",
    "print(\"Best CNN 2 results\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our **best convolutional model's accuracy above 0.35**. The model's performance is very brittle though, highly dependent on random weights initialization. It is also already overfitting. Let's calculate the F1 score for our latest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "cnn2_pred_cv_y_mfccs_2D = cnn2.predict(cv_X_mfccs_2D, batch_size=32)\n",
    "cnn2_pred_cv_y_mfccs_2D = utils.one_hot_encode(cnn2_pred_cv_y_mfccs_2D)\n",
    "cnn2_pred_cv_y_mfccs_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN 2 accuracy via sklearn (CV): 0.3500\n",
      "CNN 2 f1 score (CV): 0.3767\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "cnn2_cv_accuracy = accuracy_score(cv_y, cnn2_pred_cv_y_mfccs_2D)\n",
    "cnn2_cv_f1_score = f1_score(cv_y, cnn2_pred_cv_y_mfccs_2D, average=\"weighted\")\n",
    "print(\"CNN 2 accuracy via sklearn (CV): {:.4f}\".format(cnn2_cv_accuracy))\n",
    "print(\"CNN 2 f1 score (CV): {:.4f}\".format(cnn2_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional models (2D) with Tempogram data\n",
    "We have 2 other forms of preprocessed data with dimensions that lend themselves to 2D convolutions - MEL Spectrogram and Tempogram. Initial experiments with the MEL Spectrogram data didn't yield promising initial results, let's try Tempogram instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempogram shape:  (240, 384, 32)\n"
     ]
    }
   ],
   "source": [
    "# Our data is already 2D, we don't need to expand dimensions\n",
    "print(\"Tempogram shape: \", train_X_tempogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3 = Sequential([\n",
    "        Convolution1D(input_shape=(train_X_tempogram.shape[1], train_X_tempogram.shape[2]), \n",
    "                      kernel_size=32, filters=128, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.11),\n",
    "        MaxPooling1D(),\n",
    "        Convolution1D(kernel_size=12, filters=128, padding=\"same\", activation=\"relu\"),\n",
    "        Dropout(0.13),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(2000, activation=\"relu\"),\n",
    "        Dropout(.7),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "cnn3.compile(Adam(lr=0.0001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/50\n",
      "240/240 [==============================] - 4s - loss: 2.4760 - acc: 0.0917 - val_loss: 2.4169 - val_acc: 0.1167\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - 2s - loss: 2.4466 - acc: 0.1042 - val_loss: 2.4006 - val_acc: 0.1500\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - 2s - loss: 2.4076 - acc: 0.1292 - val_loss: 2.4018 - val_acc: 0.1667\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - 2s - loss: 2.4003 - acc: 0.1625 - val_loss: 2.3901 - val_acc: 0.1833\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - 2s - loss: 2.3821 - acc: 0.1417 - val_loss: 2.3701 - val_acc: 0.1833\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - 2s - loss: 2.3500 - acc: 0.1500 - val_loss: 2.3524 - val_acc: 0.2000\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - 2s - loss: 2.3744 - acc: 0.1792 - val_loss: 2.3573 - val_acc: 0.2333\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - 2s - loss: 2.3599 - acc: 0.1708 - val_loss: 2.3358 - val_acc: 0.2000\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - 2s - loss: 2.3464 - acc: 0.1667 - val_loss: 2.3278 - val_acc: 0.1833\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - 2s - loss: 2.3226 - acc: 0.1708 - val_loss: 2.3227 - val_acc: 0.1833\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - 2s - loss: 2.3286 - acc: 0.1750 - val_loss: 2.3141 - val_acc: 0.1833\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - 2s - loss: 2.2943 - acc: 0.1875 - val_loss: 2.2992 - val_acc: 0.2167\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - 2s - loss: 2.2680 - acc: 0.2375 - val_loss: 2.2972 - val_acc: 0.2167\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - 2s - loss: 2.3057 - acc: 0.1875 - val_loss: 2.2927 - val_acc: 0.2000\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - 2s - loss: 2.2658 - acc: 0.1833 - val_loss: 2.2678 - val_acc: 0.2333\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - 2s - loss: 2.2390 - acc: 0.2042 - val_loss: 2.2697 - val_acc: 0.2000\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - 2s - loss: 2.2391 - acc: 0.2417 - val_loss: 2.2593 - val_acc: 0.2333\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - 2s - loss: 2.2434 - acc: 0.2333 - val_loss: 2.2491 - val_acc: 0.2500\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - 2s - loss: 2.2233 - acc: 0.2292 - val_loss: 2.2484 - val_acc: 0.2500\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - 2s - loss: 2.2043 - acc: 0.2333 - val_loss: 2.2348 - val_acc: 0.2500\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - 2s - loss: 2.2209 - acc: 0.2250 - val_loss: 2.2161 - val_acc: 0.2833\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - 2s - loss: 2.2093 - acc: 0.2333 - val_loss: 2.2156 - val_acc: 0.2500\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - 2s - loss: 2.2051 - acc: 0.2292 - val_loss: 2.2160 - val_acc: 0.2500\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - 2s - loss: 2.1959 - acc: 0.2208 - val_loss: 2.1940 - val_acc: 0.2333\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - 2s - loss: 2.1527 - acc: 0.2792 - val_loss: 2.1822 - val_acc: 0.2333\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - 2s - loss: 2.1617 - acc: 0.2625 - val_loss: 2.1726 - val_acc: 0.2833\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - 2s - loss: 2.1363 - acc: 0.2542 - val_loss: 2.1591 - val_acc: 0.2833\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - 2s - loss: 2.1425 - acc: 0.2667 - val_loss: 2.1491 - val_acc: 0.2667\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - 2s - loss: 2.1138 - acc: 0.2875 - val_loss: 2.1776 - val_acc: 0.2667\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - 2s - loss: 2.1096 - acc: 0.3083 - val_loss: 2.1530 - val_acc: 0.2667\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - 2s - loss: 2.1045 - acc: 0.2792 - val_loss: 2.1347 - val_acc: 0.2833\n",
      "Epoch 32/50\n",
      "240/240 [==============================] - 2s - loss: 2.1094 - acc: 0.2958 - val_loss: 2.1373 - val_acc: 0.3000\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - 2s - loss: 2.0524 - acc: 0.3000 - val_loss: 2.1213 - val_acc: 0.2667\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - 2s - loss: 2.0636 - acc: 0.2917 - val_loss: 2.0976 - val_acc: 0.3167\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - 2s - loss: 2.0188 - acc: 0.3125 - val_loss: 2.0915 - val_acc: 0.3000\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - 2s - loss: 2.0609 - acc: 0.3000 - val_loss: 2.1023 - val_acc: 0.3167\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - 2s - loss: 2.0167 - acc: 0.3208 - val_loss: 2.0878 - val_acc: 0.3000\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - 2s - loss: 1.9785 - acc: 0.3708 - val_loss: 2.0711 - val_acc: 0.3333\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - 2s - loss: 1.9760 - acc: 0.3250 - val_loss: 2.0630 - val_acc: 0.2833\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - 2s - loss: 1.9442 - acc: 0.3500 - val_loss: 2.0455 - val_acc: 0.3333\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - 2s - loss: 1.9865 - acc: 0.3000 - val_loss: 2.0397 - val_acc: 0.3500\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - 2s - loss: 1.9434 - acc: 0.3250 - val_loss: 2.0364 - val_acc: 0.3167\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - 3s - loss: 1.9754 - acc: 0.3083 - val_loss: 2.0151 - val_acc: 0.3167\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - 3s - loss: 1.9031 - acc: 0.3833 - val_loss: 2.0037 - val_acc: 0.3333\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - 2s - loss: 1.9431 - acc: 0.3250 - val_loss: 2.0157 - val_acc: 0.3000\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - 3s - loss: 1.9001 - acc: 0.3667 - val_loss: 2.0070 - val_acc: 0.3167\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - 2s - loss: 1.8910 - acc: 0.3333 - val_loss: 1.9949 - val_acc: 0.3167\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - 2s - loss: 1.8815 - acc: 0.3500 - val_loss: 2.0005 - val_acc: 0.3167\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - 2s - loss: 1.9089 - acc: 0.3250 - val_loss: 1.9903 - val_acc: 0.3000\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - 2s - loss: 1.8556 - acc: 0.3542 - val_loss: 1.9861 - val_acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "cnn3_results = cnn3.fit(train_X_tempogram, train_y, batch_size=64, \n",
    "                        epochs=50, validation_data=(cv_X_tempogram, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CNN 3 results\n",
      "Train acc: 0.3833\n",
      "CV acc: 0.3500\n"
     ]
    }
   ],
   "source": [
    "# show best results\n",
    "best_training_accuracy = max(cnn3_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(cnn3_results.history[\"val_acc\"])\n",
    "print(\"Best CNN 3 results\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our **best convolutional model with the tempogram data reaches an accuracy around 0.35**. This model's performance is far less brittle than the 2D MFCCs. After 50 epochs we aren't particularly overfitting. Let's check the F1 score and continue training for a couple more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 12)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "cnn3_pred_cv_y_tempogram = cnn3.predict(cv_X_tempogram, batch_size=32)\n",
    "cnn3_pred_cv_y_tempogram = utils.one_hot_encode(cnn3_pred_cv_y_tempogram)\n",
    "cnn3_pred_cv_y_tempogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN 3 (tempogram) accuracy via sklearn (CV): 0.3333\n",
      "CNN 3 (tempogram) f1 score (CV): 0.2913\n"
     ]
    }
   ],
   "source": [
    "# we can also use sklearn directly to get accuracy\n",
    "cnn3_cv_accuracy = accuracy_score(cv_y, cnn3_pred_cv_y_tempogram)\n",
    "cnn3_cv_f1_score = f1_score(cv_y, cnn3_pred_cv_y_tempogram, average=\"weighted\")\n",
    "print(\"CNN 3 (tempogram) accuracy via sklearn (CV): {:.4f}\".format(cnn3_cv_accuracy))\n",
    "print(\"CNN 3 (tempogram) f1 score (CV): {:.4f}\".format(cnn3_cv_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/50\n",
      "240/240 [==============================] - 2s - loss: 1.8663 - acc: 0.3333 - val_loss: 1.9622 - val_acc: 0.3500\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - 2s - loss: 1.8919 - acc: 0.3167 - val_loss: 1.9641 - val_acc: 0.3500\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - 2s - loss: 1.8349 - acc: 0.3667 - val_loss: 1.9515 - val_acc: 0.3667\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - 2s - loss: 1.8632 - acc: 0.3333 - val_loss: 1.9513 - val_acc: 0.3667\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - 2s - loss: 1.7914 - acc: 0.4000 - val_loss: 1.9441 - val_acc: 0.3667\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - 2s - loss: 1.8184 - acc: 0.3792 - val_loss: 1.9343 - val_acc: 0.3667\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - 2s - loss: 1.8228 - acc: 0.3792 - val_loss: 1.9309 - val_acc: 0.3500\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - 2s - loss: 1.8108 - acc: 0.3458 - val_loss: 1.9374 - val_acc: 0.3833\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - 2s - loss: 1.7938 - acc: 0.3833 - val_loss: 1.9289 - val_acc: 0.3667\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - 2s - loss: 1.8062 - acc: 0.3833 - val_loss: 1.9286 - val_acc: 0.3833\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - 2s - loss: 1.7849 - acc: 0.3375 - val_loss: 1.9200 - val_acc: 0.3667\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - 2s - loss: 1.7493 - acc: 0.3875 - val_loss: 1.9068 - val_acc: 0.3500\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - 2s - loss: 1.7274 - acc: 0.4375 - val_loss: 1.9044 - val_acc: 0.3500\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - 2s - loss: 1.7345 - acc: 0.3958 - val_loss: 1.9137 - val_acc: 0.3667\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - 2s - loss: 1.7547 - acc: 0.3917 - val_loss: 1.9177 - val_acc: 0.3667\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - 2s - loss: 1.7387 - acc: 0.4125 - val_loss: 1.9047 - val_acc: 0.4000\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - 2s - loss: 1.7286 - acc: 0.3750 - val_loss: 1.9001 - val_acc: 0.4000\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - 2s - loss: 1.7485 - acc: 0.4000 - val_loss: 1.9245 - val_acc: 0.3667\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - 2s - loss: 1.7070 - acc: 0.4083 - val_loss: 1.9221 - val_acc: 0.3833\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - 2s - loss: 1.7264 - acc: 0.3875 - val_loss: 1.9156 - val_acc: 0.3500\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - 2s - loss: 1.7418 - acc: 0.3917 - val_loss: 1.9227 - val_acc: 0.3667\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - 2s - loss: 1.6804 - acc: 0.4250 - val_loss: 1.8997 - val_acc: 0.3333\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - 2s - loss: 1.6447 - acc: 0.4417 - val_loss: 1.9037 - val_acc: 0.3667\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - 2s - loss: 1.6794 - acc: 0.4208 - val_loss: 1.8957 - val_acc: 0.3833\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - 2s - loss: 1.6816 - acc: 0.4042 - val_loss: 1.8880 - val_acc: 0.3500\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - 2s - loss: 1.6686 - acc: 0.4042 - val_loss: 1.9014 - val_acc: 0.3333\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - 2s - loss: 1.6584 - acc: 0.4125 - val_loss: 1.8999 - val_acc: 0.3333\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - 2s - loss: 1.7168 - acc: 0.4125 - val_loss: 1.8923 - val_acc: 0.3333\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - 2s - loss: 1.6537 - acc: 0.4625 - val_loss: 1.8996 - val_acc: 0.3500\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - 2s - loss: 1.6205 - acc: 0.4625 - val_loss: 1.8999 - val_acc: 0.3500\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - 2s - loss: 1.5866 - acc: 0.4792 - val_loss: 1.8975 - val_acc: 0.3833\n",
      "Epoch 32/50\n",
      "240/240 [==============================] - 2s - loss: 1.6386 - acc: 0.4292 - val_loss: 1.8875 - val_acc: 0.3667\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - 2s - loss: 1.5948 - acc: 0.4333 - val_loss: 1.9121 - val_acc: 0.3667\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - 2s - loss: 1.6026 - acc: 0.4333 - val_loss: 1.8950 - val_acc: 0.3500\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - 2s - loss: 1.6066 - acc: 0.4333 - val_loss: 1.8973 - val_acc: 0.3667\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - 2s - loss: 1.6021 - acc: 0.4542 - val_loss: 1.9071 - val_acc: 0.3500\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - 2s - loss: 1.5608 - acc: 0.4708 - val_loss: 1.9079 - val_acc: 0.3333\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - 2s - loss: 1.5606 - acc: 0.4958 - val_loss: 1.9087 - val_acc: 0.3667\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - 2s - loss: 1.5695 - acc: 0.4417 - val_loss: 1.9151 - val_acc: 0.3667\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - 2s - loss: 1.5732 - acc: 0.4583 - val_loss: 1.9063 - val_acc: 0.3333\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - 2s - loss: 1.5532 - acc: 0.4667 - val_loss: 1.9142 - val_acc: 0.3333\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - 2s - loss: 1.5403 - acc: 0.4292 - val_loss: 1.9064 - val_acc: 0.3333\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - 2s - loss: 1.5204 - acc: 0.4958 - val_loss: 1.9090 - val_acc: 0.3500\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - 2s - loss: 1.5224 - acc: 0.4917 - val_loss: 1.9179 - val_acc: 0.3667\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - 2s - loss: 1.5724 - acc: 0.4833 - val_loss: 1.9155 - val_acc: 0.3333\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - 2s - loss: 1.5794 - acc: 0.4250 - val_loss: 1.9097 - val_acc: 0.3333\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - 2s - loss: 1.5236 - acc: 0.4625 - val_loss: 1.9284 - val_acc: 0.3333\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - 2s - loss: 1.5187 - acc: 0.4958 - val_loss: 1.9164 - val_acc: 0.3667\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - 2s - loss: 1.4836 - acc: 0.5000 - val_loss: 1.9084 - val_acc: 0.3500\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - 2s - loss: 1.5036 - acc: 0.5083 - val_loss: 1.9205 - val_acc: 0.3833\n"
     ]
    }
   ],
   "source": [
    "cnn3_results = cnn3.fit(train_X_tempogram, train_y, batch_size=64, \n",
    "                        epochs=50, validation_data=(cv_X_tempogram, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CNN 3 results\n",
      "Train acc: 0.5083\n",
      "CV acc: 0.4000\n"
     ]
    }
   ],
   "source": [
    "# show best results after another 50 epochs\n",
    "best_training_accuracy = max(cnn3_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(cnn3_results.history[\"val_acc\"])\n",
    "print(\"Best CNN 3 results\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CNN 3 (tempogram) accuracy via sklearn (CV): 0.3833\n",
      "Final CNN 3 (tempogram) f1 score (CV): 0.3357\n"
     ]
    }
   ],
   "source": [
    "# predict and one-hot encode\n",
    "cnn3_pred_cv_y_tempogram = cnn3.predict(cv_X_tempogram, batch_size=32)\n",
    "cnn3_pred_cv_y_tempogram = utils.one_hot_encode(cnn3_pred_cv_y_tempogram)\n",
    "\n",
    "# we can also use sklearn directly to get accuracy\n",
    "cnn3_cv_accuracy = accuracy_score(cv_y, cnn3_pred_cv_y_tempogram)\n",
    "cnn3_cv_f1_score = f1_score(cv_y, cnn3_pred_cv_y_tempogram, average=\"weighted\")\n",
    "print(\"Final CNN 3 (tempogram) accuracy via sklearn (CV): {:.4f}\".format(cnn3_cv_accuracy))\n",
    "print(\"Final CNN 3 (tempogram) f1 score (CV): {:.4f}\".format(cnn3_cv_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached the **best CV accuracy so far - 0.4**, but we're beginning to overfit. Given that we're working on a relatively small sample, this could be a viable starting point for training models on the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Models\n",
    "We can also try to take advantage of the architectures specifically designed for time sequences: RNNs. We will start with the basic keras implementations of simple RNN. After that we can consider moving on to GRUs & LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_1 = Sequential([\n",
    "        SimpleRNN(input_shape=(conv_train_X_mfccs_1D.shape[1], 1), units=1000, activation='relu'),\n",
    "        Dropout(.4),\n",
    "        Dense(2000, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.8),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "rnn_1.compile(Adam(lr=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/50\n",
      "240/240 [==============================] - 8s - loss: 3.7420 - acc: 0.0917 - val_loss: 2.4786 - val_acc: 0.0833\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - 6s - loss: 3.2680 - acc: 0.1333 - val_loss: 2.4763 - val_acc: 0.0833\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - 6s - loss: 3.1245 - acc: 0.1083 - val_loss: 2.4735 - val_acc: 0.1000\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - 6s - loss: 3.0108 - acc: 0.1417 - val_loss: 2.4777 - val_acc: 0.0833\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - 6s - loss: 2.8757 - acc: 0.1792 - val_loss: 2.4826 - val_acc: 0.0833\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - 6s - loss: 2.8516 - acc: 0.1333 - val_loss: 2.4814 - val_acc: 0.1000\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - 6s - loss: 2.6712 - acc: 0.1500 - val_loss: 2.4785 - val_acc: 0.0667\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - 7s - loss: 2.7017 - acc: 0.1417 - val_loss: 2.4772 - val_acc: 0.1000\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - 7s - loss: 2.7979 - acc: 0.1292 - val_loss: 2.4802 - val_acc: 0.0500\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - 8s - loss: 2.6468 - acc: 0.1667 - val_loss: 2.4824 - val_acc: 0.0667\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - 7s - loss: 2.5629 - acc: 0.2167 - val_loss: 2.4779 - val_acc: 0.0667\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - 7s - loss: 2.4955 - acc: 0.1708 - val_loss: 2.4744 - val_acc: 0.1167\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - 8s - loss: 2.5015 - acc: 0.2125 - val_loss: 2.4713 - val_acc: 0.1000\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - 8s - loss: 2.4450 - acc: 0.1750 - val_loss: 2.4696 - val_acc: 0.0500\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - 7s - loss: 2.3047 - acc: 0.2542 - val_loss: 2.4758 - val_acc: 0.0833\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - 8s - loss: 2.3290 - acc: 0.2417 - val_loss: 2.4769 - val_acc: 0.1000\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - 8s - loss: 2.2927 - acc: 0.2292 - val_loss: 2.4729 - val_acc: 0.1167\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - 8s - loss: 2.3143 - acc: 0.2667 - val_loss: 2.4669 - val_acc: 0.0833\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - 8s - loss: 2.1273 - acc: 0.2958 - val_loss: 2.4643 - val_acc: 0.1167\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - 8s - loss: 2.0852 - acc: 0.3083 - val_loss: 2.4597 - val_acc: 0.1667\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - 8s - loss: 2.1143 - acc: 0.3000 - val_loss: 2.4625 - val_acc: 0.1333\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - 8s - loss: 1.9646 - acc: 0.3583 - val_loss: 2.4644 - val_acc: 0.1000\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - 8s - loss: 2.0197 - acc: 0.3375 - val_loss: 2.4587 - val_acc: 0.1167\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - 8s - loss: 1.8677 - acc: 0.3792 - val_loss: 2.4584 - val_acc: 0.1000\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - 8s - loss: 1.8667 - acc: 0.3667 - val_loss: 2.4677 - val_acc: 0.0500\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - 8s - loss: 1.7144 - acc: 0.4083 - val_loss: 2.4628 - val_acc: 0.0667\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - 8s - loss: 1.7474 - acc: 0.4000 - val_loss: 2.4594 - val_acc: 0.1000\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - 7s - loss: 1.5764 - acc: 0.4917 - val_loss: 2.4463 - val_acc: 0.0667\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - 7s - loss: 1.4552 - acc: 0.5167 - val_loss: 2.4443 - val_acc: 0.0833\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - 7s - loss: 1.4246 - acc: 0.5417 - val_loss: 2.4386 - val_acc: 0.1000\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - 7s - loss: 1.3248 - acc: 0.5708 - val_loss: 2.4586 - val_acc: 0.1000\n",
      "Epoch 32/50\n",
      "240/240 [==============================] - 7s - loss: 1.2092 - acc: 0.6333 - val_loss: 2.4410 - val_acc: 0.1000\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - 7s - loss: 1.0812 - acc: 0.6333 - val_loss: 2.4526 - val_acc: 0.1167\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - 7s - loss: 1.0745 - acc: 0.6292 - val_loss: 2.4697 - val_acc: 0.0833\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - 7s - loss: 0.9265 - acc: 0.7083 - val_loss: 2.4871 - val_acc: 0.0667\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - 7s - loss: 0.8562 - acc: 0.7292 - val_loss: 2.5070 - val_acc: 0.0833\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - 7s - loss: 0.7941 - acc: 0.7625 - val_loss: 2.4631 - val_acc: 0.0667\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - 8s - loss: 0.7608 - acc: 0.7708 - val_loss: 2.4724 - val_acc: 0.1167\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - 7s - loss: 0.7329 - acc: 0.7583 - val_loss: 2.4883 - val_acc: 0.0833\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - 7s - loss: 0.6417 - acc: 0.7833 - val_loss: 2.5473 - val_acc: 0.0167\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - 7s - loss: 0.6183 - acc: 0.8167 - val_loss: 2.5189 - val_acc: 0.0833\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - 7s - loss: 0.5644 - acc: 0.8167 - val_loss: 2.5496 - val_acc: 0.1000\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - 7s - loss: 0.5527 - acc: 0.8250 - val_loss: 2.5372 - val_acc: 0.1500\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - 8s - loss: 0.4331 - acc: 0.8583 - val_loss: 2.5462 - val_acc: 0.0667\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - 7s - loss: 0.3994 - acc: 0.8708 - val_loss: 2.6039 - val_acc: 0.1167\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - 8s - loss: 0.3492 - acc: 0.8875 - val_loss: 2.6308 - val_acc: 0.0667\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - 7s - loss: 0.3296 - acc: 0.8750 - val_loss: 2.6817 - val_acc: 0.0500\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - 8s - loss: 0.2717 - acc: 0.9250 - val_loss: 2.7143 - val_acc: 0.1333\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - 8s - loss: 0.1985 - acc: 0.9458 - val_loss: 2.7824 - val_acc: 0.0667\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - 7s - loss: 0.2515 - acc: 0.9375 - val_loss: 2.8268 - val_acc: 0.0500\n"
     ]
    }
   ],
   "source": [
    "rnn_1_results = rnn_1.fit(conv_train_X_mfccs_1D, train_y, batch_size=32, epochs=50, validation_data=(conv_cv_X_mfccs_1D, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RNN 1 results\n",
      "Train acc: 0.9458\n",
      "CV acc: 0.1667\n"
     ]
    }
   ],
   "source": [
    "# show best results after another 50 epochs\n",
    "best_training_accuracy = max(rnn_1_results.history[\"acc\"])\n",
    "best_validation_accuracy = max(rnn_1_results.history[\"val_acc\"])\n",
    "print(\"Best RNN 1 results\\nTrain acc: {:.4f}\\nCV acc: {:.4f}\".format(best_training_accuracy, best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after some experiments to reduce overfitting our SimpleRNN doesn't seem able to get a good CV accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU\n",
    "Let's try the Gated Recurrent Unit network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2 = Sequential([\n",
    "        GRU(input_shape=(conv_train_X_mfccs_1D.shape[1], 1), units=1000, activation='tanh'),\n",
    "        Dense(1000, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(.7),\n",
    "        Dense(num_categories, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "rnn_2.compile(Adam(lr=0.003),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "240/240 [==============================] - 26s - loss: 4.2266 - acc: 0.0708 - val_loss: 2.7199 - val_acc: 0.0833\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 21s - loss: 4.3152 - acc: 0.0917 - val_loss: 6.1280 - val_acc: 0.1167\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 19s - loss: 3.6099 - acc: 0.0792 - val_loss: 6.5881 - val_acc: 0.0833\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 21s - loss: 3.8228 - acc: 0.0792 - val_loss: 12.8300 - val_acc: 0.0833\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 20s - loss: 2.9018 - acc: 0.0917 - val_loss: 12.3476 - val_acc: 0.0833\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 20s - loss: 2.5534 - acc: 0.1667 - val_loss: 11.3755 - val_acc: 0.0833\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 20s - loss: 3.9685 - acc: 0.1042 - val_loss: 8.7863 - val_acc: 0.1167\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 20s - loss: 3.3102 - acc: 0.1208 - val_loss: 8.2222 - val_acc: 0.1167\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 20s - loss: 3.6101 - acc: 0.0958 - val_loss: 6.0353 - val_acc: 0.1167\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 19s - loss: 3.3395 - acc: 0.1083 - val_loss: 5.5199 - val_acc: 0.1167\n"
     ]
    }
   ],
   "source": [
    "rnn_2_results = rnn_2.fit(conv_train_X_mfccs_1D, train_y, batch_size=32, epochs=10, validation_data=(conv_cv_X_mfccs_1D, cv_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
