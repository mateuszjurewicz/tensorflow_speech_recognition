{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate data for training, validation and sample\n",
    "\n",
    "Before we get to the preprocessing we should split our available data into a *training, cross-validation and testing sets*. It is also really helpful to create a small subset of the data (called **sample**) which we'll use for early experimentations and making sure the code works. \n",
    "\n",
    "The idea is to work on a small sample dataset (itself separated into train, cv and test) so that we get feedback quickly - and the same code can then be run for more epochs on the larger dataset.\n",
    "\n",
    "This is a good exercise in basic python **file manipulation** and can also be done directly in the terminal (deeply recommend *tmux*), but I prefer to have a notebook for this because I can then retrace my steps and easily repeat the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Plan\n",
    "What do we plan to achieve with this notebook and what steps need to be taken?\n",
    "\n",
    "   -  **Split into main/test and main/cv**\n",
    "   <br><font color=gray> use the provided testing_list.txt and validation_list.txt lists to split the original train set into main/test, main/cv (by moving files). This has the benefit of putting files recorded by the same person in only one subset, so the model can't latch onto a person's voice characteristics. We'll make sure there's not data leakage this way.</font><br><br>\n",
    "   -  **Prepare main/train**\n",
    "   <br><font color=gray>treat the remaining files as main/train.</font><br><br>\n",
    "   -  **Prepare main/-subset-/unknown**\n",
    "   <br><font color=gray>use the categories we won't be predicting as *unknown*, but maintain the files' uniqueness by appending their original category name to the end of the filename and then put them all into main/*subset*/unknown/. </font><br><br>\n",
    "   -  **Split background noise files**\n",
    "   <br><font color=gray>all the background noise files (which we'll treat as *silence* category members) need to be cut into approximately 1 second long files (the other files are also slightly irregular in this way).</font><br><br>\n",
    "   -  **Prepare main/-subset-/silence**\n",
    "   <br><font color=gray>all the 1-sec silence files need to be split into train, test and cv subsets (60 x 20 x 20). </font><br><br>\n",
    "   -  **Copy subsets into sample/-subset-/category**\n",
    "   <br><font color=gray>create sample folder (as opposed to /main) & copy small, random subset of each category into sample/train, sample/cv and sample/test.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "We'll need a couple of additional libraries so let's import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from shutil import copyfile\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into main/test and main/cv\n",
    "What Google Brain & Kaggle give us is a folder called \"train\" with 3 important element - the testing_list.txt, validation_list.txt and a subfolder \"audio\" with further subfolders whose names are referring to what the .wav files within them represent - e.g. \"yes\", \"no\", \"happy\" & \"dog\". \n",
    "\n",
    "Google's idea is for us to use the .txt files to grab the right .wav files and move them from the primary train folder into validation and test folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/tensorflow_speech_recognition\r\n"
     ]
    }
   ],
   "source": [
    "# make sure we're in the right folder (the one with Google's test and train folder in it), and if not cd into it.\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're aiming for the following directory tree:\n",
    "    -  data\\\n",
    "        - main\\\n",
    "            - train\\\n",
    "                - all subfolders named after categories (e.g. \"yes\", \"one\", \"cat\", \"silence\", \"unknown\")\n",
    "            - test\\\n",
    "                - (...)\n",
    "            - cv\\\n",
    "                - (...)\n",
    "        - sample\\\n",
    "            - train\\\n",
    "            - test\\\n",
    "            - cv\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our own data folder with main\\train, main\\test and main\\cv subfolders in it\n",
    "!mkdir -p data//main//train\n",
    "!mkdir -p data//main//test\n",
    "!mkdir -p data//main//cv\n",
    "\n",
    "# depending on the operating system the slashes might be backward of forward\n",
    "# e.g. for windows:\n",
    "# !mkdir data\\\\main\\\\train\n",
    "# !mkdir data\\\\main\\\\test\n",
    "# !mkdir data\\\\main\\\\cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store relative paths (we need to escape the slash)\n",
    "path_main_train = \"data/main/train\"\n",
    "path_main_test = \"data/main/test\"\n",
    "path_main_cv = \"data/main/cv\"\n",
    "\n",
    "paths_main = [path_main_train, path_main_test, path_main_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store relative paths to provided data\n",
    "path_provided = \"train/audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes', 'right', 'silence', 'three', 'eight', 'dog', 'tree']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the categories (which are also names of subfolders, all lowercase)\n",
    "categories_to_predict = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\",\n",
    "             \"silence\", \"unknown\"]\n",
    "\n",
    "categories_unknown = [\"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \n",
    "                      \"Six\", \"Seven\", \"Eight\", \"Nine\", \"Bed\", \"Bird\",\n",
    "                      \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\",\n",
    "                      \"Tree\", \"Wow\"]\n",
    "\n",
    "categories_all = categories_to_predict + categories_unknown\n",
    "\n",
    "# we need to make them all lowercase because that's how they're written in testing_list.txt and validation_list.txt\n",
    "categories_all = [category.lower() for category in categories_all]\n",
    "\n",
    "categories_all[::5]  # every 5th element will be shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all subfolders in main\n",
    "for path in paths_main:\n",
    "    for subfolder_name in categories_all:\n",
    "        subfolder_path = os.path.join(path, subfolder_name)\n",
    "        !mkdir $subfolder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Separation for training, validation and sample.ipynb  README.md\r\n",
      "1. Visualization and data investigation.ipynb\t\t req.txt\r\n",
      "2. Preprocessing and data augmentation.ipynb\t\t test\r\n",
      "3. Model experiments - sample set.ipynb\t\t\t train\r\n",
      "data\t\t\t\t\t\t\t train_backup\r\n",
      "LICENSE\t\t\t\t\t\t\t utils.py\r\n",
      "__pycache__\r\n"
     ]
    }
   ],
   "source": [
    "# we should now see our newly made data folder and the original train folder provided by Google \n",
    "# I've also made a backup of the train folder, just in case.\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed/0c40e715_nohash_0.wav\\n', 'bed/0ea0e2f4_nohash_0.wav\\n', 'bed/0ea0e2f4_nohash_1.wav\\n', 'bed/105a0eea_nohash_0.wav\\n', 'bed/1528225c_nohash_0.wav\\n']\n"
     ]
    }
   ],
   "source": [
    "# take a look at the format\n",
    "with open(\"train/testing_list.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "    # show first 5 entries\n",
    "    print(content[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the .txt files provided by Google to move files from original \\train\\ directory\n",
    "# this won't be a problem for linux/mac users but on windows we'll have to fix the slash\n",
    "def move_per_list(text_list, path_origin, path_destination):\n",
    "    \"\"\"\n",
    "    Take a list of paths to .wav files and move them from origin path to destination\n",
    "    \"\"\"\n",
    "    with open(text_list, \"r\", encoding=\"UTF-8\") as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            # strip newline (\"\\n\")\n",
    "            line = line.strip()\n",
    "            \n",
    "            # for windows: replace \"/\" with \"\\\\\"\n",
    "            # line = line.replace(\"/\", \"\\\\\")\n",
    "\n",
    "            # construct current path to .wav file\n",
    "            cur_path = os.path.join(path_origin, line)\n",
    "\n",
    "            # move the listed .wav files\n",
    "            os.rename(cur_path, os.path.join(path_destination, line))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\\test\\...\n",
    "move_per_list(\"train//testing_list.txt\", path_provided, path_main_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\\cv\\...\n",
    "move_per_list(\"train//validation_list.txt\", path_provided, path_main_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that our train\\audio directory no longer contains the files from testing_list.txt and validation_list.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare main/train\n",
    "All the .wav files remaining in the path_provided (the original contents of \\train\\audio provided by Google) are meant to be treated as the training set, so let's move them to the appropriate data\\main\\train folder.\n",
    "\n",
    "This step deals with the largest chunk of data so it can take a couple of minutes, depending on the hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use glob module to grab all .wav files in the subfolders\n",
    "for category in categories_all:\n",
    "    # create regex path for glob\n",
    "    glob_regex = os.path.join(path_provided, category)\n",
    "    glob_regex = os.path.join(glob_regex, \"*.wav\")\n",
    "    \n",
    "    # use glob to grab all .wav files in that category's subdirectory\n",
    "    train_wav_files = glob.glob(glob_regex)\n",
    "\n",
    "    # move them to our data\\main\\train folder\n",
    "    for wav_file in train_wav_files:\n",
    "        \n",
    "        # grab the actual unique name of each .wav file (e.q. yes\\004ae712_nohash_0.wav, without the parent dirs)\n",
    "        wav_unique_name = wav_file[12:]\n",
    "        \n",
    "        # move them\n",
    "        os.rename(wav_file, os.path.join(path_main_train, wav_unique_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The origina path_provided (\\train\\audio\\(...)) should now contain empty category subfolders, with the exception of _background_noise_, which we'll use for creating our \"silence\" examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare main/-subset-/unknown\n",
    "Some of the folders contain .wav files that we should consider to belong to one common category - \"unknown\", representing the fact that the user may have said something, but it was not a known command. For example categories such as \"happy\", \"wow\" and \"tree\". We can see how we'd like out voice-recognition applications to distinguish between an unknown word and silence/background noise.\n",
    "\n",
    "We'll want to move all the files from those folders into one folder named \"uknown\". The tricky part is that some of the files within \"happy\" and \"wow\" may have the same name. They're only uniquely named if we take into consideration the name of their folder.\n",
    "\n",
    "My solution is to rename all files within these categories by appending the name of the folder to the end of the filename and then move all of the renamed files to \"unknown\" and then remove the remaining empty folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zero', 'three', 'six', 'nine', 'cat', 'house', 'tree']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure yout unknown categories are lowercase\n",
    "categories_unknown = [category.lower() for category in categories_unknown]\n",
    "categories_unknown[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step below is moving a lot of files, so again it may take a couple minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for every subset (train, test, cv) and subcategory (e.g. 'house', 'tree' etc.)\n",
    "for path_to_subset in paths_main:\n",
    "    for unknown_cat in categories_unknown:\n",
    "        \n",
    "        # create regex for glob\n",
    "        path_to_cat = os.path.join(path_to_subset, unknown_cat)\n",
    "        glob_regex = os.path.join(path_to_cat, \"*.wav\")\n",
    "        \n",
    "        # grab all \"unknown\" files\n",
    "        unknown_wavs = glob.glob(glob_regex)\n",
    "        \n",
    "        # create destination path\n",
    "        path_to_destination = os.path.join(path_to_subset, \"unknown\")\n",
    "        \n",
    "        # move & rename files\n",
    "        for wav_file in unknown_wavs:\n",
    "            \n",
    "            # construct new full name (with path) - first replace the category with \"unknown\"\n",
    "            new_full_path = wav_file.replace(unknown_cat, \"unknown\")\n",
    "            \n",
    "            # then add the category name - e.g. \"_tree\" before \".wav\")\n",
    "            new_full_path = new_full_path[:-4] + \"_\" + unknown_cat + new_full_path[-4:]\n",
    "            \n",
    "            # move files\n",
    "            os.rename(wav_file, new_full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can safely remove the folders of the categories that became \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unused category folders from all subsets\n",
    "for path_to_subset in paths_main:\n",
    "    for unknown_cat in categories_unknown:\n",
    "        full_unused_path = os.path.join(path_to_subset, unknown_cat)\n",
    "        !rm -r $full_unused_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down  go  left\tno  off  on  right  silence  stop  unknown  up\tyes\r\n"
     ]
    }
   ],
   "source": [
    "# main/train, main/test and main/cv should now only contain folders named after categories that we'll be predicting\n",
    "!ls $path_to_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">It is important to notice that in main/test we now have 250 examples per category but in the case of unknown we have over 4000 examples. In main/train we have 1850 examples per category and 32K in unknown . This makes our dataset very unbalanced.</div>\n",
    "\n",
    "However at this stage we don't want to lose any data and we may be able to use data augmentation later on to reduce this imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split background noise files\n",
    "In the train / audio / background_noise folder provided by Google Brain & Kaggle we have 6 .wav files of differing length (between 1:00 and 1:35 minutes). We can use them in a couple of different ways - e.g. mix them with our categorised .wav samples so that our model might better learn how to distinguish voice commands from environments with background noise or treat them separately as members of the \"silence\" category.\n",
    "\n",
    "In this notebook we'll take the latter approach and leave mixing .wav files to the next notebook, which will put more focus on preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_1sec(input_source, output_destination, fragment_length=1000):\n",
    "    \"\"\" \n",
    "    Simple function for grabbing .wav files and splitting them into 1 sec\n",
    "    fragments, also handling differing input length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the input source\n",
    "    source = AudioSegment.from_wav(input_source)\n",
    "    \n",
    "    # store length (in milliseconds)\n",
    "    length = len(source)\n",
    "    \n",
    "    # split until there's not enough length to create a full fragment\n",
    "    for i in range(math.floor(length / fragment_length)):\n",
    "        \n",
    "        # grab the right slice\n",
    "        current_fragment = source[i * fragment_length : (i + 1) * fragment_length + 1]\n",
    "        \n",
    "        # construct output fragment's name\n",
    "        fragment_name = output_destination + \"_{}.wav\".format(i+1)\n",
    "        \n",
    "        # save the fragment\n",
    "        current_fragment.export(fragment_name, format=\"wav\")\n",
    "        \n",
    "        # close current fragment\n",
    "        del current_fragment\n",
    "    \n",
    "    # close the input source\n",
    "    del source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function to populate train/audio/\\_background\\_noise\\_ with 1 second fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_types = [\"doing_the_dishes\", \"dude_miaowing\", \n",
    "                    \"exercise_bike\", \"pink_noise\",\n",
    "                    \"running_tap\", \"white_noise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new directory for the fragments\n",
    "path_to_fragments = os.path.join(path_provided, \"_background_noise_fragments_\")\n",
    "!mkdir $path_to_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all 6 background noise files\n",
    "for background in background_types:\n",
    "    # grab the full-length background file\n",
    "    path_to_background = os.path.join(path_provided, \"_background_noise_\", background + \".wav\")\n",
    "    \n",
    "    # create destination path\n",
    "    path_to_destination = os.path.join(path_to_fragments, background)\n",
    "    \n",
    "    # split\n",
    "    split_into_1sec(path_to_background, path_to_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we listen to the files we will notice that some of them are louder than others, which may be good for the background noise \"silence\" category, but not necessarily so for other categories. We may wish to look at that in the preprocessing notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/audio/_background_noise_fragments_\n"
     ]
    }
   ],
   "source": [
    "print(path_to_fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    398     398    8167\r\n"
     ]
    }
   ],
   "source": [
    "# let's see how many \"silence\" examples we have\n",
    "!ls $path_to_fragments | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we've got almost 400 examples of 1-second background noises that we can now move to the main/ _subset_ /silence folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare main/-subset-/silence\n",
    "At this point the number of examples per category in our subsets is as follows - approximately 260 per category in test & CV (with a disproportionately larger number examples in \"unknown\" and 0 examples in \"silence\"). In train it's approximately 1800 examples per category with a similarly larger number in \"unknown\" and 0 examples in \"silence\".\n",
    "\n",
    "Having split our background noise files into 1 second fragments we have 400 examples for that category. That is not enough to fill the 3 buckets - 260 + 260 + 1840 = 2360. As this notebook is not about changing the data we'll just split our \"silence\" examples between the 3 subsets keeping the same ratio (1 : 1 : 7). \n",
    "\n",
    "We will do it in random order but we would like the have the same number of examples from each of the 6 types of background noise we were provided with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/main/cv/silence\n"
     ]
    }
   ],
   "source": [
    "# paths to silence category\n",
    "path_test_silence = os.path.join(path_main_test, \"silence\")\n",
    "path_cv_silence = os.path.join(path_main_cv, \"silence\")\n",
    "path_train_silence = os.path.join(path_main_train, \"silence\")\n",
    "print(path_cv_silence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of leftovers in doing_the_dishes: 7\n",
      "Number of leftovers in dude_miaowing: 5\n",
      "Number of leftovers in exercise_bike: 5\n",
      "Number of leftovers in pink_noise: 4\n",
      "Number of leftovers in running_tap: 5\n",
      "Number of leftovers in white_noise: 4\n"
     ]
    }
   ],
   "source": [
    "# keep the 1 : 1 : 7 ratio (splitting each group into 8 subgroups)\n",
    "for background in background_types:\n",
    "    \n",
    "    # number of subgroups needed to keep ratio\n",
    "    num_subgroup = 8\n",
    "    \n",
    "    # create regex for glob\n",
    "    path_to_fragment_type = os.path.join(path_to_fragments, background)\n",
    "    glob_regex = path_to_fragment_type + \"*.wav\"\n",
    "\n",
    "    # grab all 1-sec fragments of that background type\n",
    "    silence_wavs = glob.glob(glob_regex)\n",
    "    \n",
    "    # calculate modulo\n",
    "    num_wavs = len(silence_wavs)\n",
    "    leftover = num_wavs % num_subgroup\n",
    "    print(\"Number of leftovers in {}: {}\".format(background, leftover))\n",
    "    \n",
    "    # calculate the actual number of samples equal to 1/8th of whole\n",
    "    split_point = math.ceil(num_wavs / 8)\n",
    "    \n",
    "    # move the first 1/8th into main/test/silence\n",
    "    for wav_file in silence_wavs[:split_point]:\n",
    "        \n",
    "        # grab the name of the file (without the path)\n",
    "        filename = wav_file[41:]\n",
    "        \n",
    "        # move files\n",
    "        os.rename(wav_file, os.path.join(path_test_silence, filename))\n",
    "        \n",
    "    # move the second 1/8th into main/cv/silence\n",
    "    for wav_file in silence_wavs[split_point:split_point * 2]:\n",
    "        \n",
    "        # grab the name of the file (without the path)\n",
    "        filename = wav_file[41:]\n",
    "        \n",
    "        # move files\n",
    "        os.rename(wav_file, os.path.join(path_cv_silence, filename))\n",
    "        \n",
    "    # move the remaining 6/8ths + leftover into main/train/silence\n",
    "    for wav_file in silence_wavs[split_point * 2:]:\n",
    "        \n",
    "        # grab the name of the file (without the path)\n",
    "        filename = wav_file[41:]\n",
    "        \n",
    "        # move files\n",
    "        os.rename(wav_file, os.path.join(path_train_silence, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy subsets into sample/-subset-/category\n",
    "Now that we have the wav files in the right category folders in our test, cv and train subsets (albeit rembembering the disproportion issue) we can copy a small, proportionate number of them to a data/sample folder. It will contain the same 3 subsets, further divided into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define how many files we want per category in sample's test, cv and train\n",
    "num_sample_test = 5\n",
    "num_sample_cv = 5\n",
    "num_sample_train = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample directories\n",
    "!mkdir -p data/sample/train\n",
    "!mkdir -p data/sample/test\n",
    "!mkdir -p data/sample/cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sample paths\n",
    "path_sample_test = \"data/sample/test\"\n",
    "path_sample_cv = \"data/sample/cv\"\n",
    "path_sample_train = \"data/sample/train\"\n",
    "\n",
    "paths_sample = [path_sample_test, path_sample_cv, path_sample_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders for each category in sample\n",
    "for path in paths_sample:\n",
    "    for subfolder_name in categories_to_predict:\n",
    "        subfolder_path = os.path.join(path, subfolder_name)\n",
    "        !mkdir $subfolder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed numpy's random submodule to get the same wav files in the sample set\n",
    "np.random.seed(220788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_sample(source_path, destination_path, quantity):\n",
    "    \"\"\"\n",
    "    Simple function for moving a certain quantity of files \n",
    "    from main dataset folders to sample. \n",
    "    \n",
    "    Randomizes the order.\n",
    "    \"\"\"\n",
    "    for category in categories_to_predict:\n",
    "        \n",
    "        # grab all appropriate files\n",
    "        cur_path = os.path.join(source_path, category)\n",
    "        glob_regex = os.path.join(cur_path, \"*.wav\")\n",
    "        wav_files = glob.glob(glob_regex)\n",
    "        \n",
    "        # randomize the order\n",
    "        shuffled_wav_files = np.random.permutation(wav_files)\n",
    "        \n",
    "        # copy to sample\n",
    "        for n in range(quantity):\n",
    "            filename = shuffled_wav_files[n][len(source_path) + 1:]\n",
    "            copyfile(shuffled_wav_files[n], os.path.join(destination_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to main/sample/test\n",
    "move_to_sample(path_main_test, path_sample_test, num_sample_test)\n",
    "\n",
    "# move to main/sample/cv\n",
    "move_to_sample(path_main_cv, path_sample_cv, num_sample_cv)\n",
    "\n",
    "# move to main/sample/train\n",
    "move_to_sample(path_main_train, path_sample_train, num_sample_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! If you tried following along but got stuck feel free to contact me through my github profile.\n",
    "\n",
    "Don't get discouraged, the next steps will be more exciting from a machine learning perspective - we'll be investigating our data through various visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
