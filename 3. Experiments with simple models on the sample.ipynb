{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with simple models on the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks we have separated a small subset of our data, called \"sample\", on which we can now experiment with simple models to assess the effectiveness of our preprocessing & data augmentation techniques.\n",
    "\n",
    "We do it this way to avoid spending too much time on training on the entire set, the assumption is that the methods which are effective on the sample will work well on a larger scale too. \n",
    "\n",
    "We will start by testing a couple of simple models on untouched sample data (as numpy arrays) and then proceed towards data augmentation and finally spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/mateusz/Documents/Mateusz/Career/Machine Learning & AI/tensorflow_speech_recognition/tensorflow_speech_recognition\n"
     ]
    }
   ],
   "source": [
    "# first make sure we're in the parent dictory of our data/sample folders.\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "We'll need a couple of additional libraries so let's import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'C:\\\\Users\\\\mateusz\\\\Documents\\\\Mateusz\\\\Career\\\\Machine Learning & AI\\\\tensorflow_speech_recognition\\\\tensorflow_speech_recognition\\\\utils.py'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# utils\n",
    "from importlib import reload\n",
    "import utils; reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "The easiest way to work with data is by turning it into a list of numbers, in our case a numpy array. We can use one of the functions from utils to load the raw data or use the librosa.load() function. The difference lies in the fact that the former returns int16s whereas librosa returns float32s and uses its default sampling rate of 22050Hz, unless we explicitly tell it to use the file's original sampling rate of 16000Hz.\n",
    "\n",
    "We should also consider normalizing our data (so that it all falls within the same scale) and extracting a 1D mel-frequency cepstrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample = \"data\\\\sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to go through each of the folders in our sample/train, cv and test sets, one-hot encode their label and load the 16K long array of raw data. The y data will be of shape (m, 12), where m is the number of examples, and the X data will be of shape (m, 16000).\n",
    "\n",
    "Let's calculate **m** first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.zeros((100, 16000))\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.zeros(100, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action plan\n",
    "1) turn the sample data into numpy arrays with X and y normally <br>\n",
    "1b) turn sample data into numpy arrays with X and y via mfccs<br>\n",
    "2) Use random forest?<br>\n",
    "3) Use linear model? (towards first benchmark)<br>\n",
    "4) Use various keras CNNs?<br>\n",
    "5) Add preprocessing and test a couple of the best models<br>\n",
    "6) Experiments on images without data augmentation<br>\n",
    "7) Experiments on images with data augmentation<br>\n",
    "8) Decide on e.g. 3 most promising methods<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start by trying a simple model on the 1D mfccs -> even a linear model,then maybe 1D convolutions on keras, then move on to actual 2D stuff.\n",
    "\n",
    "**If we work on 1D data (like mfccs/waveforms) we can use the data augmentation done by the guy here:https://www.kaggle.com/CVxTz/audio-data-augmentation when passing our files into the Keras DataGenerator, but if we decide to work with the MEL images we can just use the same image augmentation as in fastai**\n",
    "\n",
    "se very simple linear model / keras network to see how we do on current sample, then experiment with different preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_mfccs(wav_file):\n",
    "    \"\"\"\n",
    "    Take a file and return the mel-frequency cepstrum.\n",
    "    \"\"\"\n",
    "    X, sample_rate = librosa.load(wav_file, res_type='kaiser_fast', sr=None)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample = \"data\\\\sample\"\n",
    "path_to_a_wav = os.path.join(path_to_sample, \"cv\\\\unknown\\\\9db2bfe9_nohash_4_five.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.06296364e+02,  6.31600698e+01, -2.38641127e+01, -4.86630969e+00,\n",
       "       -3.53521586e+01, -3.80595467e+00, -1.06260360e+01, -5.45357225e+00,\n",
       "       -5.38032267e-01, -3.13763738e+00, -1.61412864e+00, -3.92968492e+00,\n",
       "       -5.57078467e+00, -4.21382641e+00, -8.39318905e+00,  2.59598676e+00,\n",
       "       -1.21718174e+01,  6.58169994e+00, -6.52752377e+00,  2.20022835e+00,\n",
       "       -4.70370097e+00, -7.75634867e-01, -2.45838166e+00, -1.27684907e+00,\n",
       "       -9.24384769e-01, -2.84166555e+00, -2.06350172e+00, -8.51055474e-01,\n",
       "       -6.62192168e-01, -1.39785145e+00, -1.65039538e+00,  3.35274945e-03,\n",
       "        1.09041363e+00, -5.96439092e-01,  5.99651357e-01, -2.19326520e+00,\n",
       "        7.19763870e-01,  1.33843908e+00,  1.59644506e-01, -9.80777004e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_mfccs(path_to_a_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -3.0517578e-05, -3.0517578e-05, -3.0517578e-05], dtype=float32), 16000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and that's what the waveform uses, I think\n",
    "librosa.core.load(path_to_a_wav, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mateusz\\docume~1\\mateusz\\career\\machin~1\\tensor~2\\trf_venv\\lib\\site-packages\\pydub\\utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from utils import get_wav_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_wav_info(path_to_a_wav)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(librosa.core.load(path_to_a_wav)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
