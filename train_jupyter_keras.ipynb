{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with Jupyter & Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to create a **1D convolutional model** in Keras and train it on the same exact voice data as the pure Tensorflow script. \n",
    "\n",
    "The Jupyter + Keras model quickly reaches an *accuracy of over 0.6* on the cross-validation set.\n",
    "<br>The Tensorflow model gets stuck at *0.35*.\n",
    "\n",
    "**The goal is to find the reason for this discrepancy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm environment\n",
    "And check the python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.1.13\n",
      "adium-theme-ubuntu==0.3.4\n",
      "alabaster==0.7.7\n",
      "astor==0.6.2\n",
      "Babel==1.3\n",
      "backports-abc==0.5\n",
      "backports.functools-lru-cache==1.4\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "backports.weakref==1.0.post1\n",
      "bcolz==1.2.0\n",
      "bleach==1.5.0\n",
      "boto==2.38.0\n",
      "certifi==2018.1.18\n",
      "chardet==2.3.0\n",
      "configparser==3.5.0\n",
      "croniter==0.3.8\n",
      "cryptography==1.2.3\n",
      "cycler==0.10.0\n",
      "decorator==4.2.1\n",
      "docutils==0.12\n",
      "duplicity==0.7.6\n",
      "entrypoints==0.2.3\n",
      "enum34==1.1.6\n",
      "funcsigs==1.0.2\n",
      "functools32==3.2.3.post2\n",
      "futures==3.2.0\n",
      "gast==0.2.0\n",
      "grpcio==1.10.0\n",
      "html5lib==0.9999999\n",
      "idna==2.0\n",
      "ipaddress==1.0.16\n",
      "ipykernel==4.8.2\n",
      "ipython==5.6.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.2.0\n",
      "isoweek==1.3.3\n",
      "jedi==0.11.1\n",
      "Jinja2==2.10\n",
      "jsonschema==2.6.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.3\n",
      "jupyter-console==5.2.0\n",
      "jupyter-core==4.4.0\n",
      "Keras==2.1.5\n",
      "lockfile==0.12.2\n",
      "Markdown==2.6.11\n",
      "MarkupSafe==1.0\n",
      "matplotlib==2.1.0\n",
      "mistune==0.8.3\n",
      "mock==2.0.0\n",
      "msgpack-python==0.4.6\n",
      "nbconvert==5.3.1\n",
      "nbformat==4.4.0\n",
      "ndg-httpsclient==0.4.0\n",
      "nose==1.3.7\n",
      "notebook==5.4.1\n",
      "numpy==1.14.2\n",
      "olefile==0.45.1\n",
      "opencv-python==3.4.0.12\n",
      "pandas==0.22.0\n",
      "pandas-summary==0.0.41\n",
      "pandocfilters==1.4.2\n",
      "parso==0.1.1\n",
      "pathlib2==2.3.0\n",
      "pbr==4.0.1\n",
      "pexpect==4.4.0\n",
      "pickleshare==0.7.4\n",
      "Pillow==5.1.0\n",
      "prompt-toolkit==1.0.15\n",
      "protobuf==3.5.2.post1\n",
      "ptyprocess==0.5.2\n",
      "pyasn1==0.1.9\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0\n",
      "Pygments==2.2.0\n",
      "pygobject==3.20.0\n",
      "PyMySQL==0.7.2\n",
      "pyOpenSSL==0.15.1\n",
      "pyparsing==2.2.0\n",
      "Pyste==0.9.10\n",
      "python-apt==1.1.0b1+ubuntu0.16.4.1\n",
      "python-cloudfiles==1.7.10\n",
      "python-dateutil==2.7.2\n",
      "pytz==2018.3\n",
      "pyxdg==0.25\n",
      "PyYAML==3.12\n",
      "pyzmq==17.0.0\n",
      "qtconsole==4.3.1\n",
      "requests==2.9.1\n",
      "roman==2.0.0\n",
      "salt==2015.8.8\n",
      "scandir==1.7\n",
      "scipy==1.0.1\n",
      "seaborn==0.8.1\n",
      "Send2Trash==1.5.0\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.11.0\n",
      "Sphinx==1.3.6\n",
      "sphinx-rtd-theme==0.1.9\n",
      "subprocess32==3.2.7\n",
      "tensorboard==1.7.0\n",
      "tensorflow==1.7.0\n",
      "tensorflow-tensorboard==1.5.1\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.1\n",
      "testpath==0.3.1\n",
      "Theano==0.9.0\n",
      "tornado==4.5.3\n",
      "tqdm==4.20.0\n",
      "traitlets==4.3.2\n",
      "unity-lens-photos==1.0\n",
      "urllib3==1.13.1\n",
      "virtualenv==15.0.1\n",
      "wcwidth==0.1.7\n",
      "webcolors==1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.14.1\n",
      "widgetsnbextension==3.2.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# list modules\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/bin/python\n",
      "Python version:  3.6.3\n"
     ]
    }
   ],
   "source": [
    "# confirm python version\n",
    "from platform import python_version\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(\"Python version: \", python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "We'll need a couple of additional libraries so let's import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow\n",
    "import time\n",
    "\n",
    "# keras as tensorflow backend\n",
    "from tensorflow.python.keras.layers import Dense, BatchNormalization, Dropout, Conv1D\n",
    "from tensorflow.python.keras.layers import Input, MaxPooling1D, GlobalMaxPool1D, Activation\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the bcolz array saving functions\n",
    "def bcolz_save(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def bcolz_load(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "Define the path to the downloaded voice data (the parent directory). That's where all the .bc files should be located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/home/paperspace/tfvoice/tensorflow_speech_recognition/data/main/redownloaded/data_redownloaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the y\n",
    "train_y = bcolz_load(path_to_data + os.path.sep + \"train_y\" + \".bc\")\n",
    "cv_y = bcolz_load(path_to_data + os.path.sep + \"cv_y\" + \".bc\")\n",
    "test_y = bcolz_load(path_to_data + os.path.sep + \"test_y\" + \".bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the Test & CV X\n",
    "# raw\n",
    "cv_X = bcolz_load(path_to_data + os.path.sep + \"cv_X\" + \".bc\")\n",
    "test_X = bcolz_load(path_to_data + os.path.sep + \"test_X\" + \".bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the Train X\n",
    "# raw\n",
    "train_Xs = []\n",
    "for i in range(7):\n",
    "    train_subset = bcolz_load(path_to_data + os.path.sep + \"train_X\" + str(i + 1) +\".bc\")\n",
    "    train_Xs.append(train_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Train y\n",
    "Since we've split our Train X, it will be easier to split our Train y too, when we're passing it to our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train X subsets have 3168 examples each (7 total), exactly\n",
    "train_ys = []\n",
    "subset_size = 3168\n",
    "for i in range(7):\n",
    "    train_y_subset = train_y[subset_size * i : subset_size * (i + 1)]\n",
    "    train_ys.append(train_y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ys[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand dimensions for convolutions\n",
    "We have to add a dimension for the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 16000, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to expand the dimensions for 1D convolutions\n",
    "expanded_train_Xs = [np.expand_dims(train_X, axis=2) for train_X in train_Xs]\n",
    "expanded_train_Xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3051, 16000, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same for CV\n",
    "expanded_cv_X = np.expand_dims(cv_X, axis=2)\n",
    "expanded_cv_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "We're using a simple 1D-convolutional architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output needs 12 dimensions\n",
    "num_categories = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for a couple more epochs, adjusting the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D Convolutional Community Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer & batch normalization\n",
    "inputs = Input(shape = (16000,1))\n",
    "x_1d = BatchNormalization(name = 'batchnormal_1d_in')(inputs)\n",
    "\n",
    "# iteratively create 9 blocks of 2 convolutional layers with batchnorm and max-pooling\n",
    "for i in range(9):\n",
    "    \n",
    "    name = 'step'+str(i)\n",
    "    \n",
    "    # first 1D convolutional block\n",
    "    x_1d = Conv1D(8*(2 ** i), (3),padding = 'same', name = 'conv'+name+'_1')(x_1d)\n",
    "    x_1d = BatchNormalization(name = 'batch'+name+'_1')(x_1d)\n",
    "    x_1d = Activation('relu')(x_1d)\n",
    "    \n",
    "    # second 1D convolutional block\n",
    "    x_1d = Conv1D(8*(2 ** i), (3),padding = 'same', name = 'conv'+name+'_2')(x_1d)\n",
    "    x_1d = BatchNormalization(name = 'batch'+name+'_2')(x_1d)\n",
    "    x_1d = Activation('relu')(x_1d)\n",
    "    \n",
    "    # max pooling\n",
    "    x_1d = MaxPooling1D((2), padding='same')(x_1d)\n",
    "\n",
    "# final convolution and dense layer\n",
    "x_1d = Conv1D(1024, (1),name='last1024')(x_1d)\n",
    "x_1d = GlobalMaxPool1D()(x_1d)\n",
    "x_1d = Dense(1024, activation = 'relu', name= 'dense1024_onlygmax')(x_1d)\n",
    "x_1d = Dropout(0.2)(x_1d)\n",
    "\n",
    "# soft-maxed prediction layer\n",
    "predictions = Dense(num_categories, activation = 'softmax',name='cls_1d')(x_1d)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(Adam(lr=0.0001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time it\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 25s 8ms/step - loss: 2.6278 - acc: 0.2121 - val_loss: 2.6301 - val_acc: 0.0826\n",
      "Took 29.46 seconds\n",
      "\n",
      "2 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 1.5749 - acc: 0.4605 - val_loss: 2.6486 - val_acc: 0.0914\n",
      "Took 47.54 seconds\n",
      "\n",
      "3 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 1.1365 - acc: 0.6114 - val_loss: 2.2627 - val_acc: 0.3114\n",
      "Took 65.33 seconds\n",
      "\n",
      "4 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.8854 - acc: 0.7071 - val_loss: 1.6555 - val_acc: 0.5864\n",
      "Took 83.22 seconds\n",
      "\n",
      "5 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.7161 - acc: 0.7620 - val_loss: 1.7171 - val_acc: 0.6509\n",
      "Took 101.10 seconds\n",
      "\n",
      "6 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.6807 - acc: 0.7715 - val_loss: 1.5002 - val_acc: 0.7076\n",
      "Took 119.09 seconds\n",
      "\n",
      "7 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.9400 - acc: 0.7336 - val_loss: 1.1473 - val_acc: 0.5752\n",
      "Took 137.15 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keep track of epoch\n",
    "cur_epoch_nr = 1\n",
    "\n",
    "# fit iteratively\n",
    "for i, expanded_train_X in enumerate(expanded_train_Xs):\n",
    "    \n",
    "    # pretty printing\n",
    "    print(i + 1, \"/\", len(expanded_train_Xs))\n",
    "    \n",
    "    result = model.fit(expanded_train_X, train_ys[i], batch_size=32, epochs=1, \n",
    "             validation_data=(expanded_cv_X, cv_y))\n",
    "    \n",
    "    # pretty printing\n",
    "    duration = time.time() - start\n",
    "    print(\"Took {:.2f} seconds\\n\".format(duration))\n",
    "    \n",
    "    # results\n",
    "    cv_acc = \"{:.4f}\".format(result.history[\"val_acc\"][0]).replace(\".\",\"\")\n",
    "    train_acc = \"{:.4f}\".format(result.history[\"acc\"][0]).replace(\".\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.6231 - acc: 0.7986 - val_loss: 0.8713 - val_acc: 0.7729\n",
      "Took 154.82 seconds\n",
      "\n",
      "2 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.5531 - acc: 0.8150 - val_loss: 0.9874 - val_acc: 0.7686\n",
      "Took 172.61 seconds\n",
      "\n",
      "3 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.5045 - acc: 0.8232 - val_loss: 0.9109 - val_acc: 0.7850\n",
      "Took 190.42 seconds\n",
      "\n",
      "4 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.4434 - acc: 0.8513 - val_loss: 0.8984 - val_acc: 0.8043\n",
      "Took 208.25 seconds\n",
      "\n",
      "5 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.4565 - acc: 0.8475 - val_loss: 0.9400 - val_acc: 0.7961\n",
      "Took 226.45 seconds\n",
      "\n",
      "6 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.4290 - acc: 0.8589 - val_loss: 0.9559 - val_acc: 0.7876\n",
      "Took 244.75 seconds\n",
      "\n",
      "7 / 7\n",
      "Train on 3168 samples, validate on 3051 samples\n",
      "Epoch 1/1\n",
      "3168/3168 [==============================] - 18s 6ms/step - loss: 0.6963 - acc: 0.8037 - val_loss: 0.6509 - val_acc: 0.7709\n",
      "Took 262.88 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keep track of epoch\n",
    "cur_epoch_nr = 1\n",
    "\n",
    "# fit iteratively\n",
    "for i, expanded_train_X in enumerate(expanded_train_Xs):\n",
    "    \n",
    "    # pretty printing\n",
    "    print(i + 1, \"/\", len(expanded_train_Xs))\n",
    "    \n",
    "    result = model.fit(expanded_train_X, train_ys[i], batch_size=32, epochs=1, \n",
    "             validation_data=(expanded_cv_X, cv_y))\n",
    "    \n",
    "    # pretty printing\n",
    "    duration = time.time() - start\n",
    "    print(\"Took {:.2f} seconds\\n\".format(duration))\n",
    "    \n",
    "    # results\n",
    "    cv_acc = \"{:.4f}\".format(result.history[\"val_acc\"][0]).replace(\".\",\"\")\n",
    "    train_acc = \"{:.4f}\".format(result.history[\"acc\"][0]).replace(\".\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after just 2 epochs our model has reached a **validation accuracy of 0.7 - 0.8**, with relatively little overfitting (0.85). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
