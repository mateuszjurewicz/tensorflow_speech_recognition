{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation - testing models on the sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start by trying a simple model on the 1D mfccs -> even a linear model,then maybe 1D convolutions on keras, then move on to actual 2D stuff.\n",
    "\n",
    "**If we work on 1D data (like mfccs/waveforms) we can use the data augmentation done by the guy here:https://www.kaggle.com/CVxTz/audio-data-augmentation when passing our files into the Keras DataGenerator, but if we decide to work with the MEL images we can just use the same image augmentation as in fastai**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_mfccs(wav_file):\n",
    "    \"\"\"\n",
    "    Take a file and return the mel-frequency cepstrum.\n",
    "    \"\"\"\n",
    "    X, sample_rate = librosa.load(wav_file, res_type='kaiser_fast')\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample = \"data\\\\sample\"\n",
    "path_to_a_wav = os.path.join(path_to_sample, \"cv\\\\unknown\\\\9db2bfe9_nohash_4_five.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.38342023e+02,  8.29850387e+01, -3.20632889e+01,  6.37540073e+00,\n",
       "       -3.19599086e+01, -1.39741733e+01, -3.95277138e+00, -9.98425167e+00,\n",
       "       -2.48800156e+00, -1.11268423e+00, -2.25928103e+00, -1.53453264e+00,\n",
       "       -1.95897790e+00, -5.86902125e+00, -1.50266835e+00, -8.68940669e+00,\n",
       "        6.41446550e-01, -4.03155311e+00, -6.39934192e+00,  5.79747927e+00,\n",
       "       -5.60918048e+00,  9.07524699e-01, -2.10970421e+00, -3.44061912e+00,\n",
       "        9.85725218e-01, -3.85770681e+00,  1.28078267e+00, -2.56340414e+00,\n",
       "       -1.76835514e+00, -1.56781554e+00, -1.15724246e+00,  2.01605221e-02,\n",
       "       -2.16906684e+00, -5.79067656e-01, -1.08417760e+00,  1.21203371e+00,\n",
       "       -5.31809967e-01,  2.04484697e-01, -1.71348463e-01, -1.93764401e+00])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_mfccs(path_to_a_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -5.2211535e-06, -1.3541688e-05, -2.2571026e-05], dtype=float32), 22050)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and that's what the waveform uses, I think\n",
    "librosa.core.load(path_to_a_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
